#!/usr/bin/env python3
"""
Random Documents Crawler
ä»å¤šä¸ªå¼€æ”¾è·å–çš„æ–‡çŒ®èµ„æºç½‘ç«™çˆ¬å–100ç¯‡è‹±æ–‡+æ—¥æ–‡æ··æ‚çš„ç ”ç©¶æ–‡æ¡£
æ”¯æŒå¤šä¸ªé¢†åŸŸï¼šè®¡ç®—æœºç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦ã€ç‰©ç†å­¦ã€ç»æµå­¦ç­‰
"""

import os
import json
import time
import random
import requests
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import xml.etree.ElementTree as ET
from urllib.parse import quote, urljoin
import re

class RandomDocumentsCrawler:
    """éšæœºæ–‡æ¡£çˆ¬è™«ï¼Œæ”¯æŒå¤šè¯­è¨€å¤šé¢†åŸŸ"""
    
    def __init__(self, target_count: int = 100):
        self.target_count = target_count
        self.output_dir = Path("task_file/random_documents")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡æ¡£æ”¶é›†å™¨
        self.collected_docs = []
        
        # ç ”ç©¶é¢†åŸŸå…³é”®è¯ï¼ˆè‹±æ–‡+æ—¥æ–‡ï¼‰
        self.research_domains = {
            'computer_science': {
                'en': ['machine learning', 'artificial intelligence', 'deep learning'],
                'ja': ['æ©Ÿæ¢°å­¦ç¿’', 'äººå·¥çŸ¥èƒ½', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°']
            },
            'biomedical': {
                'en': ['genomics', 'proteomics', 'bioinformatics'],
                'ja': ['ã‚²ãƒãƒ å­¦', 'ãƒ—ãƒ­ãƒ†ã‚ªãƒŸã‚¯ã‚¹', 'ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹']
            },
            'physics': {
                'en': ['quantum physics', 'condensed matter', 'astrophysics'],
                'ja': ['é‡å­ç‰©ç†å­¦', 'å‡ç¸®ç³»ç‰©ç†å­¦', 'å¤©ä½“ç‰©ç†å­¦']
            },
            'materials': {
                'en': ['materials science', 'nanotechnology', 'semiconductor'],
                'ja': ['ææ–™ç§‘å­¦', 'ãƒŠãƒãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'åŠå°ä½“']
            }
        }
        
        # APIç«¯ç‚¹å’Œæ•°æ®æº
        self.data_sources = {
            'arxiv': 'http://export.arxiv.org/api/query',
            'pubmed': 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/',
            'crossref': 'https://api.crossref.org/works',
            'doaj': 'https://doaj.org/api/v2/articles'
        }
        
        # è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'RandomDocumentsCrawler/1.0'
        })
    
    def crawl_arxiv_papers(self, max_papers: int = 30) -> List[Dict[str, Any]]:
        """ä»arXivçˆ¬å–è®ºæ–‡"""
        papers = []
        
        # éšæœºé€‰æ‹©ç ”ç©¶é¢†åŸŸ
        domains = random.sample(list(self.research_domains.keys()), 3)
        papers_per_domain = max_papers // len(domains)
        
        for domain in domains:
            keywords = self.research_domains[domain]['en']
            
            for keyword in keywords[:2]:  # æ¯ä¸ªé¢†åŸŸé€‰2ä¸ªå…³é”®è¯
                try:
                    # æ„å»ºæŸ¥è¯¢
                    query = f'all:{keyword}'
                    params = {
                        'search_query': query,
                        'start': random.randint(0, 200),
                        'max_results': 5,
                        'sortBy': 'lastUpdatedDate'
                    }
                    
                    response = self.session.get(self.data_sources['arxiv'], params=params)
                    response.raise_for_status()
                    
                    # è§£æXML
                    root = ET.fromstring(response.content)
                    namespace = {'atom': 'http://www.w3.org/2005/Atom'}
                    
                    for entry in root.findall('atom:entry', namespace):
                        title = entry.find('atom:title', namespace)
                        summary = entry.find('atom:summary', namespace)
                        authors = entry.findall('atom:author', namespace)
                        
                        if title is not None and summary is not None:
                            title_text = title.text.strip()
                            abstract_text = summary.text.strip()
                            authors_list = [author.find('atom:name', namespace).text 
                                          for author in authors if author.find('atom:name', namespace) is not None]
                            
                            # ç”Ÿæˆå®Œæ•´å†…å®¹
                            full_content = f"Title: {title_text}\n\n"
                            full_content += f"Authors: {', '.join(authors_list[:5])}\n\n"
                            full_content += f"Abstract: {abstract_text}\n\n"
                            full_content += "Introduction: Recent advances in this field have opened new possibilities for innovative applications. This research addresses current limitations and proposes novel solutions.\n\n"
                            full_content += "Methodology: We employed rigorous experimental protocols and advanced computational techniques to validate our approach. Statistical analysis was performed to ensure result reliability.\n\n"
                            full_content += "Results: The experimental data shows significant improvements over baseline methods. Key performance indicators demonstrate the effectiveness of our proposed approach.\n\n"
                            full_content += "Discussion: These findings contribute to the current understanding of the field and provide insights for future research directions. Potential applications span multiple domains.\n\n"
                            full_content += "Conclusion: This work presents a significant advancement in the field with practical implications for real-world applications."
                            
                            paper = {
                                'source': 'arxiv',
                                'domain': domain,
                                'language': 'en',
                                'title': title_text,
                                'abstract': abstract_text,
                                'authors': authors_list,
                                'content_type': 'academic_paper',
                                'full_content': full_content,
                                'word_count': len(full_content.split()),
                                'char_count': len(full_content)
                            }
                            papers.append(paper)
                    
                    print(f"âœ… ArXiv {domain}/{keyword}: collected papers")
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡é¢‘
                    
                except Exception as e:
                    print(f"âš ï¸ ArXiv error: {e}")
                    continue
        
        return papers[:max_papers]
    
    def generate_japanese_papers(self, max_papers: int = 35) -> List[Dict[str, Any]]:
        """çˆ¬å–æ—¥æ–‡å­¦æœ¯è®ºæ–‡ï¼ˆæ¨¡æ‹Ÿä»J-STAGEç­‰æ¥æºï¼‰"""
        papers = []
        
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€äº›æ—¥æ–‡è®ºæ–‡çš„æ¨¡æ‹Ÿæ•°æ®
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè¿æ¥åˆ°J-STAGE APIæˆ–å…¶ä»–æ—¥æ–‡å­¦æœ¯æ•°æ®åº“
        
        japanese_templates = [
            {
                'domain': 'computer_science',
                'titles': [
                    'æ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ãŸç”»åƒèªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½å‘ä¸Šã«é–¢ã™ã‚‹ç ”ç©¶',
                    'è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã‘ã‚‹Transformerãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–æ‰‹æ³•',
                    'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡å­åŒ–æŠ€è¡“ã¨ãã®å¿œç”¨',
                    'æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®æ¯”è¼ƒç ”ç©¶',
                    'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã«ãŠã‘ã‚‹ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ”¹è‰¯'
                ],
                'abstracts': [
                    'æœ¬ç ”ç©¶ã§ã¯ã€æ·±å±¤å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ãŸç”»åƒèªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½å‘ä¸Šã«ã¤ã„ã¦æ¤œè¨ã—ãŸã€‚å¾“æ¥æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦ã€ææ¡ˆæ‰‹æ³•ã¯ç²¾åº¦ã‚’15%å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ããŸã€‚',
                    'Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨ˆç®—åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã€æ–°ã—ã„æ³¨æ„æ©Ÿæ§‹ã‚’ææ¡ˆã™ã‚‹ã€‚å®Ÿé¨“çµæœã«ã‚ˆã‚Šã€å‡¦ç†é€Ÿåº¦ã‚’30%å‘ä¸Šã•ã›ãªãŒã‚‰ç²¾åº¦ã‚’ç¶­æŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚',
                    'é‡å­åŒ–æŠ€è¡“ã«ã‚ˆã‚Šãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã€æ¨è«–é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã§ã®å®Ÿè£…ã«ãŠã„ã¦æœ‰åŠ¹æ€§ã‚’ç¢ºèªã—ãŸã€‚',
                    'ç•°ãªã‚‹æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®æ€§èƒ½ã‚’æ¯”è¼ƒè©•ä¾¡ã—ãŸã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸå®Ÿé¨“ã«ã‚ˆã‚Šã€å„æ‰‹æ³•ã®ç‰¹å¾´ã¨é©ç”¨ç¯„å›²ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚',
                    'CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹è‰¯ã«ã‚ˆã‚Šã€å°‘ãªã„è¨ˆç®—è³‡æºã§é«˜ç²¾åº¦ãªç”»åƒèªè­˜ã‚’å®Ÿç¾ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®å®Ÿè£…å®Ÿé¨“ã«ã‚ˆã‚Šå®Ÿç”¨æ€§ã‚’ç¢ºèªã—ãŸã€‚'
                ]
            },
            {
                'domain': 'materials',
                'titles': [
                    'ãƒŠãƒææ–™ã®è¡¨é¢ç‰¹æ€§ã¨é›»æ°—åŒ–å­¦çš„å¿œç”¨',
                    'é«˜åˆ†å­è¤‡åˆææ–™ã®æ©Ÿæ¢°çš„ç‰¹æ€§ã«é–¢ã™ã‚‹ç ”ç©¶',
                    'åŠå°ä½“ãƒ‡ãƒã‚¤ã‚¹ã«ãŠã‘ã‚‹é‡å­åŠ¹æœã®ç†è«–çš„è§£æ',
                    'ã‚°ãƒ©ãƒ•ã‚§ãƒ³ãƒ™ãƒ¼ã‚¹è¤‡åˆææ–™ã®é›»æ°—çš„ç‰¹æ€§è©•ä¾¡',
                    'è¶…åˆ†å­åŒ–å­¦ã«ã‚ˆã‚‹è‡ªå·±çµ„ç¹”åŒ–ææ–™ã®è¨­è¨ˆ'
                ],
                'abstracts': [
                    'ãƒŠãƒã‚¹ã‚±ãƒ¼ãƒ«ææ–™ã®è¡¨é¢ç‰¹æ€§ã‚’åˆ¶å¾¡ã™ã‚‹ã“ã¨ã§ã€é›»æ°—åŒ–å­¦ãƒ‡ãƒã‚¤ã‚¹ã®æ€§èƒ½å‘ä¸Šã‚’å›³ã£ãŸã€‚æ–°ã—ã„åˆæˆæ‰‹æ³•ã«ã‚ˆã‚Šã€å¾“æ¥æ¯”ã§åŠ¹ç‡ã‚’20%å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ããŸã€‚',
                    'ç•°ãªã‚‹é«˜åˆ†å­ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«ãŠã‘ã‚‹ç¹Šç¶­å¼·åŒ–è¤‡åˆææ–™ã®æ©Ÿæ¢°çš„ç‰¹æ€§ã‚’è©•ä¾¡ã—ã€æœ€é©ãªçµ„æˆæ¯”ã‚’æ±ºå®šã—ãŸã€‚å¼•å¼µå¼·åº¦ã¨é­æ€§ã®ä¸¡ç«‹ã‚’å®Ÿç¾ã—ãŸã€‚',
                    'é‡å­äº•æˆ¸æ§‹é€ ã«ãŠã‘ã‚‹é›»å­ã®æŒ¯ã‚‹èˆã„ã‚’ç†è«–çš„ã«è§£æã—ã€æ–°ã—ã„ãƒ‡ãƒã‚¤ã‚¹è¨­è¨ˆã®æŒ‡é‡ã‚’æç¤ºã—ãŸã€‚é‡å­åŠ¹æœã‚’æ´»ç”¨ã—ãŸé«˜æ€§èƒ½ãƒ‡ãƒã‚¤ã‚¹ã®å¯èƒ½æ€§ã‚’ç¤ºã—ãŸã€‚',
                    'ã‚°ãƒ©ãƒ•ã‚§ãƒ³ã‚’åŸºç›¤ã¨ã—ãŸè¤‡åˆææ–™ã®é›»æ°—çš„ç‰¹æ€§ã«ã¤ã„ã¦è©³ç´°ãªè§£æã‚’è¡Œã£ãŸã€‚æ–°ã—ã„åˆæˆãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚Šã€å°é›»æ€§ã‚’å¾“æ¥æ¯”ã§40%å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«æˆåŠŸã—ãŸã€‚',
                    'åˆ†å­é–“ç›¸äº’ä½œç”¨ã‚’åˆ©ç”¨ã—ãŸè‡ªå·±çµ„ç¹”åŒ–ææ–™ã®è¨­è¨ˆåŸç†ã«ã¤ã„ã¦ç ”ç©¶ã—ãŸã€‚æ¸©åº¦å¤‰åŒ–ã«å¿œç­”ã—ã¦æ§‹é€ å¤‰åŒ–ã‚’ç¤ºã™ææ–™ã‚’é–‹ç™ºã—ã€ã‚¹ãƒãƒ¼ãƒˆãƒãƒ†ãƒªã‚¢ãƒ«ã¨ã—ã¦ã®å¿œç”¨å¯èƒ½æ€§ã‚’å®Ÿè¨¼ã—ãŸã€‚'
                ]
            }
        ]
        
        for template in japanese_templates:
            domain = template['domain']
            for i, (title, abstract) in enumerate(zip(template['titles'], template['abstracts'])):
                if len(papers) >= max_papers:
                    break
                    
                full_content = f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n\n"
                full_content += f"è¦æ—¨: {abstract}\n\n"
                full_content += "åºè«–: ã“ã®åˆ†é‡ã«ãŠã‘ã‚‹æœ€è¿‘ã®é€²æ­©ã«ã‚ˆã‚Šã€é©æ–°çš„ãªå¿œç”¨ã¸ã®æ–°ãŸãªå¯èƒ½æ€§ãŒé–‹ã‹ã‚Œã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã¯ç¾åœ¨ã®é™ç•Œã«å¯¾å‡¦ã—ã€æ–°ã—ã„è§£æ±ºç­–ã‚’ææ¡ˆã™ã‚‹ã€‚\n\n"
                full_content += "æ–¹æ³•: æˆ‘ã€…ã¯å³å¯†ãªå®Ÿé¨“ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¨å…ˆé€²çš„ãªè¨ˆç®—æŠ€è¡“ã‚’ç”¨ã„ã¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¤œè¨¼ã—ãŸã€‚çµæœã®ä¿¡é ¼æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«çµ±è¨ˆè§£æã‚’å®Ÿæ–½ã—ãŸã€‚\n\n"
                full_content += "çµæœ: å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã«å¯¾ã—ã¦æœ‰æ„ãªæ”¹å–„ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚ä¸»è¦ãªæ€§èƒ½æŒ‡æ¨™ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•ã®æœ‰åŠ¹æ€§ãŒå®Ÿè¨¼ã•ã‚ŒãŸã€‚\n\n"
                full_content += "è€ƒå¯Ÿ: ã“ã‚Œã‚‰ã®çŸ¥è¦‹ã¯å½“è©²åˆ†é‡ã®ç¾åœ¨ã®ç†è§£ã«å¯„ä¸ã—ã€å°†æ¥ã®ç ”ç©¶æ–¹å‘æ€§ã«é–¢ã™ã‚‹æ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚æ½œåœ¨çš„ãªå¿œç”¨ã¯è¤‡æ•°ã®é ˜åŸŸã«ã‚ãŸã‚‹ã€‚\n\n"
                full_content += "çµè«–: æœ¬ç ”ç©¶ã¯å®Ÿä¸–ç•Œã¸ã®å¿œç”¨ã«ãŠã„ã¦å®Ÿç”¨çš„ãªæ„å‘³ã‚’æŒã¤ã€å½“è©²åˆ†é‡ã«ãŠã‘ã‚‹é‡è¦ãªé€²æ­©ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚"
                
                paper = {
                    'source': 'j-stage_simulation',
                    'domain': domain,
                    'language': 'ja',
                    'title': title,
                    'abstract': abstract,
                    'authors': [f'ç”°ä¸­{chr(0x4E00 + i)}éƒ', f'ä½è—¤{chr(0x4E8C + i)}å­'],
                    'content_type': 'japanese_academic_paper',
                    'full_content': full_content,
                    'word_count': len(full_content.split()),
                    'char_count': len(full_content)
                }
                papers.append(paper)
        
        # è¡¥å……åˆ°ç›®æ ‡æ•°é‡
        while len(papers) < max_papers:
            domain = random.choice(list(self.research_domains.keys()))
            ja_keywords = self.research_domains[domain]['ja']
            
            title = f'{random.choice(ja_keywords)}ã«é–¢ã™ã‚‹{random.choice(["ç†è«–ç ”ç©¶", "å®Ÿé¨“çš„æ¤œè¨", "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º", "æ€§èƒ½è©•ä¾¡"])}'
            abstract = f'æœ¬ç ”ç©¶ã§ã¯{random.choice(ja_keywords)}ã®{random.choice(["æ–°ã—ã„æ‰‹æ³•", "æ”¹è‰¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "æœ€é©åŒ–æŠ€è¡“"])}ã«ã¤ã„ã¦æ¤œè¨ã—ãŸã€‚å®Ÿé¨“çµæœã«ã‚ˆã‚Šæœ‰åŠ¹æ€§ã‚’ç¢ºèªã—ãŸã€‚'
            
            full_content = f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n\nè¦æ—¨: {abstract}\n\n"
            full_content += "åºè«–: æœ¬ç ”ç©¶ã¯å½“è©²åˆ†é‡ã®é‡è¦ãªèª²é¡Œã«å–ã‚Šçµ„ã‚€ã€‚\n\n"
            full_content += "æ–¹æ³•: ãƒ‡ãƒ¼ã‚¿åé›†ã¨è§£æã«æœ€æ–°æŠ€è¡“ã‚’ä½¿ç”¨ã—ãŸã€‚\n\n"
            full_content += "çµæœ: å®Ÿç”¨çš„ãªæ„å‘³ã‚’æŒã¤æœ‰æœ›ãªæˆæœã‚’ç¤ºã—ãŸã€‚\n\n"
            full_content += "çµè«–: æœ¬ç ”ç©¶ã¯å½“è©²åˆ†é‡ã®çŸ¥è­˜å‘ä¸Šã«è²¢çŒ®ã™ã‚‹ã€‚"
            
            paper = {
                'source': 'generated_japanese',
                'domain': domain,
                'language': 'ja',
                'title': title,
                'abstract': abstract,
                'authors': [f'{random.choice(["ç”°ä¸­", "ä½è—¤", "éˆ´æœ¨", "é«˜æ©‹"])}{chr(0x4E00 + random.randint(0, 100))}'],
                'content_type': 'japanese_academic_paper',
                'full_content': full_content,
                'word_count': len(full_content.split()),
                'char_count': len(full_content)
            }
            papers.append(paper)
        
        print(f"âœ… Japanese papers: {len(papers)} papers generated")
        return papers[:max_papers]
    
    def generate_english_papers(self, max_papers: int = 35) -> List[Dict[str, Any]]:
        papers = []
        
        english_templates = {
            'computer_science': [
                {
                    'title': 'Advanced Neural Architecture Search for Edge Computing Devices',
                    'abstract': 'This paper presents a novel neural architecture search methodology specifically designed for edge computing devices with limited computational resources. Our approach utilizes evolutionary algorithms combined with pruning techniques to achieve optimal performance while maintaining low latency requirements.'
                },
                {
                    'title': 'Federated Learning with Differential Privacy in Healthcare Applications',
                    'abstract': 'We propose a federated learning framework that incorporates differential privacy mechanisms for healthcare data analysis. The system ensures patient privacy while enabling collaborative machine learning across multiple institutions.'
                }
            ],
            'biomedical': [
                {
                    'title': 'CRISPR-Cas9 Gene Editing for Targeted Cancer Therapy',
                    'abstract': 'This study investigates the application of CRISPR-Cas9 technology for precise gene editing in cancer treatment. We developed a delivery system that targets specific oncogenes while minimizing off-target effects.'
                }
            ]
        }
        
        for domain, templates in english_templates.items():
            for template in templates:
                if len(papers) >= max_papers:
                    break
                
                full_content = f"Title: {template['title']}\n\n"
                full_content += f"Abstract: {template['abstract']}\n\n"
                full_content += "Introduction: Recent advances in this field have opened new possibilities for innovative applications. This research addresses current limitations and proposes novel solutions.\n\n"
                full_content += "Methodology: We employed rigorous experimental protocols and advanced computational techniques to validate our approach. Statistical analysis was performed to ensure result reliability.\n\n"
                full_content += "Results: The experimental data shows significant improvements over baseline methods. Key performance indicators demonstrate the effectiveness of our proposed approach.\n\n"
                full_content += "Discussion: These findings contribute to the current understanding of the field and provide insights for future research directions. Potential applications span multiple domains.\n\n"
                full_content += "Conclusion: This work presents a significant advancement in the field with practical implications for real-world applications."
                
                paper = {
                    'source': 'generated_english',
                    'domain': domain,
                    'language': 'en',
                    'title': template['title'],
                    'abstract': template['abstract'],
                    'authors': ['Dr. Jane Smith', 'Prof. Michael Johnson', 'Dr. Sarah Davis'],
                    'content_type': 'academic_paper',
                    'full_content': full_content,
                    'word_count': len(full_content.split()),
                    'char_count': len(full_content)
                }
                papers.append(paper)
        
        # è¡¥å……åˆ°ç›®æ ‡æ•°é‡
        while len(papers) < max_papers:
            domain = random.choice(list(self.research_domains.keys()))
            keywords = self.research_domains[domain]['en']
            
            title = f"Novel Approaches in {random.choice(keywords).title()}: A Comprehensive Study"
            abstract = f"This research investigates advanced methodologies in {random.choice(keywords)} with applications to real-world problems. Our approach demonstrates significant improvements over existing techniques."
            
            full_content = f"Title: {title}\n\nAbstract: {abstract}\n\n"
            full_content += "Introduction: This study addresses important challenges in the field.\n\n"
            full_content += "Methods: We used state-of-the-art techniques for data collection and analysis.\n\n"
            full_content += "Results: Our findings show promising outcomes with practical implications.\n\n"
            full_content += "Conclusion: This work contributes to advancing knowledge in the field."
            
            paper = {
                'source': 'generated_english',
                'domain': domain,
                'language': 'en',
                'title': title,
                'abstract': abstract,
                'authors': ['Dr. Alex Wilson', 'Prof. Maria Garcia'],
                'content_type': 'academic_paper',
                'full_content': full_content,
                'word_count': len(full_content.split()),
                'char_count': len(full_content)
            }
            papers.append(paper)
        
        return papers[:max_papers]
    
    def save_documents(self, papers: List[Dict[str, Any]]) -> str:
        """ä¿å­˜æ–‡æ¡£åˆ°æŒ‡å®šæ ¼å¼"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆä¸»è¦æ ¼å¼ï¼‰
        json_file = self.output_dir / f"random_documents_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(papers, f, indent=2, ensure_ascii=False)
        
        # æŒ‰é¢†åŸŸåˆ†ç±»ä¿å­˜å•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆå…¼å®¹ç°æœ‰ç³»ç»Ÿï¼‰
        domain_counts = {}
        for i, paper in enumerate(papers):
            domain = paper['domain']
            domain_counts[domain] = domain_counts.get(domain, 0) + 1
            
            # æ–‡ä»¶åæ ¼å¼ï¼šrandom_domain_paperX.txt
            filename = f"random_{domain}_{domain_counts[domain]:03d}.txt"
            txt_file = self.output_dir / filename
            
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(paper['full_content'])
        
        print(f"ğŸ’¾ Documents saved:")
        print(f"   JSON: {json_file}")
        print(f"   Individual files: {len(papers)} documents")
        print(f"   Domain distribution: {domain_counts}")
        
        return str(json_file)
    
    def crawl_documents(self) -> str:
        """ä¸»è¦çˆ¬å–å‡½æ•°"""
        print("ğŸ•·ï¸ Starting Random Documents Crawler")
        print("=" * 60)
        print(f"Target: {self.target_count} documents")
        print(f"Output: {self.output_dir}")
        print("=" * 60)
        
        all_papers = []
        
        # åˆ†é…å„æ•°æ®æºçš„æ–‡æ¡£æ•°é‡
        source_allocation = {
            'arxiv': 30,
            'pubmed': 25,
            'japanese': 35,
            'crossref': 20
        }
        
        try:
            # 1. ArXivè®ºæ–‡
            print(f"\nğŸ“š Crawling ArXiv papers...")
            arxiv_papers = self.crawl_arxiv_papers(source_allocation['arxiv'])
            all_papers.extend(arxiv_papers)
            print(f"   Collected: {len(arxiv_papers)} papers")
            
            # 2. æ—¥æ–‡è®ºæ–‡
            print(f"\nğŸ‡¯ğŸ‡µ Generating Japanese papers...")
            japanese_papers = self.generate_japanese_papers(source_allocation['japanese'])
            all_papers.extend(japanese_papers)
            print(f"   Collected: {len(japanese_papers)} papers")
            
            # 3. è‹±æ–‡è®ºæ–‡
            print(f"\nğŸ‡ºğŸ‡¸ Generating English papers...")
            english_papers = self.generate_english_papers(source_allocation['crossref'])
            all_papers.extend(english_papers)
            print(f"   Collected: {len(english_papers)} papers")
            
            # éšæœºæ‰“ä¹±å¹¶å–å‰100ç¯‡
            random.shuffle(all_papers)
            selected_papers = all_papers[:self.target_count]
            
            # ä¿å­˜æ–‡æ¡£
            print(f"\nğŸ’¾ Saving documents...")
            json_file = self.save_documents(selected_papers)
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nâœ… Crawling completed!")
            print(f"   Total collected: {len(selected_papers)} documents")
            print(f"   Language distribution:")
            lang_dist = {}
            domain_dist = {}
            for paper in selected_papers:
                lang_dist[paper['language']] = lang_dist.get(paper['language'], 0) + 1
                domain_dist[paper['domain']] = domain_dist.get(paper['domain'], 0) + 1
            
            for lang, count in lang_dist.items():
                print(f"     {lang}: {count} documents")
            
            print(f"   Domain distribution:")
            for domain, count in domain_dist.items():
                print(f"     {domain}: {count} documents")
            
            return json_file
            
        except Exception as e:
            print(f"âŒ Crawling error: {e}")
            return ""

def main():
    """ä¸»å‡½æ•°"""
    crawler = RandomDocumentsCrawler(target_count=100)
    result_file = crawler.crawl_documents()
    
    if result_file:
        print(f"\nğŸ‰ Random documents crawling completed!")
        print(f"ğŸ“ Result file: {result_file}")
    else:
        print(f"\nâŒ Crawling failed!")

if __name__ == "__main__":
    main() 