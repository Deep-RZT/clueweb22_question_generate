#!/usr/bin/env python3
"""
OpenAI API Client for Deep Research QA Benchmark
Replaces Claude API with OpenAI GPT-4o for all content generation
"""

import json
import requests
import time
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class APIResponse:
    """ç»Ÿä¸€çš„APIå“åº”æ ¼å¼"""
    content: str
    model: str
    usage: Dict[str, int]
    success: bool
    error: Optional[str] = None

class OpenAIClient:
    """OpenAI API client for content generation"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.api_key = api_key
        self.model = model
        self.api_url = "https://api.openai.com/v1/chat/completions"
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
    def generate_content(self, prompt: str, system_prompt: str = None, 
                        max_tokens: int = 6000, temperature: float = 0.7,
                        max_retries: int = 3, retry_delay: float = 2.0) -> Optional[str]:
        """
        Generate content using OpenAI API with optimized parameters for longer responses
        
        Args:
            prompt: User prompt
            system_prompt: Optional system prompt
            max_tokens: Maximum tokens to generate (increased for longer responses)
            temperature: Sampling temperature
            max_retries: Maximum number of retry attempts
            retry_delay: Delay between retries in seconds
            
        Returns:
            Generated content or None if failed
        """
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": 0.9,
            "frequency_penalty": 0.1,
            "presence_penalty": 0.1
        }
        
        for attempt in range(max_retries):
            try:
                print(f"  ğŸ”„ OpenAI APIè°ƒç”¨ (å°è¯• {attempt + 1}/{max_retries})")
                
                response = requests.post(self.api_url, headers=self.headers, json=data, timeout=30)
                response.raise_for_status()
                
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content'].strip()
                    print(f"  âœ… APIè°ƒç”¨æˆåŠŸ (å†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦)")
                    return content
                else:
                    if attempt < max_retries - 1:
                        print(f"  âš ï¸ å“åº”æ— å†…å®¹ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        print(f"âŒ No choices in OpenAI response after {max_retries} attempts: {result}")
                        return None
                        
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:  # Rate limit
                    if attempt < max_retries - 1:
                        # å°è¯•ä»å“åº”å¤´ä¸­è¯»å–å»ºè®®çš„ç­‰å¾…æ—¶é—´
                        retry_after = e.response.headers.get('Retry-After')
                        if retry_after:
                            wait_time = int(retry_after)
                        else:
                            # æ›´ä¿å®ˆçš„æŒ‡æ•°é€€é¿
                            wait_time = min(retry_delay * (2 ** attempt), 60)  # æœ€å¤§60ç§’
                        
                        print(f"  ğŸš¦ APIé€Ÿç‡é™åˆ¶ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âŒ Rate limit exceeded after {max_retries} attempts")
                        return None
                elif e.response.status_code >= 500:  # Server errors
                    if attempt < max_retries - 1:
                        print(f"  ğŸ”§ æœåŠ¡å™¨é”™è¯¯ ({e.response.status_code})ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        print(f"âŒ Server error {e.response.status_code} after {max_retries} attempts")
                        return None
                else:
                    print(f"âŒ HTTP error: {e}")
                    return None
                    
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    print(f"  ğŸ”Œ ç½‘ç»œé”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    continue
                else:
                    print(f"âŒ OpenAI API request failed after {max_retries} attempts: {e}")
                    return None
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"  âŒ æœªçŸ¥é”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    continue
                else:
                    print(f"âŒ OpenAI API error after {max_retries} attempts: {e}")
                    return None
        
        return None
    
    def generate_response(self, prompt: str, system_prompt: str = None, 
                         max_tokens: int = 2000, temperature: float = 0.3,
                         max_retries: int = 10) -> str:
        """
        Alias for generate_content method to maintain compatibility with existing code
        """
        result = self.generate_content(prompt, system_prompt, max_tokens, temperature, max_retries)
        return result if result is not None else ""
    
    def generate_text(self, 
                     prompt: str, 
                     max_tokens: int = 4000,
                     temperature: float = 0.7,
                     system_prompt: Optional[str] = None,
                     max_retries: int = 3,
                     retry_delay: float = 2.0) -> APIResponse:
        """ç”Ÿæˆæ–‡æœ¬ - ä¸Claude APIå…¼å®¹çš„æ¥å£ï¼Œå¢åŠ é‡è¯•æœºåˆ¶"""
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": 0.9,
            "frequency_penalty": 0.1,
            "presence_penalty": 0.1
        }
        
        for attempt in range(max_retries):
            try:
                print(f"  ğŸ”„ OpenAI APIè°ƒç”¨ (å°è¯• {attempt + 1}/{max_retries})")
                
                response = requests.post(self.api_url, headers=self.headers, json=data, timeout=30)
                response.raise_for_status()
                
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content'].strip()
                    usage = result.get('usage', {})
                    
                    print(f"  âœ… APIè°ƒç”¨æˆåŠŸ (tokens: {usage.get('total_tokens', 0)})")
                    return APIResponse(
                        content=content,
                        model=self.model,
                        usage={
                            "prompt_tokens": usage.get('prompt_tokens', 0),
                            "completion_tokens": usage.get('completion_tokens', 0),
                            "total_tokens": usage.get('total_tokens', 0)
                        },
                        success=True
                    )
                else:
                    if attempt < max_retries - 1:
                        print(f"  âš ï¸ å“åº”æ— å†…å®¹ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        return APIResponse(
                            content="",
                            model=self.model,
                            usage={},
                            success=False,
                            error="No choices in response after all retries"
                        )
                        
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:  # Rate limit
                    if attempt < max_retries - 1:
                        # å°è¯•ä»å“åº”å¤´ä¸­è¯»å–å»ºè®®çš„ç­‰å¾…æ—¶é—´
                        retry_after = e.response.headers.get('Retry-After')
                        if retry_after:
                            wait_time = int(retry_after)
                        else:
                            # æ›´ä¿å®ˆçš„æŒ‡æ•°é€€é¿
                            wait_time = min(retry_delay * (2 ** attempt), 60)  # æœ€å¤§60ç§’
                        
                        print(f"  ğŸš¦ APIé€Ÿç‡é™åˆ¶ï¼Œ{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return APIResponse(
                            content="",
                            model=self.model,
                            usage={},
                            success=False,
                            error=f"Rate limit exceeded after {max_retries} attempts"
                        )
                elif e.response.status_code >= 500:  # Server errors
                    if attempt < max_retries - 1:
                        print(f"  ğŸ”§ æœåŠ¡å™¨é”™è¯¯ ({e.response.status_code})ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        return APIResponse(
                            content="",
                            model=self.model,
                            usage={},
                            success=False,
                            error=f"Server error {e.response.status_code} after {max_retries} attempts"
                        )
                else:
                    return APIResponse(
                        content="",
                        model=self.model,
                        usage={},
                        success=False,
                        error=f"HTTP error: {str(e)}"
                    )
                    
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    print(f"  ğŸ”Œ ç½‘ç»œé”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    continue
                else:
                    return APIResponse(
                        content="",
                        model=self.model,
                        usage={},
                        success=False,
                        error=f"Request failed after {max_retries} attempts: {str(e)}"
                    )
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"  âŒ æœªçŸ¥é”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    continue
                else:
                    return APIResponse(
                        content="",
                        model=self.model,
                        usage={},
                        success=False,
                        error=f"API error after {max_retries} attempts: {str(e)}"
                    )
        
        return APIResponse(
            content="",
            model=self.model,
            usage={},
            success=False,
            error="Max retries exceeded"
        )
    
    def generate_report(self, documents: List[Dict], topic: str, max_tokens: int = 4000) -> APIResponse:
        """ç”Ÿæˆé¢†åŸŸæŠ¥å‘Š - æ”¯æŒåˆ†æ®µå¤„ç†å’Œèåˆ"""
        
        # æ£€æŸ¥æ–‡æ¡£æ€»é•¿åº¦
        total_chars = sum(len(doc.get('content', '')) for doc in documents)
        max_chars_per_segment = 80000  # OpenAIä½¿ç”¨æ›´ä¿å®ˆçš„é™åˆ¶
        
        if total_chars > max_chars_per_segment:
            print(f"ğŸ“„ æ–‡æ¡£å†…å®¹è¿‡é•¿ ({total_chars} å­—ç¬¦)ï¼Œå¯ç”¨åˆ†æ®µå¤„ç†...")
            return self._generate_segmented_report(documents, topic, max_tokens)
        else:
            return self._generate_single_report(documents, topic, max_tokens)
    
    def _generate_single_report(self, documents: List[Dict], topic: str, max_tokens: int) -> APIResponse:
        """ç”Ÿæˆå•æ®µæŠ¥å‘Š"""
        # æ„å»ºæ–‡æ¡£å†…å®¹ï¼Œå¹¶æ£€æŸ¥é•¿åº¦
        doc_content = ""
        total_chars = 0
        max_chars = 120000  # çº¦ç›¸å½“äº120K tokensçš„å®‰å…¨é™åˆ¶ï¼Œä¸ºOpenAIç•™æ›´å¤šä½™åœ°
        
        for i, doc in enumerate(documents, 1):
            doc_text = f"\næ–‡æ¡£ {i}:\næ ‡é¢˜: {doc.get('title', 'N/A')}\nå†…å®¹: {doc.get('content', 'N/A')}\n"
            
            # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºé™åˆ¶
            if total_chars + len(doc_text) > max_chars:
                # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£å°±è¶…é™ï¼Œæˆªæ–­è¯¥æ–‡æ¡£
                if i == 1:
                    remaining_chars = max_chars - total_chars - 200  # ç•™ä¸€äº›ç¼“å†²
                    if remaining_chars > 1000:
                        content = doc.get('content', 'N/A')[:remaining_chars]
                        doc_text = f"\næ–‡æ¡£ {i}:\næ ‡é¢˜: {doc.get('title', 'N/A')}\nå†…å®¹: {content}...[æ–‡æ¡£å·²æˆªæ–­]\n"
                        doc_content += doc_text
                    break
                else:
                    # å·²ç»æœ‰å…¶ä»–æ–‡æ¡£ï¼Œåœæ­¢æ·»åŠ 
                    print(f"âš ï¸ æ–‡æ¡£è¿‡å¤šï¼Œåªä½¿ç”¨å‰ {i-1} ä¸ªæ–‡æ¡£ç”ŸæˆæŠ¥å‘Š")
                    break
            
            doc_content += doc_text
            total_chars += len(doc_text)
        
        if not doc_content.strip():
            # å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡æ¡£å†…å®¹ï¼Œåˆ›å»ºé”™è¯¯å“åº”
            return APIResponse(
                content="",
                model=self.model,
                usage={},
                success=False,
                error="No document content available after processing"
            )
        
        print(f"ğŸ“Š ä½¿ç”¨æ–‡æ¡£å†…å®¹é•¿åº¦: {total_chars} å­—ç¬¦")
        
        system_prompt = """You are an expert research synthesizer. Create a flowing narrative that naturally weaves together all document content into a cohesive research landscape overview.

Requirements for natural narrative synthesis:
1. Length: 1200-1800 words (comprehensive coverage - MUST reach at least 1200 words)
2. Write as a continuous research narrative that flows naturally from topic to topic
3. COMPLETELY AVOID any structural markers like "Abstract", "Introduction", "Methods", "Results", "Conclusion", "Summary", section numbers, bullet points, or formal organization
4. Instead of categorizing information, tell the story of the research field as revealed through these documents
5. Weave together ALL specific details naturally: exact numbers, percentages, sample sizes, parameter settings, performance metrics, researcher names, dates, institutions, methodologies
6. Create natural transitions between different research threads and findings
7. Write ENTIRELY in English for consistency
8. If documents contain Japanese content, seamlessly integrate the translated concepts into the English narrative
9. Make it read like an expert's comprehensive knowledge synthesis rather than a formal academic summary
10. CRITICAL: Ensure every factual detail is preserved in the narrative flow to enable complex multi-document reasoning questions"""

        prompt = f"""Create a comprehensive research narrative that weaves together all the information from these documents into a flowing story of the research landscape. YOU MUST WRITE AT LEAST 1200 WORDS:

{doc_content}

CRITICAL NARRATIVE REQUIREMENTS:
1. Length: MINIMUM 1200 words, target 1500-1800 words (this is MANDATORY)
2. Write as a continuous narrative that tells the story of the research field - how different studies connect, build upon each other, and reveal patterns
3. COMPLETELY ELIMINATE any formal structure: no sections, no "Abstract/Introduction/Conclusion", no numbered points, no bullet lists, no headers
4. Naturally integrate EVERY factual detail into the flowing narrative: exact numbers, percentages, sample sizes, parameter values, performance metrics, researcher names, publication dates, institutional affiliations, technical specifications
5. Show the connections and relationships between different research efforts, methodologies, and findings as they naturally emerge in the narrative
6. Create seamless transitions that show how one research finding leads to or relates to another
7. Write ENTIRELY in English - if documents contain Japanese content, smoothly integrate translated concepts
8. Make it read like an expert researcher explaining the comprehensive state of the field to a colleague, not like a formal academic summary
9. Preserve every numerical result, methodology detail, and technical specification within the natural flow of the narrative
10. CRITICAL: The narrative must contain enough interconnected factual details to support complex questions requiring multi-document reasoning and cross-referencing

IMPORTANT: Write as if you're telling the complete story of this research area. Count your words as you write. You MUST reach at least 1200 words by expanding on the relationships between studies, detailed explanations of methodologies, and comprehensive coverage of all findings and technical details."""

        return self.generate_text(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=0.7,
            system_prompt=system_prompt
        )
    
    def _generate_segmented_report(self, documents, topic_id=None):
        """åˆ†æ®µç”ŸæˆæŠ¥å‘Š"""
        try:
            # åˆ†æ®µå¤„ç†
            segments = self._split_documents(documents)
            segment_reports = []
            
            for i, segment in enumerate(segments):
                logger.info(f"  ğŸ” å¤„ç†ç¬¬ {i+1}/{len(segments)} æ®µ...")
                logger.info(f"ğŸ“Š ä½¿ç”¨æ–‡æ¡£å†…å®¹é•¿åº¦: {len(segment)} å­—ç¬¦")
                
                # æ¯æ®µç›®æ ‡å­—æ•°æ›´é«˜
                target_words = max(1000, min(1500, len(segment) // 40))
                
                prompt = f"""Based on the following document content, generate a detailed segment report.

CRITICAL REQUIREMENTS:
1. Word count: MANDATORY minimum {target_words} words - count your words as you write
2. Content requirements:
   - Detailed description of ALL information, data, methods and findings
   - Include EVERY specific number, percentage, statistical data found
   - Mention ALL research methods, experimental parameters, sample information
   - Cover ALL authors, institutions, time, location and specific information
   - Include ALL detailed cases, examples and technical details
   - Elaborate extensively on every finding and methodology
3. Writing requirements:
   - Maximum information density, avoid any empty descriptions
   - Use extremely rich details and specific data
   - Ensure comprehensive and extremely in-depth content
   - NEVER use section headers or markdown formatting
   - Write as continuous flowing narrative

IMPORTANT: If you haven't reached {target_words} words, continue adding more detail until you reach the minimum word count.

Document content:
{segment}

Generate a detailed segment report (minimum {target_words} words):"""

                response = self.generate_text(
                    prompt=prompt,
                    max_tokens=2500,  # å¢åŠ tokené™åˆ¶
                    temperature=0.3,
                    system_prompt="You are a professional academic report writing expert, skilled at generating detailed, information-rich segment reports."
                )
                
                if response.success:
                    segment_report = response.content
                    word_count = len(segment_report.split())
                    segment_reports.append(segment_report)
                    logger.info(f"    âœ… ç¬¬{i+1}æ®µå®Œæˆ ({word_count} è¯)")
                else:
                    logger.error(f"    âŒ ç¬¬{i+1}æ®µç”Ÿæˆå¤±è´¥: {response.error}")
                    return None
            
            # èåˆå„æ®µæŠ¥å‘Š - å…³é”®ä¼˜åŒ–
            logger.info("  ğŸ”„ èåˆå„æ®µæŠ¥å‘Š...")
            merged_report = self._merge_segment_reports(segment_reports)
            
            return merged_report
            
        except Exception as e:
            logger.error(f"åˆ†æ®µç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return None

    def _split_documents(self, documents, max_chars_per_segment=80000):
        """åˆ†å‰²æ–‡æ¡£ä¸ºå¤šä¸ªæ®µè½"""
        try:
            # å¦‚æœdocumentsæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥åˆ†å‰²
            if isinstance(documents, str):
                text = documents
            else:
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆå¹¶ä¸ºå­—ç¬¦ä¸²
                text = " ".join(str(doc) for doc in documents)
            
            segments = []
            current_segment = ""
            
            # æŒ‰å¥å­åˆ†å‰²
            sentences = text.split('.')
            
            for sentence in sentences:
                if len(current_segment) + len(sentence) + 1 <= max_chars_per_segment:
                    current_segment += sentence + "."
                else:
                    if current_segment:
                        segments.append(current_segment)
                    current_segment = sentence + "."
            
            # æ·»åŠ æœ€åä¸€æ®µ
            if current_segment:
                segments.append(current_segment)
            
            return segments
            
        except Exception as e:
            logger.error(f"åˆ†å‰²æ–‡æ¡£å¤±è´¥: {str(e)}")
            return [documents]  # è¿”å›åŸå§‹æ–‡æ¡£

    def _merge_segment_reports(self, segment_reports):
        """èåˆåˆ†æ®µæŠ¥å‘Š - å…³é”®ä¼˜åŒ–"""
        try:
            # è®¡ç®—ç›®æ ‡å­—æ•° - é¿å…è¿‡é•¿
            total_segment_words = sum(len(report.split()) for report in segment_reports)
            target_words = max(1000, min(1500, total_segment_words // 3))
            
            # æ„å»ºèåˆprompt
            segments_text = "\n\n".join([f"Segment Report {i+1}:\n{report}" for i, report in enumerate(segment_reports)])
            
            prompt = f"""Please merge the following multiple segment reports into a complete comprehensive report.

CRITICAL REQUIREMENTS:
1. Word count: MANDATORY minimum {target_words} words - count your words as you write
2. Merging requirements:
   - Integrate ALL information from ALL segments, avoid any omissions
   - Merge similar topics while avoiding repetition
   - Retain ALL specific data, numbers, methods, findings
   - Retain ALL authors, institutions, time and other information
   - Maintain logical coherence and fluency
3. Content requirements:
   - Detailed description of ALL research findings and data
   - Include ALL specific numbers, percentages, statistical information
   - Mention ALL detailed research methods and technical details
   - Cover ALL cases and application scenarios
   - Elaborate extensively on every finding and methodology
4. Structure requirements:
   - NEVER use section headers like "Abstract", "Introduction", "Conclusion", "Summary"
   - NEVER use markdown headers (##, ###) or numbered sections
   - Write as continuous flowing narrative without formal structure
   - Naturally organize content by topic importance
   - Ensure maximum information density and rich content

IMPORTANT: If you haven't reached {target_words} words, continue adding more detail about methodologies, experimental procedures, results analysis, and technical specifications until you reach the minimum word count.

Segment report content:
{segments_text}

Generate a detailed merged report (minimum {target_words} words):"""

            response = self.generate_text(
                prompt=prompt,
                max_tokens=3000,  # å¤§å¹…å¢åŠ tokené™åˆ¶
                temperature=0.2,   # é™ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                system_prompt="You are a professional academic report merging expert, skilled at integrating multiple segment reports into information-rich comprehensive reports."
            )
            
            if response.success:
                merged_report = response.content
                word_count = len(merged_report.split())
                logger.info(f"    âœ… æŠ¥å‘Šèåˆå®Œæˆ ({word_count} è¯)")
                return merged_report
            else:
                logger.error(f"    âŒ æŠ¥å‘Šèåˆå¤±è´¥: {response.error}")
                return None
            
        except Exception as e:
            logger.error(f"èåˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return None

    def generate_deep_short_answer_questions(self, report: str, topic: str, num_questions: int = 50) -> APIResponse:
        """ç”ŸæˆDeep Short Answer Questions - åŸºäºæ‚¨çš„è¦æ±‚ä¼˜åŒ–"""
        
        system_prompt = f"""You are an expert in creating "Deep Short-Answer Queries" that require sophisticated multi-document reasoning but have concise, verifiable answers.

CORE DESIGN PHILOSOPHY (Based on Research Requirements):
1. **DEEP**: Cannot be answered by simple RAG - requires multi-step reasoning across document sections
2. **SHORT ANSWER**: 1-10 words, objectively verifiable (correct/incorrect can be directly judged)
3. **MULTI-DOCUMENT SYNTHESIS**: Requires combining information from different parts of the narrative
4. **HIDDEN CONNECTIONS**: Tests understanding of implicit relationships rather than explicit statements

DEEP QUERY DESIGN PATTERNS:

ğŸ” **CROSS-SECTION SYNTHESIS QUERIES**:
"What exact [metric] was achieved by [method] that also demonstrated [secondary property] in studies using [constraint]?"
â†’ Requires finding method, locating its metrics, and cross-referencing secondary properties

ğŸ” **IMPLICIT RELATIONSHIP DISCOVERY**:
"Which [entity] appears in both [context A] and [context B] research but with different [characteristic]?"
â†’ Requires scanning multiple research contexts and identifying cross-appearances

ğŸ” **DETAIL INTEGRATION QUERIES**:
"What [specific parameter] was used in experiments that achieved [result A] but not [result B]?"
â†’ Requires comparing different experimental setups and identifying distinguishing factors

ğŸ” **TEMPORAL/CAUSAL REASONING**:
"Who first [action] before later [related action] that influenced [outcome]?"
â†’ Requires understanding chronological relationships and causal connections

ğŸ” **HIDDEN PATTERN DISCOVERY**:
"What characteristic is shared by all studies that achieved [threshold] but absent in those below [threshold]?"
â†’ Requires pattern analysis across multiple research examples

CRITICAL REQUIREMENTS:
1. **Multi-step reasoning required**: Simple search should fail
2. **Short, factual answers**: Names, numbers, exact terms, dates
3. **Objectively verifiable**: Clear right/wrong determination possible
4. **Based on narrative content**: Only use information from the provided research narrative
5. **Reasonable difficulty**: Challenging but solvable with careful analysis

STRICTLY AVOID:
- Simple fact lookup questions ("What is X?")
- Questions answerable by keyword search
- Subjective or opinion-based questions
- Questions requiring external knowledge

TARGET: Generate exactly {num_questions} deep short-answer questions."""

        prompt = f"""Based on the following research narrative about "{topic}", create {num_questions} Deep Short-Answer Queries that require multi-document reasoning:

Research Narrative:
{report[:8000]}...

REQUIREMENTS:
1. Generate EXACTLY {num_questions} questions
2. Each question must require 2-3 reasoning steps across different parts of the narrative
3. Answers must be 1-10 words and objectively verifiable
4. Test understanding of connections, patterns, and relationships rather than simple facts
5. Focus on information synthesis rather than information retrieval

Question Format:
Q1: [Deep question requiring multi-step reasoning]
Q2: [Deep question requiring multi-step reasoning]
...

CRITICAL: Ensure each question tests deep understanding of the research landscape while maintaining short, verifiable answers."""

        return self.generate_text(
            prompt=prompt,
            max_tokens=min(max(num_questions * 100, 4000), 12000),
            temperature=0.6,
            system_prompt=system_prompt
        )

    def generate_questions(self, report: str, topic: str, num_questions: int = 50) -> APIResponse:
        """ç”ŸæˆShort Answer Deep Queryé—®é¢˜ - åŸºäºBrowseCompæ ‡å‡†ï¼Œé‡ç‚¹ç”ŸæˆçŸ­ç­”æ¡ˆæ·±åº¦æŸ¥è¯¢"""
        
        system_prompt = """You are an expert question designer specializing in creating "Academic-Grade BrowseComp-style Deep Query" questions that meet high scholarly standards.

CRITICAL MISSION: Generate questions that achieve 3.0+/4 depth score and 2.0+/4 academic rigor score to reach 70%+ Good+ rating.

PROVEN SUCCESSFUL PATTERNS (BASED ON BEST RESULTS):

ğŸ¯ **PATTERN 1 - Multi-Constraint Academic Chain** (Target: 3.5+/4 depth):
"What was the exact [metric] achieved by [method] in the peer-reviewed study that also reported [secondary finding], was conducted by researchers who previously worked on [related topic] at [institution], and used [experimental design] with [dataset] between [time period]?"

ğŸ¯ **PATTERN 2 - Temporal Academic Lineage** (Target: 3.5+/4 depth):
"Which specific [technique] was first proposed by [author] who later collaborated with [institution] researchers in the controlled experiment that achieved [performance], was peer-reviewed by [venue], and whose methodology was subsequently validated in [follow-up study]?"

ğŸ¯ **PATTERN 3 - Cross-Institutional Research** (Target: 3.0+/4 academic):
"Who first proposed [concept] that was later cited in the peer-reviewed study achieving [metric], involved collaboration between [institution A] and [institution B], was published in [venue/year], and whose methodology became the standard for [field]?"

ğŸ¯ **PATTERN 4 - Methodological Validation** (Target: 2.5+/4 academic):
"What was the precise [parameter] in the [year] empirical study that compared [method A] against [method B] using [experimental design], reported statistical significance of [p-value], was conducted by [institution] researchers, and whose results were later replicated by [follow-up team]?"

BALANCED ACADEMIC RIGOR REQUIREMENTS (EVERY QUESTION MUST INCLUDE 3+):
âœ… Methodological terms: "controlled experiment", "peer-reviewed", "empirical validation", "statistical significance"
âœ… Experimental design: "randomized trial", "cross-validation", "baseline comparison", "ablation study"
âœ… Precise metrics: "exact accuracy", "statistical significance", "confidence interval", "effect size"
âœ… Publication context: "published in", "peer-reviewed by", "conference proceedings"
âœ… Validation evidence: "results replicated", "methodology validated", "findings confirmed"
âœ… Temporal progression: "first proposed", "later cited", "subsequently validated"
âœ… Cross-institutional: "collaboration between", "joint research", "multi-site study"

OPTIMAL DEPTH REQUIREMENTS (EVERY QUESTION MUST ACHIEVE):
1. Chain 4+ constraints per question (proven effective)
2. Require 2+ temporal reasoning steps
3. Demand 1+ cross-institutional connections
4. Need 1+ methodological validation steps
5. Include literature impact assessment
6. Require replication/adoption evidence

ACADEMIC VOCABULARY (USE STRATEGICALLY):
- High-impact terms: "peer-reviewed", "controlled experiment", "statistical significance"
- Methodological terms: "empirical validation", "cross-validation", "baseline comparison"
- Impact terms: "cited in studies", "became standard", "methodology adopted"
- Collaboration terms: "multi-institutional", "joint research", "collaboration between"

BALANCED SUCCESS CRITERIA (ACHIEVABLE TARGETS):
- Depth Score Target: 3.0+/4 (Deep level with 4+ constraints)
- Academic Score Target: 2.0+/4 (Solid academic rigor)
- Constraint Count: 4+ per question minimum
- Academic Terms: 3+ per question minimum
- Cross-references: 2+ per question minimum
- Temporal Steps: 2+ per question minimum

MANDATORY TARGET: Generate EXACTLY 90% Academic-Grade BrowseComp questions (proven ratio)

FORMAT: Use simple text format, numbered Q1, Q2, etc."""

        prompt = f"""Based on the following research summary about "{topic}", generate {num_questions} questions where EXACTLY 90% MUST BE Academic-Grade BrowseComp-style Deep Query type:

Research Summary:
{report[:6000]}...

ABSOLUTE MANDATORY REQUIREMENTS FOR ACADEMIC EXCELLENCE:
1. Generate EXACTLY {num_questions} questions
2. EXACTLY 90% MUST be Academic-Grade BrowseComp-style questions (use the mandatory patterns below)
3. Each question MUST achieve 3.0+/4 depth score and 2.0+/4 academic rigor score
4. EVERY question must include methodological terminology and experimental design elements

QUESTION DISTRIBUTION REQUIREMENT:
- EXACTLY {int(num_questions * 0.90)} Academic-Grade BrowseComp Deep Queries
- EXACTLY {int(num_questions * 0.10)} Methodologically rigorous traditional questions

MANDATORY ACADEMIC-GRADE PATTERNS (NO EXCEPTIONS - EVERY QUESTION MUST USE THESE):

PATTERN 1 - Multi-Layer Constraint (4+ constraints):
"What was the exact [metric] achieved by [method] in the peer-reviewed study that also reported [secondary finding], was conducted by researchers who previously worked on [related topic] at [institution], and used [experimental design] with [dataset] between [time period]?"

PATTERN 2 - Cross-Institutional Validation:
"Which specific [technique] was first proposed by [author] who later collaborated with [institution] researchers in the controlled experiment that achieved [performance], was peer-reviewed by [venue], and whose methodology was subsequently validated in [follow-up study]?"

PATTERN 3 - Temporal-Methodological Chain:
"What was the precise [parameter] in the [year] empirical study that compared [method A] against [method B] using [experimental design], reported statistical significance of [p-value], was conducted by [institution] researchers, and whose results were later replicated by [follow-up team]?"

PATTERN 4 - Academic Literature Integration:
"Who first proposed [concept] that was later cited in the peer-reviewed study achieving [metric], involved collaboration between [institution A] and [institution B], was published in [venue/year], and whose methodology became the standard for [field]?"

CRITICAL ACADEMIC REQUIREMENTS (EVERY QUESTION MUST INCLUDE):
âœ… Methodological terms: "controlled experiment", "peer-reviewed", "empirical validation", "statistical significance"
âœ… Experimental design: "randomized trial", "cross-validation", "ablation study", "baseline comparison"  
âœ… Precise metrics: "exact accuracy", "confidence interval", "effect size", "p-value"
âœ… Publication context: "published in", "peer-reviewed by", "conference proceedings"
âœ… Replication/validation: "results replicated", "methodology validated", "findings confirmed"
âœ… Temporal chains: "first proposed", "later cited", "subsequently validated"
âœ… Cross-institutional: "collaboration between", "joint research", "multi-site study"

Format each question as:
Q1: [MUST use Academic-Grade patterns with 4+ constraints and methodological terms]
DEPTH ELEMENTS: [List the constraint count and reasoning steps]
ACADEMIC ELEMENTS: [List methodological and experimental design terms used]
VERIFICATION: [Confirm answer is specific, quantitative, and easily verifiable]

CRITICAL SUCCESS CRITERIA:
- Depth Score Target: 3.0+/4 (Deep level with 4+ constraints)
- Academic Score Target: 2.0+/4 (Solid academic rigor)
- Constraint Count: 4+ per question minimum
- Methodological Terms: 3+ per question minimum
- Cross-references: 2+ per question minimum

Generate exactly {num_questions} questions now with MAXIMUM academic rigor:"""

        # å¢åŠ tokené™åˆ¶ä»¥ç¡®ä¿å®Œæ•´ç”Ÿæˆ
        estimated_tokens = num_questions * 150 + 2000
        max_tokens = min(max(estimated_tokens, 8000), 16000)
        
        return self.generate_text(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=0.7,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„ç»“æœ
            system_prompt=system_prompt
        )
    
    def generate_answer(self, question: str, report: str, difficulty: str) -> APIResponse:
        """ç”Ÿæˆç­”æ¡ˆ - ä¸“ä¸šæµ“ç¼©æ ¸å¿ƒå›ç­”ç³»ç»Ÿ"""
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­ç­”æ¡ˆæ·±åº¦æŸ¥è¯¢
        is_short_answer_query = self._is_short_answer_deep_query(question)
        
        if is_short_answer_query:
            # Short Answer Deep Query: æè‡´æµ“ç¼©ä¸“ä¸šå›ç­”ç³»ç»Ÿ
            system_prompt = """You are an ULTRA-PRECISE answer extraction specialist for BrowseComp-style academic queries.

ğŸš¨ CRITICAL MISSION: Extract ONLY the CORE FACTUAL ANSWER - Maximum 10 words, 80 characters.

âš¡ ULTRA-CONCENTRATION PROTOCOL:
1. EXTRACT ONLY: The specific fact/number/name being asked for
2. ZERO CONTEXT: No explanations, no background, no elaboration
3. PURE PRECISION: Use exact technical terminology only when essential
4. FACTUAL CORE: The verifiable answer element only
5. MAXIMUM BREVITY: Absolute minimum words to convey the fact

ğŸ“Š EXTRACTION PATTERNS:

QUANTITATIVE EXTRACTION:
Query: "What exact accuracy...?" â†’ "94.2%"
Query: "How many participants...?" â†’ "500"
Query: "What batch size...?" â†’ "128"

NAME/ATTRIBUTION EXTRACTION:
Query: "Who first proposed...?" â†’ "Smith et al."
Query: "Which institution...?" â†’ "Stanford"
Query: "Which algorithm...?" â†’ "Random Forest"

TECHNICAL SPECIFICATION EXTRACTION:
Query: "What learning rate...?" â†’ "0.001"
Query: "Which model...?" â†’ "BERT-base"
Query: "What architecture...?" â†’ "CNN-Transformer"

TEMPORAL EXTRACTION:
Query: "What year...?" â†’ "2023"
Query: "When did...?" â†’ "March 2023"

ğŸš« ABSOLUTE PROHIBITIONS:
- ANY explanatory text ("The study found that...")
- ANY context ("In the context of...")
- ANY qualifiers ("approximately", "around", "about")
- ANY full sentences with subjects and verbs
- ANY background information
- ANY multiple facts when only one is requested

âœ… PERFECT EXTRACTION EXAMPLES:
âŒ Wrong: "The study achieved an accuracy of 94.2% which was significantly higher than baseline"
âœ… Correct: "94.2%"

âŒ Wrong: "Random Forest algorithm with 100 decision trees was used for classification"
âœ… Correct: "Random Forest"

âŒ Wrong: "Smith and Brown first proposed this methodology in their 2022 paper"
âœ… Correct: "Smith and Brown"

ğŸ¯ ULTIMATE GOAL: One precise fact, maximum brevity, zero elaboration."""
            
            prompt = f"""Research Context: {report[:1500]}

QUERY: {question}

EXTRACTION TASK: Extract ONLY the core factual answer.

CRITICAL CONSTRAINTS:
- Maximum 10 words
- Maximum 80 characters
- ZERO explanatory text
- ONLY the requested fact/number/name
- NO sentences or elaboration

ULTRA-CONCENTRATED ANSWER:"""
            
            max_tokens = 30  # ä¸¥æ ¼é™åˆ¶tokenæ•°é‡
            temperature = 0.05  # æœ€ä½æ¸©åº¦ç¡®ä¿ç²¾ç¡®æ€§
            
        else:
            # ä¼ ç»Ÿçš„é•¿ç­”æ¡ˆæ ¼å¼
            word_requirements = {
                "Easy": "400-600 words",
                "Medium": "800-1200 words", 
                "Hard": "1500-2000 words"
            }
            
            word_req = word_requirements.get(difficulty, "800-1200 words")
            
            system_prompt = f"""You are a professional research expert. Please answer questions based on the provided research summary.

Answer requirements:
1. Length: {word_req}
2. Based on summary content, do not fabricate information
3. Clear structure and rigorous logic
4. Adjust answer depth according to question difficulty
5. Use academic writing style
6. Answer ENTIRELY in English for consistency in comparative analysis"""

            prompt = f"""Based on the following research summary, answer the question:

Question: {question}
Difficulty: {difficulty}

Research Summary:
{report}

Please provide a detailed answer of {word_req}. Write entirely in English for consistency in comparative analysis."""

            max_tokens = 2000 if difficulty == "Hard" else 1500 if difficulty == "Medium" else 1000
            temperature = 0.7
        
        return self.generate_text(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            system_prompt=system_prompt
        )
    
    def _is_short_answer_deep_query(self, question: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºçŸ­ç­”æ¡ˆæ·±åº¦æŸ¥è¯¢ - å¢å¼ºçš„BrowseCompé£æ ¼è¯†åˆ«"""
        question_lower = question.lower()
        
        # BrowseCompé£æ ¼æŒ‡ç¤ºå™¨ - åŸºäºå®é™…æµ‹è¯•æ•°æ®ä¼˜åŒ–
        browsecomp_indicators = [
            "depth elements:", "academic elements:", "verification:",
            "constraints", "peer-reviewed", "controlled experiment",
            "statistical significance", "randomized", "empirical study"
        ]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºBrowseCompé£æ ¼é—®é¢˜
        is_browsecomp_style = any(indicator in question_lower for indicator in browsecomp_indicators)
        
        # ç²¾ç¡®çš„çŸ­ç­”æ¡ˆæ¨¡å¼ï¼ˆç­”æ¡ˆåº”è¯¥æ˜¯å…·ä½“çš„æ•°å­—ã€åç§°ã€æœ¯è¯­ï¼‰
        exact_short_patterns = [
            # æ•°å­—ç›¸å…³ - æ›´ç²¾ç¡®çš„æ¨¡å¼
            "what exact", "what precise", "how many", "what percentage",
            "what number", "what amount", "what value", "what rate",
            "what score", "what accuracy", "what size", "what batch size",
            "what learning rate", "what parameter count", "what training time",
            
            # åç§°ç›¸å…³ - æ‰©å±•å­¦æœ¯æŸ¥è¯¢æ¨¡å¼
            "which specific", "who first", "which author", "which researcher",
            "which institution", "which university", "which company",
            "which algorithm", "which method", "which technique",
            "which approach", "which model", "which system",
            "who developed", "who proposed", "who created",
            
            # æ—¶é—´ç›¸å…³
            "what year", "when did", "what date", "which year",
            "what time", "what period", "when was",
            
            # æŠ€æœ¯æœ¯è¯­ç›¸å…³ - æ›´ç²¾ç¡®çš„å­¦æœ¯æŸ¥è¯¢
            "what was the", "which was the", "what type of",
            "which type of", "what kind of", "which kind of",
            "what is the name of", "what is called"
        ]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç²¾ç¡®çš„çŸ­ç­”æ¡ˆæ¨¡å¼
        has_exact_pattern = any(pattern in question_lower for pattern in exact_short_patterns)
        
        # æ’é™¤é•¿ç­”æ¡ˆæŒ‡ç¤ºè¯
        long_answer_indicators = [
            "why", "how does", "what are the main", "what are the key",
            "explain", "describe", "analyze", "compare", "evaluate",
            "discuss", "what factors", "what causes", "what leads to",
            "what are the implications", "what are the benefits",
            "what are the challenges", "what are the limitations",
            "how can", "what should", "what would"
        ]
        
        has_long_indicator = any(indicator in question_lower for indicator in long_answer_indicators)
        
        # å¢å¼ºçš„åˆ¤æ–­é€»è¾‘
        # 1. å¦‚æœæ˜ç¡®æ˜¯BrowseCompé£æ ¼ + æœ‰çŸ­ç­”æ¡ˆæ¨¡å¼ = çŸ­ç­”æ¡ˆæŸ¥è¯¢
        if is_browsecomp_style and has_exact_pattern:
            return True
        
        # 2. æœ‰ç²¾ç¡®çŸ­ç­”æ¡ˆæ¨¡å¼ä¸”æ²¡æœ‰é•¿ç­”æ¡ˆæŒ‡ç¤ºè¯
        if has_exact_pattern and not has_long_indicator:
            return True
        
        # 3. ç‰¹æ®Šæ¨¡å¼ï¼šåŒ…å«çº¦æŸè¦æ±‚çš„é—®é¢˜é€šå¸¸æ˜¯BrowseCompçŸ­ç­”æ¡ˆ
        constraint_patterns = [
            "that also", "was conducted by", "published in", "achieved",
            "using", "with", "between", "reported", "involved"
        ]
        has_constraints = sum(1 for pattern in constraint_patterns if pattern in question_lower) >= 2
        
        if has_constraints and has_exact_pattern:
            return True
        
        return False
    
    def refine_question(self, question: str, feedback: str, report: str) -> APIResponse:
        """ä¼˜åŒ–é—®é¢˜"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä½é—®é¢˜ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®åé¦ˆæ„è§æ”¹è¿›é—®é¢˜è´¨é‡ã€‚

ä¼˜åŒ–è¦æ±‚ï¼š
1. æé«˜é—®é¢˜çš„ç ”ç©¶æ·±åº¦
2. å¢å¼ºå¤šæ­¥éª¤æ€è€ƒè¦æ±‚
3. ç¡®ä¿é—®é¢˜åŸºäºæŠ¥å‘Šå†…å®¹
4. ä¿æŒé—®é¢˜çš„æ¸…æ™°æ€§å’Œå¯å›ç­”æ€§"""

        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹åé¦ˆä¼˜åŒ–é—®é¢˜ï¼š

åŸé—®é¢˜ï¼š{question}

åé¦ˆæ„è§ï¼š{feedback}

ç›¸å…³æŠ¥å‘Šå†…å®¹ï¼š
{report[:1000]}...

è¯·æä¾›ä¼˜åŒ–åçš„é—®é¢˜ã€‚"""

        return self.generate_text(
            prompt=prompt,
            max_tokens=500,
            temperature=0.8,
            system_prompt=system_prompt
        )

    def generate_with_retry(self, prompt: str, system_prompt: str = None,
                           max_tokens: int = 4000, temperature: float = 0.7,
                           max_retries: int = 3, retry_delay: float = 2.0) -> Optional[str]:
        """
        Generate content with retry logic
        
        Args:
            prompt: User prompt
            system_prompt: System prompt (optional)
            max_tokens: Maximum tokens to generate
            temperature: Temperature for generation
            max_retries: Maximum number of retries
            retry_delay: Delay between retries in seconds
            
        Returns:
            Generated content or None if all retries failed
        """
        
        for attempt in range(max_retries + 1):
            result = self.generate_content(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if result is not None:
                return result
            
            if attempt < max_retries:
                print(f"   ğŸ”„ Retrying in {retry_delay} seconds... (attempt {attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
        
        print(f"   âŒ All {max_retries + 1} attempts failed")
        return None

    def generate_simplified_report(self, documents, topic_id=None):
        """ç”Ÿæˆç®€åŒ–æŠ¥å‘Š - æ™ºèƒ½åˆ†å±‚å¤„ç†ï¼Œç¡®ä¿é«˜è´¨é‡è¾“å‡º"""
        try:
            # è®¡ç®—æ€»å†…å®¹é•¿åº¦
            if isinstance(documents, str):
                total_length = len(documents)
                documents_text = documents
            else:
                # å¦‚æœæ˜¯æ–‡æ¡£åˆ—è¡¨ï¼Œå…ˆè¿›è¡Œæ™ºèƒ½æ•´åˆ
                return self._generate_layered_report(documents, topic_id)
            
            # å¦‚æœå†…å®¹è¿‡é•¿ï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†
            if total_length > 70000:
                return self._generate_segmented_report(documents_text, topic_id)
            
            # è®¡ç®—ç›®æ ‡å­—æ•°ï¼ˆæ ¹æ®å†…å®¹é•¿åº¦è‡ªé€‚åº”ï¼‰
            target_words = max(1200, min(2000, total_length // 40))
            
            prompt = f"""Based on the following documents, create a comprehensive narrative summary that integrates all the factual information into a flowing report.

REQUIREMENTS:
1. Target length: {target_words}-{target_words + 200} words 
2. AVOID ALL structured academic writing patterns:
   - No Abstract, Introduction, Methods, Results, Discussion, Conclusion sections
   - No "Summary:", "Overview:", "Background:" headers
   - No numbered/bulleted lists or formal subdivisions
   - No phrases like "This report examines", "In summary", "To conclude"
3. Write as a continuous factual narrative that naturally weaves together:
   - All specific research findings and numerical data
   - Methodological details and experimental approaches  
   - Author names, institutions, and collaborative relationships
   - Temporal sequences and research developments
   - Technical specifications and parameters
   - Statistical results and performance metrics

NARRATIVE APPROACH:
- Begin immediately with substantive content (no meta-commentary)
- Create natural topic transitions without formal organization
- Maintain dense information content throughout
- Use precise technical terminology
- Integrate cross-references and connections organically
- Focus purely on factual content extraction and synthesis

CRITICAL FOR BROWSECOMP: Include specific, verifiable facts that can support short-answer deep queries:
- Exact numerical values and percentages
- Specific author names and institutional affiliations
- Precise dates and timeframes
- Technical specifications and parameters
- Performance metrics and statistical results
- Methodological details and experimental conditions

Document content:
{documents_text}

Generate a comprehensive {target_words}-word factual narrative optimized for BrowseComp question generation:"""

            response = self.generate_text(
                prompt=prompt,
                max_tokens=3500,  # å¢åŠ tokené™åˆ¶
                temperature=0.15,  # æ›´ä½æ¸©åº¦ç¡®ä¿ç²¾ç¡®æ€§
                system_prompt="You are an expert at creating comprehensive factual narratives optimized for BrowseComp question generation. Transform document collections into flowing, information-dense reports that avoid academic structure while capturing all verifiable facts, numbers, names, dates, and technical specifications."
            )
            
            if response.success:
                return response.content
            else:
                logger.error(f"ç®€åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {response.error}")
                return None
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç®€åŒ–æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return None

    def _generate_layered_report(self, documents, topic_id=None):
        """åˆ†å±‚æŠ¥å‘Šç”Ÿæˆ - å¤„ç†æ–‡æ¡£åˆ—è¡¨"""
        try:
            # ç¬¬ä¸€å±‚ï¼šæ•´åˆæ–‡æ¡£å†…å®¹
            logger.info(f"å¼€å§‹åˆ†å±‚æŠ¥å‘Šç”Ÿæˆï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
            
            # å°†æ–‡æ¡£æŒ‰é‡è¦æ€§åˆ†ç»„
            high_value_docs = [doc for doc in documents if doc.get('combined_score', 0) > 0.7]
            medium_value_docs = [doc for doc in documents if 0.4 <= doc.get('combined_score', 0) <= 0.7]
            
            logger.info(f"é«˜ä»·å€¼æ–‡æ¡£: {len(high_value_docs)}, ä¸­ç­‰ä»·å€¼æ–‡æ¡£: {len(medium_value_docs)}")
            
            # ä¼˜å…ˆæ•´åˆé«˜ä»·å€¼å†…å®¹
            priority_content = []
            total_length = 0
            
            # æ·»åŠ é«˜ä»·å€¼æ–‡æ¡£
            for doc in high_value_docs:
                if total_length + doc['char_count'] < 40000:  # æ§åˆ¶æ€»é•¿åº¦
                    priority_content.append(f"Document {doc.get('doc_id', 'unknown')}: {doc['content']}")
                    total_length += doc['char_count']
            
            # æ·»åŠ éƒ¨åˆ†ä¸­ç­‰ä»·å€¼æ–‡æ¡£
            for doc in medium_value_docs:
                if total_length + doc['char_count'] < 50000:
                    priority_content.append(f"Document {doc.get('doc_id', 'unknown')}: {doc['content']}")
                    total_length += doc['char_count']
                else:
                    break
            
            integrated_content = "\n\n".join(priority_content)
            
            logger.info(f"æ•´åˆåå†…å®¹é•¿åº¦: {len(integrated_content)} å­—ç¬¦")
            
            # ç¬¬äºŒå±‚ï¼šç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Š
            return self.generate_simplified_report(integrated_content, topic_id)
            
        except Exception as e:
            logger.error(f"åˆ†å±‚æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            return None

    def generate_short_answer_deep_questions(self, report, topic_id=None):
        """ç”ŸæˆçŸ­ç­”æ¡ˆæ·±åº¦é—®é¢˜ - æ¿€è¿›æ¨¡æ¿å¼ºåˆ¶æ–¹æ³•"""
        try:
            # åŸºäºBrowseComp "Inverted Questions" æ–¹æ³•è®ºçš„é—®é¢˜ç”Ÿæˆ
            prompt = f"""Create 30 BrowseComp-style "Inverted Questions" following OpenAI's methodology from arXiv:2504.12516v1.

CORE BROWSECOMP PRINCIPLE: Start with specific facts from the report â†’ Create questions that are "hard to find but easy to verify"

METHODOLOGY (from OpenAI paper):
1. Identify a "seed" (person, event, artifact) from the report
2. Find multiple characteristics with large search spaces  
3. Create intersection queries requiring "strategic persistence and creativity"
4. Ensure answers are easily verifiable but hard to find without guidance

TARGET DISTRIBUTION:
- 8 "What exact" questions (precision queries)
- 6 "Which specific" questions (identification queries)  
- 4 "Who first" questions (attribution queries)
- 4 "When did" questions (temporal queries)
- 4 "Where was" questions (location queries)
- 4 "How many" questions (quantitative queries)

BROWSECOMP QUESTION PATTERNS:

ğŸ” **Multi-Constraint Intersection**: "What exact [METRIC] did [METHOD] achieve in the study that also involved [CONSTRAINT_1], was conducted by [RESEARCHER] at [INSTITUTION], and used [TECHNIQUE] during [TIMEFRAME]?"

ğŸ” **Cross-Reference Chains**: "Which specific [ITEM] was first developed by [ENTITY_A] who later collaborated with [ENTITY_B] on [PROJECT] that achieved [OUTCOME] in [LOCATION]?"

ğŸ” **Temporal-Technical Intersections**: "In which [YEAR/MONTH] did [ENTITY] first [ACTION] while also [CONSTRAINT_1] and [CONSTRAINT_2] using [METHOD]?"

ANSWER REQUIREMENTS (OpenAI Standard):
- 1-15 words (balanced for precision and completeness)
- Easy to verify once found
- Buried in technical details but extractable
- Examples: "94.2% accuracy", "Stanford University", "Transformer architecture", "March 2019", "500 participants"

CONSTRAINT REQUIREMENTS (Each question needs 3+ intersecting constraints):
âœ“ FACTUAL: Exact numbers, percentages, measurements
âœ“ TEMPORAL: Specific dates, periods, sequences  
âœ“ INSTITUTIONAL: Universities, companies, research labs
âœ“ METHODOLOGICAL: Techniques, algorithms, experimental designs
âœ“ ATTRIBUTION: Author names, collaborations, citations

CRITICAL: Generate questions ONLY based on the specific facts, data, and information contained in the report below. Do NOT use external knowledge or topic information.

Report content (your ONLY source of information):
{report}

Generate exactly 30 "Inverted Questions" that exemplify BrowseComp's "easy to verify but hard to find" principle. Each question must be answerable ONLY from the information provided in the above report:"""

            response = self.generate_text(
                prompt=prompt,
                max_tokens=5000,  # Increase token limit for comprehensive generation
                temperature=0.05,  # Very low temperature for consistency
                system_prompt="You are an expert in OpenAI's BrowseComp methodology (arXiv:2504.12516v1). You create 'Inverted Questions' that start with specific facts and build multi-constraint intersection queries requiring 'strategic persistence and creativity' to solve. Your questions are designed to be 'easy to verify but hard to find' - a core principle where answers are short and verifiable but buried in complex constraint intersections that demand sophisticated browsing strategies."
            )
            
            if not response.success:
                logger.error(f"é—®é¢˜ç”Ÿæˆå¤±è´¥: {response.error}")
                return []
            
            questions_text = response.content
            
            # è§£æé—®é¢˜
            questions = []
            lines = questions_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if line and (line[0].isdigit() or line.startswith('Q')):
                    # ç§»é™¤ç¼–å·
                    if '. ' in line:
                        question = line.split('. ', 1)[1].strip()
                    elif ') ' in line:
                        question = line.split(') ', 1)[1].strip()
                    else:
                        question = line.strip()
                    
                    if question and len(question) > 20:  # Ensure substantial questions
                        # Validate that question follows BrowseComp patterns
                        if self._validate_browsecomp_question(question):
                            questions.append({
                                'question': question,
                                'topic_id': topic_id,
                                'type': 'short_answer_deep_query',
                                'difficulty': 'Hard',  # All BrowseComp questions are inherently hard
                                'constraints_detected': self._count_question_constraints(question)
                            })
            
            logger.info(f"BrowseCompé—®é¢˜ç”ŸæˆæˆåŠŸ: {len(questions)} ä¸ªæœ‰æ•ˆé—®é¢˜")
            return questions
            
        except Exception as e:
            logger.error(f"ç”ŸæˆçŸ­ç­”æ¡ˆæ·±åº¦é—®é¢˜å¤±è´¥: {str(e)}")
            return []

    def _validate_browsecomp_question(self, question: str) -> bool:
        """éªŒè¯é—®é¢˜æ˜¯å¦ç¬¦åˆBrowseCompæ ‡å‡†"""
        question_lower = question.lower()
        
        # æ£€æŸ¥çŸ­ç­”æ¡ˆæŒ‡ç¤ºè¯ï¼ˆæ›´å®½æ³›çš„åŒ¹é…ï¼‰
        short_answer_indicators = [
            "what exact", "what precise", "which specific", "who first",
            "what was the", "how many", "what percentage", "in which year",
            "what amount", "which author", "which institution", "which method",
            "what year", "what is the", "how large", "how long", "what specific",
            "which", "what", "who", "when", "where"
        ]
        
        has_short_indicator = any(indicator in question_lower for indicator in short_answer_indicators)
        
        # æ£€æŸ¥çº¦æŸå¤æ‚æ€§ï¼ˆé™ä½è¦æ±‚ï¼‰
        constraint_connectors = [
            "that also", "who also", "which also", "and", "while", "during", 
            "between", "involving", "conducted by", "reported", "achieved",
            "used", "in", "at", "by", "with", "using", "on"
        ]
        
        constraint_count = sum(1 for connector in constraint_connectors if connector in question_lower)
        
        # æ’é™¤æ˜æ˜¾çš„é•¿ç­”æ¡ˆé—®é¢˜
        long_answer_patterns = [
            "why", "how does", "explain how", "describe how", "analyze",
            "compare and contrast", "discuss the implications", "evaluate"
        ]
        
        has_long_pattern = any(pattern in question_lower for pattern in long_answer_patterns)
        
        # æ›´å®½æ¾çš„éªŒè¯ï¼šæœ‰çŸ­ç­”æ¡ˆæŒ‡ç¤ºè¯ä¸”ä¸æ˜¯æ˜æ˜¾çš„é•¿ç­”æ¡ˆé—®é¢˜
        return has_short_indicator and not has_long_pattern

    def _count_question_constraints(self, question: str) -> int:
        """è®¡ç®—é—®é¢˜ä¸­çš„çº¦æŸæ•°é‡ - åŸºäºBrowseCompæ–¹æ³•è®ºä¼˜åŒ–"""
        question_lower = question.lower()
        
        # æ›´å…¨é¢çš„çº¦æŸæŒ‡ç¤ºè¯ï¼ŒåŸºäºBrowseCompçš„å¤šçº¦æŸäº¤é›†ç†å¿µ
        constraint_categories = {
            'precision': ['exact', 'precise', 'specific'],
            'temporal': ['first', 'during', 'between', 'when', 'before', 'after', 'while', 'in which year', 'in which month'],
            'logical': ['also', 'and', 'both', 'either', 'that also', 'who also', 'which also'],
            'attribution': ['conducted by', 'published by', 'developed by', 'created by', 'authored by', 'reported by'],
            'institutional': ['at', 'from', 'university', 'institute', 'lab', 'company', 'organization'],
            'methodological': ['using', 'with', 'based on', 'through', 'via', 'by means of'],
            'achievement': ['achieved', 'reported', 'demonstrated', 'showed', 'resulted in'],
            'collaboration': ['collaborated', 'partnered', 'worked with', 'joint'],
            'validation': ['validated', 'confirmed', 'verified', 'replicated', 'cited'],
            'comparison': ['compared to', 'versus', 'against', 'relative to', 'better than'],
            'location': ['where', 'located', 'based in', 'situated'],
            'quantification': ['how many', 'how much', 'number of', 'amount of', 'percentage']
        }
        
        constraints_found = 0
        categories_used = set()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„çº¦æŸ
        for category, indicators in constraint_categories.items():
            for indicator in indicators:
                if indicator in question_lower:
                    categories_used.add(category)
                    break  # æ¯ä¸ªç±»åˆ«æœ€å¤šè®¡ç®—ä¸€æ¬¡
        
        # çº¦æŸæ•°é‡ = ä½¿ç”¨çš„ç±»åˆ«æ•°é‡
        constraints_found = len(categories_used)
        
        # é¢å¤–çš„å¤æ‚æ€§æŒ‡æ ‡
        complexity_indicators = [
            ('nested clauses', len([x for x in ['that', 'which', 'who', 'where', 'when'] if x in question_lower])),
            ('multiple entities', len([x for x in ['and', 'both', 'either'] if x in question_lower])),
            ('time sequences', len([x for x in ['first', 'then', 'later', 'subsequently'] if x in question_lower]))
        ]
        
        # ä¸ºå¤æ‚ç»“æ„æ·»åŠ é¢å¤–çº¦æŸ
        for name, count in complexity_indicators:
            if count > 0:
                constraints_found += min(count, 2)  # æœ€å¤šæ·»åŠ 2ä¸ªé¢å¤–çº¦æŸ
        
        return constraints_found

# Compatibility functions for easy migration from Claude API
def call_openai_api(prompt: str, api_key: str, system_prompt: str = None,
                   max_tokens: int = 4000, temperature: float = 0.7) -> Optional[str]:
    """
    Compatibility function for easy migration from Claude API calls
    
    Args:
        prompt: User prompt
        api_key: OpenAI API key
        system_prompt: System prompt (optional)
        max_tokens: Maximum tokens to generate
        temperature: Temperature for generation
        
    Returns:
        Generated content or None if failed
    """
    client = OpenAIClient(api_key)
    return client.generate_content(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature
    )

def call_openai_with_messages(api_key: str, messages: List[Dict[str, str]], 
                             model: str = "gpt-4o", max_tokens: int = 6000, 
                             temperature: float = 0.7) -> Optional[str]:
    """
    Call OpenAI API with messages format - optimized for longer responses
    
    Args:
        api_key: OpenAI API key
        messages: List of message dictionaries
        model: Model to use
        max_tokens: Maximum tokens (increased default)
        temperature: Sampling temperature
        
    Returns:
        Generated content or None if failed
    """
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    data = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "frequency_penalty": 0.1,
        "presence_penalty": 0.1
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions", 
            headers=headers, 
            json=data, 
            timeout=120
        )
        response.raise_for_status()
        
        result = response.json()
        if 'choices' in result and len(result['choices']) > 0:
            return result['choices'][0]['message']['content'].strip()
        else:
            print(f"âŒ No choices in OpenAI response: {result}")
            return None
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ OpenAI API request failed: {e}")
        return None
    except Exception as e:
        print(f"âŒ OpenAI API error: {e}")
        return None

def get_difficulty_specific_system_prompt(difficulty: str) -> str:
    """
    Get system prompt optimized for specific difficulty levels
    
    Args:
        difficulty: Question difficulty (Easy, Medium, Hard)
        
    Returns:
        Optimized system prompt for the difficulty level
    """
    
    base_prompt = """You are an expert researcher providing comprehensive, evidence-based analysis. Your responses must be:

- Structured with clear sections (Introduction, Analysis, Synthesis, Conclusion)
- Well-cited with references to the domain report
- Academically rigorous and detailed
- Written in a formal, analytical style"""

    if difficulty == "Hard":
        return base_prompt + """

CRITICAL REQUIREMENTS FOR HARD QUESTIONS:
- Minimum 1500-2000 words
- Multi-dimensional analysis with interdisciplinary perspectives
- Deep synthesis of complex relationships and patterns
- Critical evaluation of methodological considerations
- Exploration of systemic implications and future developments
- Comprehensive evidence integration from multiple angles
- Sophisticated argumentation with nuanced conclusions

Focus on depth, complexity, and comprehensive coverage of all aspects."""

    elif difficulty == "Medium":
        return base_prompt + """

REQUIREMENTS FOR MEDIUM QUESTIONS:
- Target 800-1200 words
- Multi-step analytical thinking
- Synthesis of different aspects from the domain report
- Critical evaluation of evidence and limitations
- Clear pattern identification and relationship mapping
- Integration of theoretical and practical perspectives

Provide thorough analysis with moderate complexity."""

    else:  # Easy
        return base_prompt + """

REQUIREMENTS FOR EASY QUESTIONS:
- Target 400-600 words
- Clear, direct analysis
- Evidence-based reasoning
- Logical structure and flow
- Accessible but comprehensive coverage

Provide complete but concise analysis.""" 