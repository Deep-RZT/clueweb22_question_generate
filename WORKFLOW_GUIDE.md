# ğŸ“˜ ClueWeb22 Question Generation - Complete Workflow Guide

## ğŸ¯ Project Overview

This project provides multiple experimental approaches for generating high-quality research questions and answers from ClueWeb22 datasets and academic literature. It has been reorganized into a clean, modular structure for easy use and extension.

## ğŸ—ï¸ Project Structure

```
clueweb22_question_generate/
â”œâ”€â”€ ğŸ“„ README.md                      # Project overview
â”œâ”€â”€ ğŸ“„ WORKFLOW_GUIDE.md              # This comprehensive guide
â”œâ”€â”€ ğŸ“„ PROJECT_REORGANIZATION_PLAN.md # Reorganization documentation
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ experiments/                   # ğŸš€ All experimental approaches
â”‚   â”œâ”€â”€ 01_prompt_only/              # Pure prompt-based generation
â”‚   â”œâ”€â”€ 02_rag_approach/             # RAG with knowledge corpus
â”‚   â”œâ”€â”€ 03_hybrid_prompt_rag/        # PROMPT+RAG combination
â”‚   â”œâ”€â”€ 04_multi_stage/              # Multi-stage deep thinking
â”‚   â”œâ”€â”€ 05_comparative/              # ğŸ† RECOMMENDED APPROACH
â”‚   â”œâ”€â”€ 06_short_answer_deep_query/  # Short answer focused generation
â”‚   â””â”€â”€ 07_tree_extension_deep_query/ # ğŸ§  AGENT REASONING FRAMEWORK
â”‚
â”œâ”€â”€ ğŸ“ core/                         # Shared components
â”‚   â”œâ”€â”€ llm_clients/                 # API clients (OpenAI, Claude)
â”‚   â”œâ”€â”€ data_processing/             # Data utilities
â”‚   â”œâ”€â”€ evaluation/                  # Quality assessment
â”‚   â””â”€â”€ output/                      # Export formatters
â”‚
â”œâ”€â”€ ğŸ“ data/                         # All data sources
â”‚   â”œâ”€â”€ clueweb22/                   # ClueWeb22 query results
â”‚   â”œâ”€â”€ academic_papers/             # Academic literature
â”‚   â””â”€â”€ rag_corpus/                  # RAG knowledge base
â”‚
â”œâ”€â”€ ğŸ“ results/                      # Experimental outputs
â””â”€â”€ ğŸ“ archived/                     # Historical versions
```

## ğŸŒŸ Quick Start - Recommended Approaches

### ğŸ§  For AI Agent Testing (NEW)

**Latest and most advanced approach:**

```bash
# 1. Navigate to Agent Reasoning Framework
cd experiments/07_tree_extension_deep_query

# 2. Run the Agent deep reasoning test generator
python main.py
```

**Why this approach?**
- ğŸš€ **Cutting-edge**: Latest Agent reasoning test framework
- ğŸ›¡ï¸ **Anti-masking**: Prevents direct answer exposure
- ğŸ“Š **Production-ready**: Automated quality assurance
- ğŸ” **Real-time**: OpenAI Web Search integration

### ğŸ† For General Question Generation

**Traditional recommended approach:**

```bash
# 1. Navigate to the comparative approach
cd experiments/05_comparative

# 2. Run test mode (fast validation)
python four_way_comparative_experiment.py test

# 3. Run full experiment (production quality)
python four_way_comparative_experiment.py
```

**Why this approach?**
- âœ… **Battle-tested**: 100% success rate across experiments
- âœ… **Comprehensive**: Multiple models and datasets
- âœ… **Production-ready**: Robust error handling
- âœ… **Well-documented**: Complete analysis and reports

## ğŸ”¬ Experimental Approaches

### 1. ğŸ† Comparative Experiments (RECOMMENDED)

**Location**: `experiments/05_comparative/`
**Best for**: Production use, research benchmarks, model comparison

**Features**:
- Four-way comparison (2 models Ã— 2 datasets)
- Comprehensive quality analysis
- Multiple output formats
- Automatic retry and error handling

**Usage**:
```bash
cd experiments/05_comparative
python four_way_comparative_experiment.py
```

### 2. ğŸ“ PROMPT-Only Approach

**Location**: `experiments/01_prompt_only/`
**Best for**: Quick prototyping, self-contained generation

**Features**:
- Pure prompt-based generation
- Fast processing
- No external dependencies
- Simple setup

**Usage**:
```bash
cd experiments/01_prompt_only
python prompt_generator.py
```

### 3. ğŸ” RAG Approach

**Location**: `experiments/02_rag_approach/`
**Best for**: Knowledge-grounded generation, fact verification

**Features**:
- External knowledge integration
- Factual accuracy
- Literature-based answers
- Citation support

**Usage**:
```bash
cd experiments/02_rag_approach
python rag_generator.py
```

### 4. ğŸ”„ Hybrid PROMPT+RAG

**Location**: `experiments/03_hybrid_prompt_rag/`
**Best for**: Best of both approaches

**Features**:
- PROMPT for questions
- RAG for answers
- Balanced approach
- Quality optimization

**Usage**:
```bash
cd experiments/03_hybrid_prompt_rag
python hybrid_generator.py
```

### 5. ğŸ§  Multi-Stage Generation

**Location**: `experiments/04_multi_stage/`
**Best for**: Deep thinking processes, iterative refinement

**Features**:
- Multi-step generation
- Quality refinement
- Deep thinking simulation
- Sophisticated prompting

**Usage**:
```bash
cd experiments/04_multi_stage
python main.py
```

### 6. ğŸ§  Agent Deep Reasoning Framework (LATEST)

**Location**: `experiments/07_tree_extension_deep_query/`
**Best for**: AI Agent reasoning test, preventing direct answers

**Features**:
- **6-Step Agent Design Flow**: Complete pipeline from short answers to composite questions
- **Root Answer Exposure Protection**: Intelligent detection and prevention of answer leakage
- **Pure Objective Q&A**: Elimination of LLM thinking process descriptions
- **Real Web Search Integration**: OpenAI official API, no mock data pollution
- **Tree Extension Structure**: Multi-level question dependency relationships
- **4-Layer Quality Verification**: Comprehensive validation system

**Usage**:
```bash
cd experiments/07_tree_extension_deep_query
python main.py
```

**Key Innovation**:
- Generates questions that **cannot be directly answered** by normal LLMs
- Requires **Agent step-by-step reasoning** to solve
- **Prevents masking**: Ensures root answers are not exposed during reasoning
- **Production-ready**: Automated generation with quality guarantee

## âš™ï¸ Configuration & Setup

### API Keys Setup

Create a `.env` file in the project root:
```bash
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_claude_key_here
```

### Dependencies

```bash
pip install requests pandas openpyxl anthropic openai
```

### Data Sources

The project uses two main data sources:
1. **ClueWeb22**: Web-crawled multilingual documents
2. **Academic Papers**: Research literature corpus

Data is automatically loaded from `data/` directory.

## ğŸ“Š Core Components

### LLM Clients (`core/llm_clients/`)

- `openai_api_client.py` - OpenAI GPT-4o integration
- `claude_api_client.py` - Claude Sonnet 4 integration  
- `llm_manager.py` - Unified LLM management

### Data Processing (`core/data_processing/`)

- `utils.py` - JSON parsing and data utilities
- `convert_clueweb22_data.py` - ClueWeb22 data converter

### Evaluation (`core/evaluation/`)

- `deep_research_evaluation_framework.py` - Quality assessment
- `question_refinement_system.py` - Question optimization

### Output (`core/output/`)

- `export_to_excel.py` - Excel export functionality
- Multiple format support (JSON, Excel, Markdown)

## ğŸ¯ Workflow Details

### Standard Pipeline

1. **Data Loading**
   ```python
   # Load ClueWeb22 topics
   topics = load_clueweb_topics()
   
   # Load academic papers
   papers = load_academic_papers()
   ```

2. **Report Generation**
   ```python
   # Generate domain-specific reports
   report = generate_domain_report(documents, topic_id)
   ```

3. **Question Generation**
   ```python
   # Generate research questions
   questions = generate_questions(report, num_questions=50)
   ```

4. **Answer Generation**
   ```python
   # Generate comprehensive answers
   answers = generate_answers(questions, report)
   ```

5. **Export & Analysis**
   ```python
   # Export results
   export_to_excel(results, filename)
   export_to_json(results, filename)
   ```

### Quality Control

- **Automatic retry mechanisms**
- **Error handling and fallbacks**
- **Quality scoring and filtering**
- **Format validation**

## ğŸ“ˆ Performance Metrics

Based on comprehensive testing:

| Approach | Speed | Quality | Reliability | Use Case |
|----------|-------|---------|-------------|----------|
| **Agent Reasoning (07)** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | **Agent Testing** |
| Comparative | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | Production |
| PROMPT-only | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | Prototyping |
| RAG | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | Research |
| Hybrid | â­â­â­ | â­â­â­â­ | â­â­â­â­ | Balanced |
| Multi-stage | â­â­ | â­â­â­â­â­ | â­â­â­ | Deep analysis |

## ğŸ”§ Customization

### Adding New Models

```python
# In core/llm_clients/
class NewModelClient:
    def __init__(self, api_key):
        self.api_key = api_key
    
    def generate_content(self, prompt, **kwargs):
        # Implementation
        pass
```

### Adding New Data Sources

```python
# In core/data_processing/
def load_new_data_source(path):
    # Implementation
    return processed_data
```

### Custom Evaluation Metrics

```python
# In core/evaluation/
def custom_quality_metric(question, answer):
    # Implementation
    return score
```

## ğŸ› Troubleshooting

### Common Issues

1. **API Rate Limits**
   - Solution: Built-in rate limiting and retry logic
   - Adjust delays in configuration

2. **Memory Issues**
   - Solution: Process topics in batches
   - Reduce document count per topic

3. **Import Errors**
   - Solution: Check Python path and imports
   - Ensure core components are accessible

4. **Data Loading Errors**
   - Solution: Verify data directory structure
   - Check file permissions and encoding

### Debug Mode

```python
# Enable detailed logging
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“‹ Best Practices

### For Research
- Use **Comparative Experiments** for reproducible results
- Document all configuration parameters
- Save intermediate results for analysis
- Validate with multiple topics

### For Production
- Start with **test mode** for validation
- Use error handling and retry mechanisms
- Monitor API usage and costs
- Implement quality checks

### For Development
- Follow the modular structure
- Add comprehensive documentation
- Include example usage
- Write unit tests for new components

## ğŸ“š Documentation

- Each experiment has its own README
- Core components are well-documented
- Configuration options are explained
- Examples are provided throughout

## ğŸš€ Future Extensions

The modular structure supports:
- New experimental approaches
- Additional LLM providers
- Extended evaluation metrics
- Custom data sources
- Advanced analysis tools

---

## ğŸ“ Support

For questions or issues:
1. Check the specific experiment README
2. Review troubleshooting section
3. Examine example outputs
4. Test with simplified configurations

---

*This guide provides comprehensive coverage of all experimental approaches. Start with the recommended comparative framework for best results.* 