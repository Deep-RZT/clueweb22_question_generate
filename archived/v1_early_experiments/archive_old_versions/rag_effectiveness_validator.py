#!/usr/bin/env python3
"""
RAG Effectiveness Validation System
ç³»ç»Ÿæ€§éªŒè¯RAGèåˆçš„æœ‰æ•ˆæ€§å’Œæ”¹è¿›æ•ˆæœ
"""

import json
import sys
import os
import time
import random
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime
import requests
import pandas as pd
from collections import defaultdict
import re

# Add paths
sys.path.append('PROMPT')
sys.path.append('RAG')

class RAGEffectivenessValidator:
    """RAGæœ‰æ•ˆæ€§éªŒè¯å™¨"""
    
    def __init__(self, claude_api_key: str):
        self.claude_api_key = claude_api_key
        self.claude_api_url = "https://api.anthropic.com/v1/messages"
        self.headers = {
            "Content-Type": "application/json",
            "x-api-key": claude_api_key,
            "anthropic-version": "2023-06-01"
        }
        
        # åŠ è½½RAGç³»ç»Ÿ
        from enhanced_generation_system import EnhancedEnergyGenerator
        self.rag_generator = EnhancedEnergyGenerator(claude_api_key)
        
        print(f"âœ… RAGéªŒè¯å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def call_claude_api(self, messages: List[Dict], max_tokens: int = 2000) -> str:
        """è°ƒç”¨Claude API"""
        payload = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": max_tokens,
            "messages": messages
        }
        
        try:
            response = requests.post(
                self.claude_api_url,
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['content'][0]['text']
            else:
                print(f"âŒ API Error {response.status_code}: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ API Call failed: {e}")
            return None
    
    def generate_baseline_answer(self, question: str) -> str:
        """ç”ŸæˆåŸºçº¿ç­”æ¡ˆï¼ˆæ— RAGæ”¯æ’‘ï¼‰"""
        messages = [
            {
                "role": "user",
                "content": f"""Please provide a comprehensive answer to the following energy domain question:

Question: {question}

Provide a detailed, research-grade analysis covering:
- Overview of the topic
- Key technical aspects
- Current challenges and opportunities
- Future directions

Answer length: 800-1200 words."""
            }
        ]
        
        return self.call_claude_api(messages, max_tokens=2000)
    
    def validate_literature_retrieval_accuracy(self, test_questions: List[str]) -> Dict[str, Any]:
        """éªŒè¯æ–‡çŒ®æ£€ç´¢å‡†ç¡®æ€§"""
        
        print("ğŸ” éªŒè¯æ–‡çŒ®æ£€ç´¢å‡†ç¡®æ€§...")
        
        retrieval_results = {
            'total_questions': len(test_questions),
            'successful_retrievals': 0,
            'average_relevance_score': 0.0,
            'coverage_by_subdomain': defaultdict(int),
            'detailed_results': []
        }
        
        total_relevance = 0
        
        for i, question in enumerate(test_questions, 1):
            print(f"  æ£€ç´¢æµ‹è¯• {i}/{len(test_questions)}: {question[:50]}...")
            
            # æ£€ç´¢ç›¸å…³è®ºæ–‡
            relevant_papers = self.rag_generator.retrieve_relevant_papers(question, top_k=5)
            
            if relevant_papers:
                retrieval_results['successful_retrievals'] += 1
                
                # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆåŸºäºå…³é”®è¯é‡å ï¼‰
                question_words = set(question.lower().split())
                paper_relevance_scores = []
                
                for paper in relevant_papers:
                    paper_text = f"{paper['title']} {paper['abstract']}".lower()
                    paper_words = set(paper_text.split())
                    
                    # Jaccardç›¸ä¼¼åº¦
                    intersection = question_words.intersection(paper_words)
                    union = question_words.union(paper_words)
                    relevance = len(intersection) / len(union) if union else 0
                    paper_relevance_scores.append(relevance)
                
                avg_relevance = sum(paper_relevance_scores) / len(paper_relevance_scores)
                total_relevance += avg_relevance
                
                # è®°å½•è¯¦ç»†ç»“æœ
                retrieval_results['detailed_results'].append({
                    'question': question,
                    'papers_found': len(relevant_papers),
                    'average_relevance': avg_relevance,
                    'top_paper_title': relevant_papers[0]['title'] if relevant_papers else None
                })
                
                # ç»Ÿè®¡å­é¢†åŸŸè¦†ç›–
                for paper in relevant_papers:
                    for keyword in paper.get('keywords', []):
                        retrieval_results['coverage_by_subdomain'][keyword] += 1
        
        if retrieval_results['successful_retrievals'] > 0:
            retrieval_results['average_relevance_score'] = total_relevance / retrieval_results['successful_retrievals']
        
        retrieval_results['success_rate'] = retrieval_results['successful_retrievals'] / retrieval_results['total_questions']
        
        return retrieval_results
    
    def compare_answer_quality(self, test_questions: List[str]) -> Dict[str, Any]:
        """å¯¹æ¯”RAGç­”æ¡ˆä¸åŸºçº¿ç­”æ¡ˆçš„è´¨é‡"""
        
        print("ğŸ“Š å¯¹æ¯”ç­”æ¡ˆè´¨é‡...")
        
        comparison_results = {
            'total_comparisons': len(test_questions),
            'rag_wins': 0,
            'baseline_wins': 0,
            'ties': 0,
            'quality_metrics': {
                'rag_avg_citations': 0,
                'baseline_avg_citations': 0,
                'rag_avg_word_count': 0,
                'baseline_avg_word_count': 0,
                'rag_factual_accuracy': 0,
                'baseline_factual_accuracy': 0
            },
            'detailed_comparisons': []
        }
        
        total_rag_citations = 0
        total_baseline_citations = 0
        total_rag_words = 0
        total_baseline_words = 0
        
        for i, question in enumerate(test_questions, 1):
            print(f"  è´¨é‡å¯¹æ¯” {i}/{len(test_questions)}: {question[:50]}...")
            
            # ç”ŸæˆRAGç­”æ¡ˆ
            relevant_papers = self.rag_generator.retrieve_relevant_papers(question, top_k=5)
            rag_result = self.rag_generator.generate_rag_fused_answer(question, relevant_papers)
            rag_answer = rag_result['answer']
            
            # ç”ŸæˆåŸºçº¿ç­”æ¡ˆ
            baseline_answer = self.generate_baseline_answer(question)
            
            if rag_answer and baseline_answer:
                # åˆ†æå¼•ç”¨æ•°é‡
                rag_citations = len(rag_result.get('sources', []))
                baseline_citations = self.count_citations(baseline_answer)
                
                total_rag_citations += rag_citations
                total_baseline_citations += baseline_citations
                
                # åˆ†æè¯æ•°
                rag_words = len(rag_answer.split())
                baseline_words = len(baseline_answer.split())
                
                total_rag_words += rag_words
                total_baseline_words += baseline_words
                
                # ä½¿ç”¨Claudeè¯„ä¼°è´¨é‡
                quality_comparison = self.evaluate_answer_quality(question, rag_answer, baseline_answer)
                
                if quality_comparison:
                    if 'rag' in quality_comparison.lower():
                        comparison_results['rag_wins'] += 1
                    elif 'baseline' in quality_comparison.lower():
                        comparison_results['baseline_wins'] += 1
                    else:
                        comparison_results['ties'] += 1
                
                # è®°å½•è¯¦ç»†å¯¹æ¯”
                comparison_results['detailed_comparisons'].append({
                    'question': question,
                    'rag_citations': rag_citations,
                    'baseline_citations': baseline_citations,
                    'rag_words': rag_words,
                    'baseline_words': baseline_words,
                    'quality_assessment': quality_comparison
                })
            
            # é™åˆ¶APIè°ƒç”¨é¢‘ç‡
            time.sleep(1)
        
        # è®¡ç®—å¹³å‡å€¼
        if len(test_questions) > 0:
            comparison_results['quality_metrics']['rag_avg_citations'] = total_rag_citations / len(test_questions)
            comparison_results['quality_metrics']['baseline_avg_citations'] = total_baseline_citations / len(test_questions)
            comparison_results['quality_metrics']['rag_avg_word_count'] = total_rag_words / len(test_questions)
            comparison_results['quality_metrics']['baseline_avg_word_count'] = total_baseline_words / len(test_questions)
        
        return comparison_results
    
    def count_citations(self, text: str) -> int:
        """ç»Ÿè®¡æ–‡æœ¬ä¸­çš„å¼•ç”¨æ•°é‡"""
        citation_patterns = [
            r'\([^)]*\d{4}[^)]*\)',  # (Author, 2024)
            r'\[[^\]]*\d+[^\]]*\]',  # [1], [Author 2024]
            r'et al\.',              # et al.
            r'doi:',                 # doi:
            r'http[s]?://',          # URLs
            r'www\.',                # www.
        ]
        
        total_citations = 0
        for pattern in citation_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            total_citations += len(matches)
        
        return total_citations
    
    def evaluate_answer_quality(self, question: str, rag_answer: str, baseline_answer: str) -> str:
        """ä½¿ç”¨Claudeè¯„ä¼°ä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Please evaluate the quality of two answers to the same energy domain question. Consider factual accuracy, depth of analysis, use of evidence, and overall helpfulness.

Question: {question}

Answer A (RAG-based): {rag_answer[:1000]}...

Answer B (Baseline): {baseline_answer[:1000]}...

Which answer is better and why? Respond with either "Answer A (RAG)" or "Answer B (Baseline)" followed by a brief explanation focusing on:
1. Factual accuracy and evidence
2. Depth and comprehensiveness
3. Specific citations and sources
4. Overall research value

Keep your response concise (2-3 sentences)."""
            }
        ]
        
        return self.call_claude_api(messages, max_tokens=300)
    
    def validate_factual_accuracy(self, test_samples: List[Dict]) -> Dict[str, Any]:
        """éªŒè¯äº‹å®å‡†ç¡®æ€§"""
        
        print("ğŸ”¬ éªŒè¯äº‹å®å‡†ç¡®æ€§...")
        
        accuracy_results = {
            'total_samples': len(test_samples),
            'rag_accurate_facts': 0,
            'baseline_accurate_facts': 0,
            'verifiable_claims': {
                'rag': 0,
                'baseline': 0
            },
            'detailed_analysis': []
        }
        
        for i, sample in enumerate(test_samples, 1):
            print(f"  äº‹å®éªŒè¯ {i}/{len(test_samples)}")
            
            question = sample['question']
            rag_answer = sample['rag_answer']
            baseline_answer = sample['baseline_answer']
            
            # ä½¿ç”¨Claudeåˆ†æäº‹å®å‡†ç¡®æ€§
            fact_analysis = self.analyze_factual_claims(question, rag_answer, baseline_answer)
            
            if fact_analysis:
                accuracy_results['detailed_analysis'].append({
                    'question': question,
                    'analysis': fact_analysis
                })
            
            time.sleep(1)
        
        return accuracy_results
    
    def analyze_factual_claims(self, question: str, rag_answer: str, baseline_answer: str) -> str:
        """åˆ†æç­”æ¡ˆä¸­çš„äº‹å®å£°æ˜"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Analyze the factual claims in these two answers to an energy domain question. Focus on verifiable facts, statistics, and technical details.

Question: {question}

Answer A (RAG-based): {rag_answer[:800]}...

Answer B (Baseline): {baseline_answer[:800]}...

For each answer, identify:
1. Number of specific factual claims
2. Presence of citations/sources
3. Technical accuracy (if verifiable)
4. Potential inaccuracies or unsupported claims

Provide a brief comparison focusing on factual reliability."""
            }
        ]
        
        return self.call_claude_api(messages, max_tokens=500)
    
    def measure_ground_truth_quality(self, sample_questions: List[str]) -> Dict[str, Any]:
        """æµ‹é‡ground truthè´¨é‡"""
        
        print("ğŸ“ æµ‹é‡Ground Truthè´¨é‡...")
        
        gt_quality = {
            'total_questions': len(sample_questions),
            'literature_supported': 0,
            'verifiable_claims': 0,
            'citation_quality': 0,
            'research_depth': 0,
            'quality_scores': []
        }
        
        for i, question in enumerate(sample_questions, 1):
            print(f"  è´¨é‡æµ‹é‡ {i}/{len(sample_questions)}")
            
            # ç”ŸæˆRAGç­”æ¡ˆ
            relevant_papers = self.rag_generator.retrieve_relevant_papers(question, top_k=5)
            rag_result = self.rag_generator.generate_rag_fused_answer(question, relevant_papers)
            
            if rag_result['answer']:
                # è¯„ä¼°è´¨é‡æŒ‡æ ‡
                quality_metrics = self.rag_generator.assess_answer_quality(
                    question, rag_result['answer'], rag_result['sources']
                )
                
                gt_quality['quality_scores'].append(quality_metrics['quality_scores']['overall'])
                
                if len(rag_result['sources']) > 0:
                    gt_quality['literature_supported'] += 1
                
                if quality_metrics['has_citations']:
                    gt_quality['verifiable_claims'] += 1
        
        # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
        if gt_quality['quality_scores']:
            gt_quality['average_quality'] = sum(gt_quality['quality_scores']) / len(gt_quality['quality_scores'])
            gt_quality['literature_support_rate'] = gt_quality['literature_supported'] / gt_quality['total_questions']
            gt_quality['verifiable_rate'] = gt_quality['verifiable_claims'] / gt_quality['total_questions']
        
        return gt_quality
    
    def run_comprehensive_validation(self, num_test_questions: int = 10) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆéªŒè¯"""
        
        print(f"ğŸš€ å¼€å§‹RAGæœ‰æ•ˆæ€§ç»¼åˆéªŒè¯")
        print("=" * 70)
        print(f"æµ‹è¯•é—®é¢˜æ•°é‡: {num_test_questions}")
        print("éªŒè¯ç»´åº¦: æ£€ç´¢å‡†ç¡®æ€§ã€ç­”æ¡ˆè´¨é‡ã€äº‹å®å‡†ç¡®æ€§ã€Ground Truthè´¨é‡")
        print("=" * 70)
        
        # ç”Ÿæˆæµ‹è¯•é—®é¢˜
        test_questions = self.generate_test_questions(num_test_questions)
        
        validation_results = {
            'test_metadata': {
                'num_questions': num_test_questions,
                'test_timestamp': datetime.now().isoformat(),
                'rag_corpus_size': len(self.rag_generator.rag_corpus)
            },
            'retrieval_accuracy': {},
            'answer_quality_comparison': {},
            'ground_truth_quality': {},
            'overall_assessment': {}
        }
        
        # 1. éªŒè¯æ£€ç´¢å‡†ç¡®æ€§
        validation_results['retrieval_accuracy'] = self.validate_literature_retrieval_accuracy(test_questions)
        
        # 2. å¯¹æ¯”ç­”æ¡ˆè´¨é‡
        validation_results['answer_quality_comparison'] = self.compare_answer_quality(test_questions[:5])  # é™åˆ¶æ•°é‡ä»¥èŠ‚çœæ—¶é—´
        
        # 3. æµ‹é‡Ground Truthè´¨é‡
        validation_results['ground_truth_quality'] = self.measure_ground_truth_quality(test_questions[:5])
        
        # 4. ç”Ÿæˆæ€»ä½“è¯„ä¼°
        validation_results['overall_assessment'] = self.generate_overall_assessment(validation_results)
        
        return validation_results
    
    def generate_test_questions(self, num_questions: int) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•é—®é¢˜"""
        
        test_questions = [
            "How do energy storage systems impact renewable energy integration?",
            "What are the economic challenges of nuclear power deployment?",
            "How does carbon pricing affect fossil fuel investment decisions?",
            "What role do smart grids play in energy transition?",
            "How do environmental policies influence renewable energy adoption?",
            "What are the technical barriers to offshore wind development?",
            "How does energy security relate to renewable energy deployment?",
            "What are the lifecycle impacts of solar photovoltaic systems?",
            "How do energy markets adapt to variable renewable generation?",
            "What policy frameworks support energy storage deployment?"
        ]
        
        return test_questions[:num_questions]
    
    def generate_overall_assessment(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        
        retrieval = validation_results['retrieval_accuracy']
        quality = validation_results['answer_quality_comparison']
        gt_quality = validation_results['ground_truth_quality']
        
        assessment = {
            'rag_effectiveness_score': 0.0,
            'key_strengths': [],
            'areas_for_improvement': [],
            'recommendation': ""
        }
        
        # è®¡ç®—ç»¼åˆæ•ˆæœåˆ†æ•°
        retrieval_score = retrieval.get('success_rate', 0) * 0.3
        quality_score = (quality.get('rag_wins', 0) / max(quality.get('total_comparisons', 1), 1)) * 0.4
        gt_score = gt_quality.get('average_quality', 0) * 0.3
        
        assessment['rag_effectiveness_score'] = retrieval_score + quality_score + gt_score
        
        # è¯†åˆ«ä¼˜åŠ¿
        if retrieval.get('success_rate', 0) > 0.9:
            assessment['key_strengths'].append("é«˜æ–‡çŒ®æ£€ç´¢æˆåŠŸç‡")
        
        if quality.get('rag_wins', 0) > quality.get('baseline_wins', 0):
            assessment['key_strengths'].append("RAGç­”æ¡ˆè´¨é‡ä¼˜äºåŸºçº¿")
        
        if gt_quality.get('literature_support_rate', 0) > 0.8:
            assessment['key_strengths'].append("å¼ºæ–‡çŒ®æ”¯æ’‘çš„Ground Truth")
        
        # ç”Ÿæˆå»ºè®®
        if assessment['rag_effectiveness_score'] > 0.7:
            assessment['recommendation'] = "RAGç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼Œå»ºè®®ç”¨äºç”Ÿäº§ç¯å¢ƒ"
        elif assessment['rag_effectiveness_score'] > 0.5:
            assessment['recommendation'] = "RAGç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–"
        else:
            assessment['recommendation'] = "RAGç³»ç»Ÿéœ€è¦æ˜¾è‘—æ”¹è¿›"
        
        return assessment
    
    def save_validation_results(self, results: Dict[str, Any], output_dir: str = "validation_output") -> str:
        """ä¿å­˜éªŒè¯ç»“æœ"""
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = output_path / f"rag_validation_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… éªŒè¯ç»“æœå·²ä¿å­˜: {filename}")
        return str(filename)
    
    def print_validation_summary(self, results: Dict[str, Any]):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        
        print(f"\nğŸ“Š RAGæœ‰æ•ˆæ€§éªŒè¯ç»“æœæ‘˜è¦")
        print("=" * 70)
        
        # æ£€ç´¢å‡†ç¡®æ€§
        retrieval = results['retrieval_accuracy']
        print(f"ğŸ” æ–‡çŒ®æ£€ç´¢å‡†ç¡®æ€§:")
        print(f"   æˆåŠŸç‡: {retrieval.get('success_rate', 0)*100:.1f}%")
        print(f"   å¹³å‡ç›¸å…³æ€§: {retrieval.get('average_relevance_score', 0):.3f}")
        
        # ç­”æ¡ˆè´¨é‡å¯¹æ¯”
        quality = results['answer_quality_comparison']
        print(f"\nğŸ“Š ç­”æ¡ˆè´¨é‡å¯¹æ¯”:")
        print(f"   RAGèƒœå‡º: {quality.get('rag_wins', 0)}")
        print(f"   åŸºçº¿èƒœå‡º: {quality.get('baseline_wins', 0)}")
        print(f"   å¹³å±€: {quality.get('ties', 0)}")
        
        # Ground Truthè´¨é‡
        gt = results['ground_truth_quality']
        print(f"\nğŸ“ Ground Truthè´¨é‡:")
        print(f"   å¹³å‡è´¨é‡åˆ†: {gt.get('average_quality', 0):.3f}")
        print(f"   æ–‡çŒ®æ”¯æ’‘ç‡: {gt.get('literature_support_rate', 0)*100:.1f}%")
        
        # æ€»ä½“è¯„ä¼°
        overall = results['overall_assessment']
        print(f"\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        print(f"   RAGæœ‰æ•ˆæ€§åˆ†æ•°: {overall.get('rag_effectiveness_score', 0):.3f}")
        print(f"   å»ºè®®: {overall.get('recommendation', 'N/A')}")
        
        print("=" * 70)

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ”¬ RAGæœ‰æ•ˆæ€§éªŒè¯ç³»ç»Ÿ")
    print("=" * 70)
    
    api_key = "xxxxx"
    
    try:
        validator = RAGEffectivenessValidator(api_key)
        
        # è¿è¡ŒéªŒè¯
        results = validator.run_comprehensive_validation(num_test_questions=10)
        
        # ä¿å­˜ç»“æœ
        results_file = validator.save_validation_results(results)
        
        # æ‰“å°æ‘˜è¦
        validator.print_validation_summary(results)
        
        print(f"\nâœ… RAGæœ‰æ•ˆæ€§éªŒè¯å®Œæˆ!")
        print(f"è¯¦ç»†ç»“æœ: {results_file}")
        
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 