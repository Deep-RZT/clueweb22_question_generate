#!/usr/bin/env python3
"""
Unified Comparative Experiment
ç»Ÿä¸€çš„LLMå¯¹æ¯”å®éªŒç³»ç»Ÿ

æ”¯æŒOpenAI GPT-4oå’ŒClaude Sonnet 4çš„å®Œæ•´å¯¹æ¯”å®éªŒ
è‡ªåŠ¨å¤„ç†API keyæ— æ•ˆçš„æƒ…å†µ
"""

import os
import json
import time
import pandas as pd
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dynamic_llm_manager import DynamicLLMManager
from json_parser_utils import extract_questions_from_response

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UnifiedComparativeExperiment:
    """ç»Ÿä¸€çš„å¯¹æ¯”å®éªŒç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®éªŒç³»ç»Ÿ"""
        self.manager = DynamicLLMManager()
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = f"unified_comparative_experiment_{self.timestamp}"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # å®éªŒé…ç½®
        self.config = {
            'clueweb22_topics': 9,      # ClueWeb22ä¸»é¢˜æ•°é‡ï¼ˆå®é™…å¯ç”¨ï¼‰
            'random_topics': 5,         # éšæœºæ–‡æ¡£ä¸»é¢˜æ•°é‡
            'questions_per_topic': 10,  # æ¯ä¸ªä¸»é¢˜ç”Ÿæˆçš„é—®é¢˜æ•°
            'answers_per_topic': 5,     # æ¯ä¸ªä¸»é¢˜ç”Ÿæˆçš„ç­”æ¡ˆæ•°
            'documents_per_topic': 8,   # æ¯ä¸ªä¸»é¢˜ä½¿ç”¨çš„æ–‡æ¡£æ•°
            'max_tokens_report': 3000,  # æŠ¥å‘Šæœ€å¤§tokenæ•°
            'rest_time': 2              # APIè°ƒç”¨é—´éš”æ—¶é—´(ç§’)
        }
        
        # æ£€æŸ¥å¯ç”¨çš„LLMæä¾›å•†
        self.available_providers = self._check_available_providers()
        
        logger.info(f"å®éªŒåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"å¯ç”¨çš„LLMæä¾›å•†: {self.available_providers}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _check_available_providers(self) -> List[str]:
        """æ£€æŸ¥å¯ç”¨çš„LLMæä¾›å•†"""
        available = []
        
        # æ£€æŸ¥OpenAI
        try:
            test_response = self.manager.generate_text(
                "Test", provider="openai", max_tokens=10
            )
            if test_response.success:
                available.append("openai")
                logger.info("âœ… OpenAI API å¯ç”¨")
            else:
                logger.warning(f"âŒ OpenAI API ä¸å¯ç”¨: {test_response.error}")
        except Exception as e:
            logger.warning(f"âŒ OpenAI API æ£€æŸ¥å¤±è´¥: {e}")
        
        # æ£€æŸ¥Claude
        try:
            test_response = self.manager.generate_text(
                "Test", provider="claude", max_tokens=10
            )
            if test_response.success:
                available.append("claude")
                logger.info("âœ… Claude API å¯ç”¨")
            else:
                logger.warning(f"âŒ Claude API ä¸å¯ç”¨: {test_response.error}")
        except Exception as e:
            logger.warning(f"âŒ Claude API æ£€æŸ¥å¤±è´¥: {e}")
        
        return available
    
    def load_clueweb22_data(self) -> Dict[str, List[Dict]]:
        """åŠ è½½ClueWeb22æ•°æ®"""
        topics_data = {}
        clueweb_path = "task_file/clueweb22_json_results"
        
        if not os.path.exists(clueweb_path):
            logger.error(f"ClueWeb22æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {clueweb_path}")
            return {}
        
        json_files = [f for f in os.listdir(clueweb_path) 
                     if f.endswith('.json') and not f.startswith('conversion_stats')]
        
        for i, filename in enumerate(json_files[:self.config['clueweb22_topics']]):
            file_path = os.path.join(clueweb_path, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                topic = data.get('topic', filename.replace('.json', ''))
                documents = data.get('results', [])[:self.config['documents_per_topic']]
                
                if documents:
                    topics_data[topic] = documents
                    
            except Exception as e:
                logger.error(f"åŠ è½½ClueWeb22æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        logger.info(f"åŠ è½½äº† {len(topics_data)} ä¸ªClueWeb22ä¸»é¢˜")
        return topics_data
    
    def load_random_academic_data(self) -> Dict[str, List[Dict]]:
        """åŠ è½½é«˜è´¨é‡éšæœºå­¦æœ¯æ–‡æ¡£æ•°æ®"""
        topics_data = {}
        
        # ä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡æ–‡æ¡£
        high_quality_file = "task_file/random_academic_docs/high_quality_academic_documents.json"
        fallback_file = "task_file/random_academic_docs/academic_documents.json"
        
        docs_file = high_quality_file if os.path.exists(high_quality_file) else fallback_file
        
        if not os.path.exists(docs_file):
            logger.warning("æœªæ‰¾åˆ°ç°æœ‰å­¦æœ¯æ–‡æ¡£ï¼Œå°†ç”Ÿæˆæ–°çš„é«˜è´¨é‡æ–‡æ¡£...")
            try:
                from optimized_academic_crawler import OptimizedAcademicCrawler
                
                crawler = OptimizedAcademicCrawler(100)
                all_docs = crawler.generate_high_quality_documents()
                crawler.save_documents(all_docs)
                
                logger.info(f"âœ… ç”Ÿæˆäº† {len(all_docs)} ç¯‡é«˜è´¨é‡å­¦æœ¯æ–‡æ¡£")
                
            except Exception as e:
                logger.error(f"ç”Ÿæˆé«˜è´¨é‡æ–‡æ¡£å¤±è´¥: {e}")
                return {}
        else:
            try:
                with open(docs_file, 'r', encoding='utf-8') as f:
                    all_docs = json.load(f)
                
                logger.info(f"åŠ è½½äº† {len(all_docs)} ç¯‡å­¦æœ¯æ–‡æ¡£")
                if docs_file == high_quality_file:
                    avg_words = sum(doc.get('word_count', 0) for doc in all_docs) / len(all_docs)
                    avg_quality = sum(doc.get('quality_score', 0) for doc in all_docs) / len(all_docs)
                    logger.info(f"ğŸ“Š å¹³å‡è¯æ•°: {avg_words:.0f}, å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.2f}")
                    
            except Exception as e:
                logger.error(f"åŠ è½½å­¦æœ¯æ–‡æ¡£å¤±è´¥: {e}")
                return {}
        
        # æŒ‰é¢†åŸŸåˆ†ç»„
        domain_groups = {}
        for doc in all_docs:
            domain = doc.get('domain', 'unknown')
            if domain not in domain_groups:
                domain_groups[domain] = []
            domain_groups[domain].append(doc)
        
        # é€‰æ‹©å‰Nä¸ªé¢†åŸŸï¼Œä¼˜å…ˆé€‰æ‹©æ–‡æ¡£æ•°é‡å¤šçš„é¢†åŸŸ
        sorted_domains = sorted(domain_groups.items(), key=lambda x: len(x[1]), reverse=True)
        
        for i, (domain, docs) in enumerate(sorted_domains):
            if i >= self.config['random_topics']:
                break
            
            # é€‰æ‹©è´¨é‡æœ€é«˜çš„æ–‡æ¡£
            if docs_file == high_quality_file:
                sorted_docs = sorted(docs, key=lambda x: x.get('quality_score', 0), reverse=True)
            else:
                sorted_docs = docs
            
            selected_docs = sorted_docs[:self.config['documents_per_topic']]
            topics_data[domain] = selected_docs
        
        logger.info(f"âœ… å‡†å¤‡äº† {len(topics_data)} ä¸ªé«˜è´¨é‡å­¦æœ¯æ–‡æ¡£ä¸»é¢˜")
        return topics_data
    
    def process_single_topic(self, topic: str, documents: List[Dict], 
                           data_source: str, provider: str) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªä¸»é¢˜"""
        
        logger.info(f"  å¤„ç†ä¸»é¢˜: {topic} (ä½¿ç”¨ {provider})")
        start_time = time.time()
        
        result = {
            'topic': topic,
            'data_source': data_source,
            'provider': provider,
            'documents_count': len(documents),
            'success': False,
            'processing_time': 0,
            'steps': {},
            'qa_pairs': [],
            'statistics': {}
        }
        
        try:
            # 1. ç”ŸæˆæŠ¥å‘Š
            logger.info(f"    1. ç”ŸæˆæŠ¥å‘Š...")
            report_response = self.manager.generate_report(
                documents, topic, provider=provider, 
                max_tokens=self.config['max_tokens_report']
            )
            
            if not report_response.success:
                logger.error(f"    âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report_response.error}")
                result['steps']['report'] = {'success': False, 'error': report_response.error}
                return result
            
            report_content = report_response.content
            result['steps']['report'] = {
                'success': True,
                'content': report_content,
                'length': len(report_content),
                'word_count': len(report_content.split()),
                'usage': report_response.usage
            }
            logger.info(f"    âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ ({len(report_content)} å­—ç¬¦, {len(report_content.split())} è¯)")
            
            # 2. ç”Ÿæˆé—®é¢˜
            logger.info(f"    2. ç”Ÿæˆé—®é¢˜...")
            questions_response = self.manager.generate_questions(
                report_content, topic, self.config['questions_per_topic'], provider=provider
            )
            
            if not questions_response.success:
                logger.error(f"    âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥: {questions_response.error}")
                result['steps']['questions'] = {'success': False, 'error': questions_response.error}
                return result
            
            questions_data = extract_questions_from_response(questions_response.content)
            
            if not questions_data:
                logger.error(f"    âŒ é—®é¢˜è§£æå¤±è´¥")
                result['steps']['questions'] = {'success': False, 'error': 'JSONè§£æå¤±è´¥'}
                return result
            
            result['steps']['questions'] = {
                'success': True,
                'count': len(questions_data),
                'questions_data': questions_data,
                'usage': questions_response.usage
            }
            logger.info(f"    âœ… é—®é¢˜ç”ŸæˆæˆåŠŸ ({len(questions_data)} ä¸ªé—®é¢˜)")
            
            # 3. ç”Ÿæˆç­”æ¡ˆ
            logger.info(f"    3. ç”Ÿæˆç­”æ¡ˆ (å‰{self.config['answers_per_topic']}ä¸ªé—®é¢˜)...")
            qa_pairs = []
            
            for j, q_data in enumerate(questions_data[:self.config['answers_per_topic']]):
                question = q_data.get('question', '')
                difficulty = q_data.get('difficulty', 'Medium')
                question_type = q_data.get('type', 'unknown')
                reasoning = q_data.get('reasoning', '')
                
                if question:
                    logger.info(f"      ç”Ÿæˆç­”æ¡ˆ {j+1}/{self.config['answers_per_topic']}...")
                    answer_response = self.manager.generate_answer(
                        question, report_content, difficulty, provider=provider
                    )
                    
                    if answer_response.success:
                        qa_pairs.append({
                            'question_id': j + 1,
                            'question': question,
                            'answer': answer_response.content,
                            'difficulty': difficulty,
                            'type': question_type,
                            'reasoning': reasoning,
                            'answer_length': len(answer_response.content),
                            'answer_word_count': len(answer_response.content.split()),
                            'usage': answer_response.usage
                        })
                        logger.info(f"      âœ… ç­”æ¡ˆ {j+1} å®Œæˆ ({len(answer_response.content.split())} è¯)")
                    else:
                        logger.warning(f"      âŒ ç­”æ¡ˆ {j+1} å¤±è´¥: {answer_response.error}")
                    
                    # çŸ­æš‚ä¼‘æ¯
                    time.sleep(1)
            
            result['qa_pairs'] = qa_pairs
            result['steps']['answers'] = {
                'success': True,
                'count': len(qa_pairs),
                'total_answer_length': sum(qa['answer_length'] for qa in qa_pairs)
            }
            logger.info(f"    âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ ({len(qa_pairs)} ä¸ªç­”æ¡ˆ)")
            
            # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            result['statistics'] = self._calculate_topic_statistics(qa_pairs)
            result['success'] = True
            
        except Exception as e:
            logger.error(f"    âŒ å¤„ç†å¤±è´¥: {e}")
            result['error'] = str(e)
        
        result['processing_time'] = time.time() - start_time
        logger.info(f"    âœ… ä¸»é¢˜å¤„ç†å®Œæˆ (è€—æ—¶: {result['processing_time']:.2f}ç§’)")
        
        return result
    
    def _calculate_topic_statistics(self, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—ä¸»é¢˜ç»Ÿè®¡ä¿¡æ¯"""
        
        if not qa_pairs:
            return {}
        
        # éš¾åº¦åˆ†å¸ƒ
        difficulty_dist = {'Easy': 0, 'Medium': 0, 'Hard': 0}
        for qa in qa_pairs:
            difficulty = qa.get('difficulty', 'Medium')
            if difficulty in difficulty_dist:
                difficulty_dist[difficulty] += 1
        
        # é—®é¢˜ç±»å‹åˆ†å¸ƒ
        type_dist = {}
        for qa in qa_pairs:
            q_type = qa.get('type', 'unknown')
            type_dist[q_type] = type_dist.get(q_type, 0) + 1
        
        # ç­”æ¡ˆé•¿åº¦ç»Ÿè®¡
        answer_lengths = [qa['answer_length'] for qa in qa_pairs]
        answer_word_counts = [qa['answer_word_count'] for qa in qa_pairs]
        
        return {
            'total_qa_pairs': len(qa_pairs),
            'difficulty_distribution': difficulty_dist,
            'difficulty_percentages': {
                k: (v / len(qa_pairs) * 100) if len(qa_pairs) > 0 else 0 
                for k, v in difficulty_dist.items()
            },
            'type_distribution': type_dist,
            'answer_length_stats': {
                'min': min(answer_lengths) if answer_lengths else 0,
                'max': max(answer_lengths) if answer_lengths else 0,
                'avg': sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0
            },
            'answer_word_count_stats': {
                'min': min(answer_word_counts) if answer_word_counts else 0,
                'max': max(answer_word_counts) if answer_word_counts else 0,
                'avg': sum(answer_word_counts) / len(answer_word_counts) if answer_word_counts else 0
            }
        }
    
    def run_experiment_set(self, data_source: str, topics_data: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """è¿è¡Œä¸€ç»„å®éªŒï¼ˆæ‰€æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼‰"""
        
        results = {}
        
        for provider in self.available_providers:
            logger.info(f"\n{'='*60}")
            logger.info(f"è¿è¡Œ {data_source} + {provider.upper()} å®éªŒ")
            logger.info(f"{'='*60}")
            
            provider_results = []
            
            for i, (topic, documents) in enumerate(topics_data.items()):
                logger.info(f"\n--- å¤„ç†ä¸»é¢˜ {i+1}/{len(topics_data)} ---")
                
                topic_result = self.process_single_topic(
                    topic, documents, data_source, provider
                )
                
                if topic_result and topic_result.get('success', False):
                    provider_results.append(topic_result)
                    
                    # ä¿å­˜å•ä¸ªä¸»é¢˜ç»“æœ
                    safe_topic = topic.replace(' ', '_').replace('/', '_')[:50]
                    topic_file = os.path.join(
                        self.output_dir, 
                        f"{data_source}_{provider}_{i+1}_{safe_topic}.json"
                    )
                    with open(topic_file, 'w', encoding='utf-8') as f:
                        json.dump(topic_result, f, ensure_ascii=False, indent=2)
                
                # ä¼‘æ¯é¿å…APIé™åˆ¶
                time.sleep(self.config['rest_time'])
            
            results[provider] = provider_results
            logger.info(f"{data_source} + {provider.upper()} å®éªŒå®Œæˆ: {len(provider_results)} ä¸ªæˆåŠŸä¸»é¢˜")
        
        return results
    
    def run_all_experiments(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        
        if not self.available_providers:
            logger.error("æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œæ— æ³•è¿è¡Œå®éªŒ")
            return {}
        
        logger.info("å¼€å§‹è¿è¡Œç»Ÿä¸€å¯¹æ¯”å®éªŒ...")
        logger.info(f"å®éªŒé…ç½®: {self.config}")
        
        all_results = {
            'timestamp': self.timestamp,
            'config': self.config,
            'available_providers': self.available_providers,
            'experiments': {}
        }
        
        # å®éªŒ1: ClueWeb22æ•°æ®
        logger.info(f"\n{'='*80}")
        logger.info("å®éªŒç»„1: ClueWeb22æ•°æ®")
        logger.info(f"{'='*80}")
        
        clueweb_data = self.load_clueweb22_data()
        if clueweb_data:
            clueweb_results = self.run_experiment_set('clueweb22', clueweb_data)
            all_results['experiments']['clueweb22'] = clueweb_results
        else:
            logger.warning("ClueWeb22æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡è¯¥å®éªŒç»„")
        
        # å®éªŒ2: éšæœºå­¦æœ¯æ–‡æ¡£
        logger.info(f"\n{'='*80}")
        logger.info("å®éªŒç»„2: éšæœºå­¦æœ¯æ–‡æ¡£")
        logger.info(f"{'='*80}")
        
        random_data = self.load_random_academic_data()
        if random_data:
            random_results = self.run_experiment_set('random_academic', random_data)
            all_results['experiments']['random_academic'] = random_results
        else:
            logger.warning("éšæœºå­¦æœ¯æ–‡æ¡£æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡è¯¥å®éªŒç»„")
        
        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        all_results['summary'] = self._generate_experiment_summary(all_results['experiments'])
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = os.path.join(self.output_dir, "complete_experiment_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # å¯¼å‡ºåˆ°Excel
        self._export_to_excel(all_results)
        
        logger.info(f"\n{'='*80}")
        logger.info("æ‰€æœ‰å®éªŒå®Œæˆï¼")
        logger.info(f"{'='*80}")
        logger.info(f"ç»“æœä¿å­˜åˆ°: {self.output_dir}")
        
        return all_results
    
    def _generate_experiment_summary(self, experiments: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®éªŒæ±‡æ€»ç»Ÿè®¡"""
        
        summary = {
            'total_experiments': 0,
            'successful_experiments': 0,
            'by_data_source': {},
            'by_provider': {},
            'comparison_matrix': {}
        }
        
        for data_source, provider_results in experiments.items():
            summary['by_data_source'][data_source] = {}
            
            for provider, results in provider_results.items():
                summary['total_experiments'] += 1
                
                if results:
                    summary['successful_experiments'] += 1
                    
                    # æŒ‰æ•°æ®æºç»Ÿè®¡
                    if provider not in summary['by_data_source'][data_source]:
                        summary['by_data_source'][data_source][provider] = {}
                    
                    # æŒ‰æä¾›å•†ç»Ÿè®¡
                    if provider not in summary['by_provider']:
                        summary['by_provider'][provider] = {}
                    
                    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    stats = self._calculate_provider_stats(results)
                    summary['by_data_source'][data_source][provider] = stats
                    
                    if data_source not in summary['by_provider'][provider]:
                        summary['by_provider'][provider][data_source] = stats
        
        # ç”Ÿæˆå¯¹æ¯”çŸ©é˜µ
        summary['comparison_matrix'] = self._generate_comparison_matrix(experiments)
        
        return summary
    
    def _calculate_provider_stats(self, results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—æä¾›å•†ç»Ÿè®¡ä¿¡æ¯"""
        
        if not results:
            return {}
        
        total_qa_pairs = sum(len(r.get('qa_pairs', [])) for r in results)
        total_processing_time = sum(r.get('processing_time', 0) for r in results)
        
        # æ±‡æ€»éš¾åº¦åˆ†å¸ƒ
        total_difficulty = {'Easy': 0, 'Medium': 0, 'Hard': 0}
        for result in results:
            stats = result.get('statistics', {})
            difficulty_dist = stats.get('difficulty_distribution', {})
            for difficulty, count in difficulty_dist.items():
                if difficulty in total_difficulty:
                    total_difficulty[difficulty] += count
        
        return {
            'topics_processed': len(results),
            'total_qa_pairs': total_qa_pairs,
            'avg_qa_pairs_per_topic': total_qa_pairs / len(results) if results else 0,
            'total_processing_time': total_processing_time,
            'avg_processing_time_per_topic': total_processing_time / len(results) if results else 0,
            'difficulty_distribution': total_difficulty,
            'difficulty_percentages': {
                k: (v / total_qa_pairs * 100) if total_qa_pairs > 0 else 0 
                for k, v in total_difficulty.items()
            }
        }
    
    def _generate_comparison_matrix(self, experiments: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”çŸ©é˜µ"""
        
        matrix = {}
        
        for data_source, provider_results in experiments.items():
            for provider, results in provider_results.items():
                key = f"{data_source}_{provider}"
                
                if results:
                    stats = self._calculate_provider_stats(results)
                    matrix[key] = {
                        'data_source': data_source,
                        'provider': provider,
                        'success': True,
                        'topics': stats.get('topics_processed', 0),
                        'qa_pairs': stats.get('total_qa_pairs', 0),
                        'avg_processing_time': stats.get('avg_processing_time_per_topic', 0),
                        'difficulty_easy_pct': stats.get('difficulty_percentages', {}).get('Easy', 0),
                        'difficulty_medium_pct': stats.get('difficulty_percentages', {}).get('Medium', 0),
                        'difficulty_hard_pct': stats.get('difficulty_percentages', {}).get('Hard', 0)
                    }
                else:
                    matrix[key] = {
                        'data_source': data_source,
                        'provider': provider,
                        'success': False,
                        'topics': 0,
                        'qa_pairs': 0,
                        'avg_processing_time': 0,
                        'difficulty_easy_pct': 0,
                        'difficulty_medium_pct': 0,
                        'difficulty_hard_pct': 0
                    }
        
        return matrix
    
    def _export_to_excel(self, all_results: Dict[str, Any]):
        """å¯¼å‡ºç»“æœåˆ°Excel"""
        
        try:
            excel_file = os.path.join(self.output_dir, "unified_comparative_experiment.xlsx")
            
            # å‡†å¤‡æ•°æ®
            summary_data = []
            topics_data = []
            qa_pairs_data = []
            
            # æ±‡æ€»æ•°æ®
            for experiment_key, stats in all_results['summary']['comparison_matrix'].items():
                summary_data.append(stats)
            
            # è¯¦ç»†æ•°æ®
            for data_source, provider_results in all_results['experiments'].items():
                for provider, results in provider_results.items():
                    for result in results:
                        # ä¸»é¢˜æ•°æ®
                        stats = result.get('statistics', {})
                        topic_data = {
                            'experiment': f"{data_source}_{provider}",
                            'data_source': data_source,
                            'provider': provider,
                            'topic': result.get('topic', ''),
                            'documents_count': result.get('documents_count', 0),
                            'qa_pairs_count': stats.get('total_qa_pairs', 0),
                            'processing_time': result.get('processing_time', 0),
                            'report_length': result.get('steps', {}).get('report', {}).get('length', 0),
                            'report_word_count': result.get('steps', {}).get('report', {}).get('word_count', 0),
                            'avg_answer_length': stats.get('answer_length_stats', {}).get('avg', 0),
                            'avg_answer_word_count': stats.get('answer_word_count_stats', {}).get('avg', 0),
                            'easy_questions': stats.get('difficulty_distribution', {}).get('Easy', 0),
                            'medium_questions': stats.get('difficulty_distribution', {}).get('Medium', 0),
                            'hard_questions': stats.get('difficulty_distribution', {}).get('Hard', 0)
                        }
                        topics_data.append(topic_data)
                        
                        # QAå¯¹æ•°æ®
                        for qa in result.get('qa_pairs', []):
                            qa_data = {
                                'experiment': f"{data_source}_{provider}",
                                'data_source': data_source,
                                'provider': provider,
                                'topic': result.get('topic', ''),
                                'question_id': qa.get('question_id', 0),
                                'question': qa.get('question', ''),
                                'difficulty': qa.get('difficulty', ''),
                                'type': qa.get('type', ''),
                                'answer_length': qa.get('answer_length', 0),
                                'answer_word_count': qa.get('answer_word_count', 0),
                                'reasoning': qa.get('reasoning', '')
                            }
                            qa_pairs_data.append(qa_data)
            
            # åˆ›å»ºExcelæ–‡ä»¶
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                if summary_data:
                    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Experiment_Summary', index=False)
                if topics_data:
                    pd.DataFrame(topics_data).to_excel(writer, sheet_name='Topics_Detail', index=False)
                if qa_pairs_data:
                    pd.DataFrame(qa_pairs_data).to_excel(writer, sheet_name='QA_Pairs_Detail', index=False)
            
            logger.info(f"Excelæ–‡ä»¶å·²å¯¼å‡º: {excel_file}")
            
        except Exception as e:
            logger.error(f"Excelå¯¼å‡ºå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("="*80)
    print("ç»Ÿä¸€LLMå¯¹æ¯”å®éªŒç³»ç»Ÿ")
    print("="*80)
    print("æ”¯æŒ: OpenAI GPT-4o, Claude Sonnet 4")
    print("æ•°æ®æº: ClueWeb22, éšæœºå­¦æœ¯æ–‡æ¡£")
    print("è‡ªåŠ¨å¤„ç†API keyæ— æ•ˆæƒ…å†µ")
    print("="*80)
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = UnifiedComparativeExperiment()
    
    if not experiment.available_providers:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œè¯·æ£€æŸ¥APIé…ç½®")
        return
    
    print(f"âœ… æ£€æµ‹åˆ°å¯ç”¨çš„LLMæä¾›å•†: {', '.join(experiment.available_providers)}")
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜åˆ°: {experiment.output_dir}")
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = experiment.run_all_experiments()
    
    # æ‰“å°æœ€ç»ˆæ‘˜è¦
    if results and 'summary' in results:
        summary = results['summary']
        print(f"\n{'='*80}")
        print("å®éªŒå®Œæˆæ‘˜è¦")
        print(f"{'='*80}")
        print(f"æ€»å®éªŒæ•°: {summary['total_experiments']}")
        print(f"æˆåŠŸå®éªŒæ•°: {summary['successful_experiments']}")
        print(f"æˆåŠŸç‡: {summary['successful_experiments']/summary['total_experiments']*100:.1f}%")
        
        for experiment_key, stats in summary['comparison_matrix'].items():
            if stats['success']:
                print(f"âœ… {experiment_key}: {stats['topics']} ä¸»é¢˜, {stats['qa_pairs']} QAå¯¹")
            else:
                print(f"âŒ {experiment_key}: å¤±è´¥")
        
        print(f"\nğŸ“Š è¯¦ç»†ç»“æœ: {experiment.output_dir}/unified_comparative_experiment.xlsx")

if __name__ == "__main__":
    main() 