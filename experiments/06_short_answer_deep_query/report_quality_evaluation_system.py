"""
Report Quality Evaluation System
ç‹¬ç«‹çš„æŠ¥å‘Šè´¨é‡è¯„ä¼°ç³»ç»Ÿ - é’ˆå¯¹ç®€åŒ–æŠ¥å‘Šä¼˜åŒ–çš„è´¨é‡åˆ¤æ–­
"""

import json
import re
import math
import logging
from typing import Dict, List, Any, Tuple, Set
from dataclasses import dataclass
from collections import Counter
import numpy as np
from pathlib import Path
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡æ•°æ®ç±»"""
    relevance_score: float          # ç›¸å…³æ€§åˆ†æ•° (0-1)
    information_density: float      # ä¿¡æ¯å¯†åº¦ (0-1)
    coherence_score: float         # è¿è´¯æ€§åˆ†æ•° (0-1)
    factual_richness: float        # äº‹å®ä¸°å¯Œåº¦ (0-1)
    technical_depth: float         # æŠ€æœ¯æ·±åº¦ (0-1)
    structural_quality: float      # ç»“æ„è´¨é‡ (0-1)
    overall_score: float           # ç»¼åˆåˆ†æ•° (0-1)
    grade: str                     # è´¨é‡ç­‰çº§ (A/B/C/D/F)

class ReportQualityEvaluator:
    """æŠ¥å‘Šè´¨é‡è¯„ä¼°å™¨ - é’ˆå¯¹ç®€åŒ–æŠ¥å‘Šä¼˜åŒ–çš„è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self):
        self.initialize_nlp_resources()
        # è°ƒæ•´è´¨é‡é˜ˆå€¼ - å¯¹ç®€åŒ–æŠ¥å‘Šæ›´å‹å¥½
        self.quality_thresholds = {
            'excellent': 0.75,  # Açº§ (é™ä½)
            'good': 0.60,       # Bçº§ (é™ä½)
            'acceptable': 0.45,  # Cçº§ (é™ä½)
            'poor': 0.30,       # Dçº§ (é™ä½)
            # < 0.30 = Fçº§
        }
        # è°ƒæ•´æƒé‡é…ç½® - æ›´é‡è§†å†…å®¹è´¨é‡è€Œéç›¸å…³æ€§åŒ¹é…
        self.weight_config = {
            'relevance': 0.15,          # é™ä½ç›¸å…³æ€§æƒé‡
            'information_density': 0.25, # æé«˜ä¿¡æ¯å¯†åº¦æƒé‡
            'coherence': 0.20,          # æé«˜è¿è´¯æ€§æƒé‡
            'factual_richness': 0.25,   # æé«˜äº‹å®ä¸°å¯Œåº¦æƒé‡
            'technical_depth': 0.10,    # é™ä½æŠ€æœ¯æ·±åº¦æƒé‡
            'structural_quality': 0.05   # ä¿æŒç»“æ„è´¨é‡æƒé‡
        }
        
        # æ‰©å±•æŠ€æœ¯æœ¯è¯­è¯å…¸
        self.technical_terms = {
            'research': ['study', 'research', 'investigation', 'analysis', 'experiment', 'trial', 'survey', 'examination'],
            'methodology': ['method', 'approach', 'technique', 'procedure', 'algorithm', 'framework', 'model', 'strategy'],
            'data': ['data', 'dataset', 'sample', 'population', 'measurement', 'observation', 'variable', 'metric'],
            'statistics': ['significant', 'correlation', 'regression', 'p-value', 'confidence', 'statistical', 'analysis'],
            'technology': ['system', 'technology', 'platform', 'architecture', 'infrastructure', 'implementation', 'software'],
            'performance': ['performance', 'efficiency', 'optimization', 'accuracy', 'precision', 'recall', 'effectiveness'],
            'academic': ['published', 'journal', 'conference', 'peer-reviewed', 'citation', 'bibliography', 'findings'],
            'domain_specific': ['healthcare', 'medical', 'clinical', 'patient', 'treatment', 'diagnosis', 'therapy']
        }
        
        # æ•°å€¼æ¨¡å¼ (ç”¨äºè¯†åˆ«å…·ä½“æ•°æ®)
        self.numerical_patterns = [
            r'\d+\.?\d*\s*%',           # ç™¾åˆ†æ¯”
            r'\d+\.?\d*\s*[a-zA-Z]+',   # æ•°å­—+å•ä½
            r'\$\d+\.?\d*[KMB]?',       # é‡‘é¢
            r'\d{4}å¹´?',                # å¹´ä»½
            r'\d+\.?\d*\s*å€',          # å€æ•°
            r'[+-]?\d+\.?\d*',          # ä¸€èˆ¬æ•°å­—
        ]
    
    def initialize_nlp_resources(self):
        """åˆå§‹åŒ–NLPèµ„æº"""
        try:
            # å°è¯•ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
            import ssl
            try:
                _create_unverified_https_context = ssl._create_unverified_context
            except AttributeError:
                pass
            else:
                ssl._create_default_https_context = _create_unverified_https_context
            
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            self.stop_words = set(stopwords.words('english'))
        except Exception as e:
            logger.warning(f"NLTKèµ„æºåˆå§‹åŒ–å¤±è´¥: {e}")
            # ä½¿ç”¨ç®€å•çš„åœç”¨è¯åˆ—è¡¨ä½œä¸ºåå¤‡
            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])
    
    def evaluate_report_quality(self, report_content: str, topic_documents: List[Dict[str, Any]],
                               topic_id: str = None) -> Tuple[QualityMetrics, Dict[str, Any]]:
        """
        ç»¼åˆè¯„ä¼°æŠ¥å‘Šè´¨é‡ - ç®€åŒ–æŠ¥å‘Šå‹å¥½ç‰ˆæœ¬
        
        Args:
            report_content: æŠ¥å‘Šå†…å®¹
            topic_documents: åŸå§‹ä¸»é¢˜æ–‡æ¡£
            topic_id: ä¸»é¢˜ID (å¯é€‰)
        
        Returns:
            (QualityMetrics, è¯¦ç»†åˆ†æç»“æœ)
        """
        logger.info(f"ğŸ” å¼€å§‹è¯„ä¼°æŠ¥å‘Šè´¨é‡: {topic_id}")
        
        detailed_analysis = {
            'topic_id': topic_id,
            'report_stats': self._analyze_basic_stats(report_content),
            'relevance_analysis': {},
            'information_analysis': {},
            'coherence_analysis': {},
            'factual_analysis': {},
            'technical_analysis': {},
            'structural_analysis': {},
            'recommendations': []
        }
        
        # 1. ç›¸å…³æ€§è¯„ä¼° - ç®€åŒ–æŠ¥å‘Šä¼˜åŒ–ç‰ˆæœ¬
        relevance_score, relevance_details = self._evaluate_relevance_optimized(report_content, topic_documents)
        detailed_analysis['relevance_analysis'] = relevance_details
        
        # 2. ä¿¡æ¯å¯†åº¦è¯„ä¼° - å¢å¼ºç‰ˆæœ¬
        info_density, info_details = self._evaluate_information_density_enhanced(report_content)
        detailed_analysis['information_analysis'] = info_details
        
        # 3. è¿è´¯æ€§è¯„ä¼° - æ›´çµæ´»çš„æ ‡å‡†
        coherence_score, coherence_details = self._evaluate_coherence_flexible(report_content)
        detailed_analysis['coherence_analysis'] = coherence_details
        
        # 4. äº‹å®ä¸°å¯Œåº¦è¯„ä¼° - æ›´æ³¨é‡è´¨é‡è€Œéæ•°é‡
        factual_richness, factual_details = self._evaluate_factual_richness_enhanced(report_content)
        detailed_analysis['factual_analysis'] = factual_details
        
        # 5. æŠ€æœ¯æ·±åº¦è¯„ä¼° - é™ä½è¦æ±‚
        technical_depth, technical_details = self._evaluate_technical_depth_balanced(report_content)
        detailed_analysis['technical_analysis'] = technical_details
        
        # 6. ç»“æ„è´¨é‡è¯„ä¼° - æ›´é€‚åˆç®€åŒ–æŠ¥å‘Š
        structural_quality, structural_details = self._evaluate_structural_quality_simplified(report_content)
        detailed_analysis['structural_analysis'] = structural_details
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        overall_score = (
            relevance_score * self.weight_config['relevance'] +
            info_density * self.weight_config['information_density'] +
            coherence_score * self.weight_config['coherence'] +
            factual_richness * self.weight_config['factual_richness'] +
            technical_depth * self.weight_config['technical_depth'] +
            structural_quality * self.weight_config['structural_quality']
        )
        
        # ç¡®å®šè´¨é‡ç­‰çº§
        grade = self._determine_quality_grade(overall_score)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        recommendations = self._generate_improvement_recommendations(detailed_analysis, overall_score)
        detailed_analysis['recommendations'] = recommendations
        
        metrics = QualityMetrics(
            relevance_score=relevance_score,
            information_density=info_density,
            coherence_score=coherence_score,
            factual_richness=factual_richness,
            technical_depth=technical_depth,
            structural_quality=structural_quality,
            overall_score=overall_score,
            grade=grade
        )
        
        logger.info(f"âœ… è´¨é‡è¯„ä¼°å®Œæˆ: {grade}çº§ (åˆ†æ•°: {overall_score:.3f})")
        logger.info(f"  keyword_matching: {relevance_details.get('keyword_overlap_ratio', 0.0):.3f}")
        logger.info(f"  tfidf_similarity: {relevance_details.get('tfidf_similarity', 0.0):.3f}")
        logger.info(f"  semantic_overlap: {relevance_details.get('semantic_overlap', 0.0):.3f}")
        logger.info(f"  entity_consistency: {relevance_details.get('entity_consistency', 0.0):.3f}")
        
        return metrics, detailed_analysis
    
    def _analyze_basic_stats(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        try:
            sentences = sent_tokenize(text)
            words = word_tokenize(text.lower())
            non_stop_words = [w for w in words if w.isalpha() and w not in self.stop_words]
        except:
            # NLTKæ•…éšœæ—¶çš„ç®€å•åå¤‡æ–¹æ¡ˆ
            sentences = text.split('.')
            words = text.lower().split()
            non_stop_words = [w for w in words if w.isalpha() and len(w) > 2]
        
        return {
            'total_chars': len(text),
            'total_words': len(words),
            'total_sentences': len(sentences),
            'unique_words': len(set(non_stop_words)),
            'avg_sentence_length': len(words) / max(len(sentences), 1),
            'vocabulary_richness': len(set(non_stop_words)) / max(len(non_stop_words), 1)
        }
    
    def _evaluate_relevance_optimized(self, report: str, documents: List[Dict[str, Any]]) -> Tuple[float, Dict[str, Any]]:
        """è¯„ä¼°æŠ¥å‘Šä¸åŸå§‹æ–‡æ¡£çš„ç›¸å…³æ€§ - ç®€åŒ–æŠ¥å‘Šä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # æå–åŸå§‹æ–‡æ¡£æ–‡æœ¬
            doc_texts = []
            for doc in documents:
                content = doc.get('content', '')
                if content:
                    doc_texts.append(content)
            
            if not doc_texts:
                return 0.5, {'error': 'æ²¡æœ‰å¯ç”¨çš„åŸå§‹æ–‡æ¡£', 'fallback_score': 0.5}
            
            # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
            combined_docs = ' '.join(doc_texts)
            
            # 1. å…³é”®è¯é‡å åˆ†æ - æ›´å®½æ¾
            report_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', report.lower()))
            doc_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', combined_docs.lower()))
            
            # è¿‡æ»¤å¸¸è§è¯æ±‡
            common_words = ['the', 'and', 'that', 'this', 'with', 'from', 'they', 'were', 'been', 'have']
            report_words -= set(common_words)
            doc_words -= set(common_words)
            
            if report_words and doc_words:
                overlap = len(report_words.intersection(doc_words))
                # ä½¿ç”¨æ›´å‹å¥½çš„è®¡ç®—ï¼šé‡å è¯ / æŠ¥å‘Šè¯æ±‡æ•°
                keyword_overlap_ratio = overlap / len(report_words)
            else:
                keyword_overlap_ratio = 0.0
            
            # 2. TF-IDFç›¸ä¼¼åº¦ - æ›´å®½æ¾çš„å‚æ•°
            tfidf_similarity = 0.0
            try:
                vectorizer = TfidfVectorizer(
                    max_features=300,  # å‡å°‘ç‰¹å¾æ•°
                    stop_words='english',
                    ngram_range=(1, 1),  # åªä½¿ç”¨å•è¯
                    min_df=1,  # å…è®¸ä½é¢‘è¯
                    max_df=0.95  # å…è®¸é«˜é¢‘è¯
                )
                
                corpus = [report, combined_docs]
                tfidf_matrix = vectorizer.fit_transform(corpus)
                if tfidf_matrix.shape[0] >= 2:
                    tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            except Exception as e:
                logger.warning(f"TF-IDFè®¡ç®—å¤±è´¥: {e}")
                tfidf_similarity = 0.0
            
            # 3. è¯­ä¹‰é‡å  - åŸºäºæ¦‚å¿µè€Œéå­—é¢åŒ¹é…
            semantic_overlap = self._calculate_semantic_overlap(report, combined_docs)
            
            # 4. å®ä½“ä¸€è‡´æ€§ - ä¸“æœ‰åè¯åŒ¹é…
            entity_consistency = self._calculate_entity_consistency(report, combined_docs)
            
            # ç»¼åˆç›¸å…³æ€§åˆ†æ•° - ç»™è¯­ä¹‰é‡å æ›´é«˜æƒé‡
            relevance_score = (
                keyword_overlap_ratio * 0.25 +
                tfidf_similarity * 0.25 +
                semantic_overlap * 0.35 +
                entity_consistency * 0.15
            )
            
            details = {
                'keyword_overlap_ratio': keyword_overlap_ratio,
                'tfidf_similarity': tfidf_similarity,
                'semantic_overlap': semantic_overlap,
                'entity_consistency': entity_consistency,
                'combined_relevance': relevance_score,
                'relevance_level': 'High' if relevance_score > 0.6 else 'Medium' if relevance_score > 0.3 else 'Low'
            }
            
            return min(relevance_score, 1.0), details
            
        except Exception as e:
            logger.error(f"ç›¸å…³æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.4, {'error': str(e), 'fallback_used': True}
    
    def _calculate_semantic_overlap(self, report: str, documents: str) -> float:
        """è®¡ç®—è¯­ä¹‰é‡å  - åŸºäºæ¦‚å¿µè€Œéå­—é¢åŒ¹é…"""
        # æå–æ•°å­—ä¿¡æ¯
        report_numbers = set(re.findall(r'\d+\.?\d*(?:\s*%)?', report))
        doc_numbers = set(re.findall(r'\d+\.?\d*(?:\s*%)?', documents))
        number_overlap = len(report_numbers.intersection(doc_numbers)) / max(len(report_numbers), 1) if report_numbers else 0
        
        # æå–æŠ€æœ¯æ¦‚å¿µ
        tech_concepts = []
        for category, terms in self.technical_terms.items():
            tech_concepts.extend(terms)
        
        report_tech = set(word for word in tech_concepts if word in report.lower())
        doc_tech = set(word for word in tech_concepts if word in documents.lower())
        tech_overlap = len(report_tech.intersection(doc_tech)) / max(len(report_tech), 1) if report_tech else 0
        
        # æå–å¹´ä»½ä¿¡æ¯
        report_years = set(re.findall(r'\b(19|20)\d{2}\b', report))
        doc_years = set(re.findall(r'\b(19|20)\d{2}\b', documents))
        year_overlap = len(report_years.intersection(doc_years)) / max(len(report_years), 1) if report_years else 0
        
        # ç»¼åˆè¯­ä¹‰é‡å åˆ†æ•°
        semantic_score = (number_overlap * 0.4 + tech_overlap * 0.4 + year_overlap * 0.2)
        return min(semantic_score, 1.0)
    
    def _calculate_entity_consistency(self, report: str, documents: str) -> float:
        """è®¡ç®—å®ä½“ä¸€è‡´æ€§"""
        # æå–ä¸“æœ‰åè¯
        report_entities = set(re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', report))
        doc_entities = set(re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', documents))
        
        if not report_entities:
            return 0.5  # ä¸­æ€§åˆ†æ•°
        
        entity_overlap = len(report_entities.intersection(doc_entities))
        return entity_overlap / len(report_entities)
    
    def _evaluate_information_density_enhanced(self, text: str) -> Tuple[float, Dict[str, Any]]:
        """è¯„ä¼°ä¿¡æ¯å¯†åº¦ - å¢å¼ºç‰ˆæœ¬"""
        # è¯†åˆ«ä¿¡æ¯æ€§å†…å®¹
        numerical_count = len(re.findall(r'\d+\.?\d*', text))
        factual_indicators = ['research', 'study', 'analysis', 'data', 'result', 'finding', 'evidence', 'showed', 'found']
        factual_count = sum(text.lower().count(word) for word in factual_indicators)
        
        # è¯†åˆ«å…·ä½“ç»†èŠ‚
        specific_patterns = [
            r'\d{4}å¹´?',           # å¹´ä»½
            r'\d+\.?\d*\s*%',      # ç™¾åˆ†æ¯”
            r'\d+\.?\d*\s*[a-zA-Z]+', # æ•°å­—+å•ä½
            r'[A-Z][a-z]+\s+et\s+al\.', # ä½œè€…å¼•ç”¨
            r'\b[A-Z][a-z]+\s+(University|Institute|Hospital)', # æœºæ„åç§°
        ]
        
        specific_details = sum(len(re.findall(pattern, text)) for pattern in specific_patterns)
        
        # è®¡ç®—å¯†åº¦åˆ†æ•° - æ›´å‹å¥½çš„è®¡ç®—æ–¹å¼
        text_length = len(text.split())
        if text_length == 0:
            return 0.0, {'error': 'æ–‡æœ¬ä¸ºç©º'}
        
        # åŸºç¡€ä¿¡æ¯å¯†åº¦
        base_density = (numerical_count + factual_count + specific_details) / text_length
        # å½’ä¸€åŒ–åˆ°0-1åŒºé—´ï¼Œä½†ç»™è¾ƒå°çš„æ–‡æœ¬æ›´å¤šå®¹å¿åº¦
        info_density = min(base_density * 8, 1.0)  # è°ƒæ•´ç³»æ•°
        
        details = {
            'numerical_elements': numerical_count,
            'factual_indicators': factual_count,
            'specific_details': specific_details,
            'total_words': text_length,
            'density_ratio': info_density,
            'density_level': 'High' if info_density > 0.5 else 'Medium' if info_density > 0.25 else 'Low'
        }
        
        return info_density, details
    
    def _evaluate_coherence_flexible(self, text: str) -> Tuple[float, Dict[str, Any]]:
        """è¯„ä¼°æ–‡æœ¬è¿è´¯æ€§ - æ›´çµæ´»çš„æ ‡å‡†"""
        try:
            sentences = sent_tokenize(text)
        except:
            sentences = text.split('.')
        
        if len(sentences) < 2:
            return 0.7, {'warning': 'å¥å­æ•°é‡ä¸è¶³ï¼Œç»™äºˆä¸­ç­‰åˆ†æ•°'}
        
        # åŸºç¡€è¿è´¯æ€§ - ç®€åŒ–æŠ¥å‘Šé€šå¸¸è¾ƒçŸ­ï¼Œç»™äºˆåŸºç¡€åˆ†æ•°
        base_coherence = 0.6
        
        # è¿æ¥è¯åˆ†æ - é™ä½è¦æ±‚
        connectives = ['however', 'furthermore', 'moreover', 'therefore', 'thus', 'consequently', 
                      'additionally', 'meanwhile', 'similarly', 'also', 'but', 'and', 'while']
        connective_count = sum(text.lower().count(conn) for conn in connectives)
        connective_bonus = min(connective_count / len(sentences) * 0.3, 0.2)
        
        # ä¸»é¢˜è¯é‡å¤åˆ†æ - æ›´å®½æ¾
        sentence_words = []
        for sent in sentences:
            try:
                words = word_tokenize(sent.lower())
            except:
                words = sent.lower().split()
            content_words = [w for w in words if w.isalpha() and w not in self.stop_words and len(w) > 3]
            sentence_words.append(set(content_words))
        
        # è®¡ç®—ç›¸é‚»å¥å­çš„è¯æ±‡é‡å 
        overlaps = []
        for i in range(len(sentence_words) - 1):
            overlap = len(sentence_words[i].intersection(sentence_words[i + 1]))
            total_words = len(sentence_words[i].union(sentence_words[i + 1]))
            if total_words > 0:
                overlaps.append(overlap / total_words)
        
        avg_overlap = sum(overlaps) / max(len(overlaps), 1)
        overlap_bonus = avg_overlap * 0.2
        
        # ç»¼åˆè¿è´¯æ€§åˆ†æ•°
        coherence_score = base_coherence + connective_bonus + overlap_bonus
        
        details = {
            'connective_count': connective_count,
            'avg_sentence_overlap': avg_overlap,
            'sentence_count': len(sentences),
            'base_coherence': base_coherence,
            'coherence_level': 'High' if coherence_score > 0.7 else 'Medium' if coherence_score > 0.5 else 'Low'
        }
        
        return min(coherence_score, 1.0), details
    
    def _evaluate_factual_richness_enhanced(self, text: str) -> Tuple[float, Dict[str, Any]]:
        """è¯„ä¼°äº‹å®ä¸°å¯Œåº¦ - æ›´æ³¨é‡è´¨é‡è€Œéæ•°é‡"""
        factual_elements = {
            'numbers': len(re.findall(r'\d+\.?\d*', text)),
            'percentages': len(re.findall(r'\d+\.?\d*\s*%', text)),
            'years': len(re.findall(r'\b(19|20)\d{2}\b', text)),
            'measurements': len(re.findall(r'\d+\.?\d*\s*(kg|km|cm|mm|ml|gb|mb|hz|mhz)', text, re.IGNORECASE)),
            'citations': len(re.findall(r'[A-Z][a-z]+\s+et\s+al\.', text)),
            'institutions': len(re.findall(r'\b(University|Institute|Laboratory|College|School|Hospital)\b', text, re.IGNORECASE)),
        }
        
        # è¯†åˆ«æŠ€æœ¯æœ¯è¯­ - æ›´æ™ºèƒ½çš„è®¡ç®—
        tech_score = 0
        for category, terms in self.technical_terms.items():
            category_count = sum(1 for term in terms if term in text.lower())
            # ç»™æ¯ä¸ªç±»åˆ«æœ€å¤šè´¡çŒ®1åˆ†ï¼Œé¿å…å•ä¸€ç±»åˆ«è¿‡åº¦è®¡åˆ†
            tech_score += min(category_count, 1)
        
        factual_elements['technical_terms'] = tech_score
        
        # è®¡ç®—æ€»åˆ† - æ›´å¹³è¡¡çš„è®¡ç®—æ–¹å¼
        # ç»™ä¸åŒç±»å‹çš„äº‹å®å…ƒç´ ä¸åŒæƒé‡
        weighted_elements = (
            factual_elements['numbers'] * 1.5 +
            factual_elements['percentages'] * 2.0 +
            factual_elements['years'] * 1.5 +
            factual_elements['measurements'] * 2.0 +
            factual_elements['citations'] * 2.5 +
            factual_elements['institutions'] * 2.0 +
            factual_elements['technical_terms'] * 1.0
        )
        
        text_length = len(text.split())
        if text_length == 0:
            return 0.0, {'error': 'æ–‡æœ¬ä¸ºç©º'}
        
        # æ›´å‹å¥½çš„ä¸°å¯Œåº¦è®¡ç®—
        richness_score = min(weighted_elements / text_length * 3, 1.0)
        
        details = factual_elements.copy()
        details.update({
            'weighted_factual_score': weighted_elements,
            'text_length': text_length,
            'richness_ratio': richness_score,
            'richness_level': 'High' if richness_score > 0.4 else 'Medium' if richness_score > 0.2 else 'Low'
        })
        
        return richness_score, details
    
    def _evaluate_technical_depth_balanced(self, text: str) -> Tuple[float, Dict[str, Any]]:
        """è¯„ä¼°æŠ€æœ¯æ·±åº¦ - å¹³è¡¡è¦æ±‚ï¼Œé€‚åˆç®€åŒ–æŠ¥å‘Š"""
        depth_indicators = {
            'methodology_terms': 0,
            'analysis_terms': 0,
            'research_terms': 0,
            'statistical_terms': 0,
            'domain_terms': 0
        }
        
        # æ–¹æ³•è®ºæœ¯è¯­ - æ£€æŸ¥å­˜åœ¨è€Œéè®¡æ•°
        methodology_words = ['algorithm', 'framework', 'methodology', 'approach', 'technique', 'procedure', 'protocol', 'method']
        depth_indicators['methodology_terms'] = min(sum(1 for word in methodology_words if word in text.lower()), 3)
        
        # åˆ†ææœ¯è¯­
        analysis_words = ['analysis', 'evaluation', 'assessment', 'comparison', 'validation', 'verification', 'examined']
        depth_indicators['analysis_terms'] = min(sum(1 for word in analysis_words if word in text.lower()), 3)
        
        # ç ”ç©¶æœ¯è¯­
        research_words = ['hypothesis', 'experiment', 'observation', 'measurement', 'investigation', 'study', 'research']
        depth_indicators['research_terms'] = min(sum(1 for word in research_words if word in text.lower()), 3)
        
        # ç»Ÿè®¡æœ¯è¯­
        statistical_words = ['significant', 'correlation', 'regression', 'p-value', 'confidence', 'variance', 'statistical']
        depth_indicators['statistical_terms'] = min(sum(1 for word in statistical_words if word in text.lower()), 3)
        
        # é¢†åŸŸç‰¹å®šæœ¯è¯­
        domain_words = ['clinical', 'medical', 'healthcare', 'patient', 'treatment', 'diagnosis', 'therapeutic']
        depth_indicators['domain_terms'] = min(sum(1 for word in domain_words if word in text.lower()), 3)
        
        # è®¡ç®—æŠ€æœ¯æ·±åº¦åˆ†æ•° - æ›´å®½æ¾çš„æ ‡å‡†
        total_depth_elements = sum(depth_indicators.values())
        
        # åŸºç¡€æŠ€æœ¯æ·±åº¦åˆ†æ•°
        base_depth = 0.4  # ç»™ç®€åŒ–æŠ¥å‘ŠåŸºç¡€åˆ†æ•°
        
        # é¢å¤–æŠ€æœ¯æ·±åº¦å¥–åŠ±
        if total_depth_elements > 0:
            depth_bonus = min(total_depth_elements / 10, 0.6)  # æœ€å¤šå¥–åŠ±0.6åˆ†
        else:
            depth_bonus = 0
        
        depth_score = base_depth + depth_bonus
        
        details = depth_indicators.copy()
        details.update({
            'total_depth_elements': total_depth_elements,
            'base_depth': base_depth,
            'depth_bonus': depth_bonus,
            'depth_ratio': depth_score,
            'depth_level': 'High' if depth_score > 0.7 else 'Medium' if depth_score > 0.5 else 'Low'
        })
        
        return min(depth_score, 1.0), details
    
    def _evaluate_structural_quality_simplified(self, text: str) -> Tuple[float, Dict[str, Any]]:
        """è¯„ä¼°ç»“æ„è´¨é‡ - ç®€åŒ–æŠ¥å‘Šå‹å¥½ç‰ˆæœ¬"""
        # æ£€æŸ¥ä¸è‰¯ç»“æ„åŒ–æ ‡è®° - ç®€åŒ–æŠ¥å‘Šåº”è¯¥é¿å…è¿™äº›
        bad_structures = [
            'abstract:', 'introduction:', 'conclusion:', 'summary:',
            '## abstract', '## introduction', '## conclusion', '## summary',
            '1. introduction', '2. methodology', '3. results', '4. conclusion'
        ]
        
        bad_structure_count = sum(text.lower().count(marker) for marker in bad_structures)
        
        # åŸºç¡€ç»“æ„åˆ†æ•° - ç®€åŒ–æŠ¥å‘Šç»™äºˆæ›´é«˜åŸºç¡€åˆ†æ•°
        base_structure_score = 0.8
        
        # ç»“æ„æƒ©ç½š - æ¯ä¸ªå­¦æœ¯ç»“æ„æ ‡è®°æ‰£åˆ†
        structure_penalty = min(bad_structure_count * 0.3, 0.6)
        
        # å¥å­é•¿åº¦åˆ†æ - æ›´å®½æ¾çš„æ ‡å‡†
        try:
            sentences = sent_tokenize(text)
        except:
            sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        if sentences:
            sentence_lengths = [len(s.split()) for s in sentences]
            avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)
            
            # ç®€åŒ–æŠ¥å‘Šçš„ç†æƒ³å¥å­é•¿åº¦ï¼š10-30è¯
            if 10 <= avg_sentence_length <= 30:
                sentence_bonus = 0.1
            elif 5 <= avg_sentence_length <= 35:
                sentence_bonus = 0.05
            else:
                sentence_bonus = 0
        else:
            avg_sentence_length = 0
            sentence_bonus = 0
        
        # æµç•…æ€§å¥–åŠ± - æ£€æŸ¥è‡ªç„¶çš„è¿æ¥
        flow_indicators = ['the', 'this', 'these', 'that', 'it', 'they', 'also', 'however', 'therefore']
        flow_count = sum(1 for indicator in flow_indicators if indicator in text.lower())
        flow_bonus = min(flow_count / len(text.split()) * 5, 0.1)
        
        # è®¡ç®—æœ€ç»ˆç»“æ„è´¨é‡åˆ†æ•°
        structural_quality = base_structure_score - structure_penalty + sentence_bonus + flow_bonus
        
        details = {
            'bad_structure_count': bad_structure_count,
            'sentence_count': len(sentences),
            'avg_sentence_length': avg_sentence_length,
            'base_structure_score': base_structure_score,
            'structure_penalty': structure_penalty,
            'sentence_bonus': sentence_bonus,
            'flow_bonus': flow_bonus,
            'quality_level': 'High' if structural_quality > 0.8 else 'Medium' if structural_quality > 0.6 else 'Low'
        }
        
        return min(structural_quality, 1.0), details
    
    def _determine_quality_grade(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°ç¡®å®šè´¨é‡ç­‰çº§"""
        if score >= self.quality_thresholds['excellent']:
            return 'A'
        elif score >= self.quality_thresholds['good']:
            return 'B'
        elif score >= self.quality_thresholds['acceptable']:
            return 'C'
        elif score >= self.quality_thresholds['poor']:
            return 'D'
        else:
            return 'F'
    
    def _generate_improvement_recommendations(self, analysis: Dict[str, Any], overall_score: float) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # ç›¸å…³æ€§å»ºè®®
        if analysis['relevance_analysis'].get('cosine_similarity', 0) < 0.5:
            recommendations.append("æé«˜å†…å®¹ç›¸å…³æ€§: æ›´å¤šå¼•ç”¨å’Œæ•´åˆåŸå§‹æ–‡æ¡£çš„æ ¸å¿ƒæ¦‚å¿µ")
        
        # ä¿¡æ¯å¯†åº¦å»ºè®®
        if analysis['information_analysis'].get('density_ratio', 0) < 0.4:
            recommendations.append("å¢åŠ ä¿¡æ¯å¯†åº¦: æ·»åŠ æ›´å¤šå…·ä½“æ•°æ®ã€ç»Ÿè®¡æ•°å­—å’Œäº‹å®ç»†èŠ‚")
        
        # è¿è´¯æ€§å»ºè®®
        if analysis['coherence_analysis'].get('avg_sentence_overlap', 0) < 0.3:
            recommendations.append("æ”¹å–„æ–‡æœ¬è¿è´¯æ€§: ä½¿ç”¨æ›´å¤šè¿æ¥è¯å’Œä¿æŒä¸»é¢˜ä¸€è‡´æ€§")
        
        # äº‹å®ä¸°å¯Œåº¦å»ºè®®
        if analysis['factual_analysis'].get('richness_ratio', 0) < 0.4:
            recommendations.append("å¢å¼ºäº‹å®ä¸°å¯Œåº¦: åŒ…å«æ›´å¤šæ•°å­—ã€å¹´ä»½ã€æœºæ„åç§°ç­‰å…·ä½“ä¿¡æ¯")
        
        # æŠ€æœ¯æ·±åº¦å»ºè®®
        if analysis['technical_analysis'].get('depth_ratio', 0) < 0.4:
            recommendations.append("æå‡æŠ€æœ¯æ·±åº¦: ä½¿ç”¨æ›´å¤šä¸“ä¸šæœ¯è¯­å’Œæ–¹æ³•è®ºæ¦‚å¿µ")
        
        # ç»“æ„è´¨é‡å»ºè®®
        if analysis['structural_analysis'].get('bad_structure_count', 0) > 0:
            recommendations.append("æ”¹å–„ç»“æ„è´¨é‡: é¿å…ä½¿ç”¨æ­£å¼å­¦æœ¯ç»“æ„æ ‡è®°ï¼Œé‡‡ç”¨è‡ªç„¶æµç•…çš„å™è¿°")
        
        # ç»¼åˆå»ºè®®
        if overall_score < 0.6:
            recommendations.append("æ•´ä½“ä¼˜åŒ–: å»ºè®®é‡æ–°ç”ŸæˆæŠ¥å‘Šï¼Œé‡ç‚¹å…³æ³¨å†…å®¹ç›¸å…³æ€§å’Œä¿¡æ¯å¯†åº¦")
        
        return recommendations if recommendations else ["è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«æ”¹è¿›"]


class TopicRelevanceAnalyzer:
    """ä¸»é¢˜ç›¸å…³æ€§åˆ†æå™¨ - ä¸“é—¨åˆ†æå†…å®¹ä¸ä¸»é¢˜çš„ç›¸å…³åº¦"""
    
    def __init__(self):
        self.relevance_algorithms = {
            'keyword_matching': self._keyword_matching_relevance,
            'tfidf_similarity': self._tfidf_similarity_relevance,
            'semantic_overlap': self._semantic_overlap_relevance,
            'entity_consistency': self._entity_consistency_relevance
        }
    
    def analyze_topic_relevance(self, report: str, documents: List[Dict], topic_id: str) -> Dict[str, Any]:
        """åˆ†ææŠ¥å‘Šä¸ä¸»é¢˜çš„ç›¸å…³æ€§"""
        relevance_scores = {}
        detailed_analysis = {
            'topic_id': topic_id,
            'algorithms_used': list(self.relevance_algorithms.keys()),
            'individual_scores': {},
            'weighted_average': 0.0,
            'relevance_level': 'Unknown'
        }
        
        # åº”ç”¨æ‰€æœ‰ç›¸å…³æ€§ç®—æ³•
        for algorithm_name, algorithm_func in self.relevance_algorithms.items():
            try:
                score = algorithm_func(report, documents)
                relevance_scores[algorithm_name] = score
                detailed_analysis['individual_scores'][algorithm_name] = score
                logger.info(f"  {algorithm_name}: {score:.3f}")
            except Exception as e:
                logger.error(f"ç›¸å…³æ€§ç®—æ³• {algorithm_name} å¤±è´¥: {e}")
                relevance_scores[algorithm_name] = 0.0
                detailed_analysis['individual_scores'][algorithm_name] = 0.0
        
        # è®¡ç®—åŠ æƒå¹³å‡
        weights = {
            'keyword_matching': 0.25,
            'tfidf_similarity': 0.35,
            'semantic_overlap': 0.25,
            'entity_consistency': 0.15
        }
        
        weighted_average = sum(
            relevance_scores.get(alg, 0) * weights.get(alg, 0) 
            for alg in weights.keys()
        )
        
        detailed_analysis['weighted_average'] = weighted_average
        detailed_analysis['relevance_level'] = (
            'High' if weighted_average > 0.7 else
            'Medium' if weighted_average > 0.4 else
            'Low'
        )
        
        return detailed_analysis
    
    def _keyword_matching_relevance(self, report: str, documents: List[Dict]) -> float:
        """åŸºäºå…³é”®è¯åŒ¹é…çš„ç›¸å…³æ€§åˆ†æ - ä¼˜åŒ–ç‰ˆæœ¬"""
        # æå–æ–‡æ¡£å…³é”®è¯ (ä¼˜åŒ–ï¼šè¿‡æ»¤å¸¸è§è¯ï¼Œå…³æ³¨å®è´¨å†…å®¹)
        doc_words = set()
        for doc in documents:
            content = doc.get('content', '').lower()
            words = re.findall(r'\b[a-zA-Z]{4,}\b', content)  # å¢åŠ åˆ°4ä¸ªå­—ç¬¦ä»¥ä¸Š
            # è¿‡æ»¤æ›´å¤šåœç”¨è¯
            filtered_words = [w for w in words if w not in {
                'that', 'this', 'with', 'from', 'they', 'were', 'been', 'have', 
                'their', 'would', 'there', 'could', 'other', 'after', 'first',
                'also', 'time', 'very', 'what', 'know', 'just', 'into', 'over',
                'think', 'about', 'through', 'should', 'before', 'here', 'how'
            }]
            doc_words.update(filtered_words)
        
        # æå–æŠ¥å‘Šå…³é”®è¯
        report_words = set(re.findall(r'\b[a-zA-Z]{4,}\b', report.lower()))
        report_words = {w for w in report_words if w not in {
            'that', 'this', 'with', 'from', 'they', 'were', 'been', 'have', 
            'their', 'would', 'there', 'could', 'other', 'after', 'first',
            'also', 'time', 'very', 'what', 'know', 'just', 'into', 'over',
            'think', 'about', 'through', 'should', 'before', 'here', 'how'
        }}
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦ï¼Œä½†ç»™é‡å æ›´é«˜æƒé‡
        if not doc_words or not report_words:
            return 0.0
        
        overlap = len(doc_words.intersection(report_words))
        # ä½¿ç”¨æ›´å‹å¥½çš„è®¡ç®—æ–¹å¼ï¼šé‡å è¯ / è¾ƒå°é›†åˆå¤§å°
        denominator = min(len(doc_words), len(report_words))
        
        return overlap / denominator if denominator > 0 else 0.0
    
    def _tfidf_similarity_relevance(self, report: str, documents: List[Dict]) -> float:
        """åŸºäºTF-IDFç›¸ä¼¼åº¦çš„ç›¸å…³æ€§åˆ†æ"""
        try:
            doc_texts = [doc.get('content', '') for doc in documents if doc.get('content')]
            if not doc_texts:
                return 0.0
            
            combined_docs = ' '.join(doc_texts)
            corpus = [report, combined_docs]
            
            vectorizer = TfidfVectorizer(
                max_features=500,
                stop_words='english',
                ngram_range=(1, 2)
            )
            
            tfidf_matrix = vectorizer.fit_transform(corpus)
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            
            return similarity
        except Exception as e:
            logger.error(f"TF-IDFç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _semantic_overlap_relevance(self, report: str, documents: List[Dict]) -> float:
        """åŸºäºè¯­ä¹‰é‡å çš„ç›¸å…³æ€§åˆ†æ"""
        # ç®€åŒ–çš„è¯­ä¹‰åˆ†æ - åŸºäºå‘½åå®ä½“å’Œä¸“ä¸šæœ¯è¯­
        
        # æå–æ•°å­—å’Œä¸“ä¸šæ¦‚å¿µ
        numerical_entities = re.findall(r'\d+\.?\d*(?:\s*%|\s*[a-zA-Z]+)?', report)
        tech_terms = re.findall(r'\b(?:algorithm|method|system|analysis|study|research|data|model)\w*\b', 
                               report.lower())
        
        doc_numerical = []
        doc_tech = []
        for doc in documents:
            content = doc.get('content', '')
            doc_numerical.extend(re.findall(r'\d+\.?\d*(?:\s*%|\s*[a-zA-Z]+)?', content))
            doc_tech.extend(re.findall(r'\b(?:algorithm|method|system|analysis|study|research|data|model)\w*\b', 
                                     content.lower()))
        
        # è®¡ç®—æ¦‚å¿µé‡å 
        numerical_overlap = len(set(numerical_entities).intersection(set(doc_numerical)))
        tech_overlap = len(set(tech_terms).intersection(set(doc_tech)))
        
        total_report_concepts = len(set(numerical_entities + tech_terms))
        total_overlaps = numerical_overlap + tech_overlap
        
        return total_overlaps / max(total_report_concepts, 1)
    
    def _entity_consistency_relevance(self, report: str, documents: List[Dict]) -> float:
        """åŸºäºå®ä½“ä¸€è‡´æ€§çš„ç›¸å…³æ€§åˆ†æ"""
        # æå–ä¸“æœ‰åè¯å’Œæœºæ„åç§°
        report_entities = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', report)
        
        doc_entities = []
        for doc in documents:
            content = doc.get('content', '')
            entities = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', content)
            doc_entities.extend(entities)
        
        if not report_entities or not doc_entities:
            return 0.5  # ä¸­æ€§åˆ†æ•°
        
        # è®¡ç®—å®ä½“é‡å 
        common_entities = set(report_entities).intersection(set(doc_entities))
        total_entities = set(report_entities).union(set(doc_entities))
        
        return len(common_entities) / len(total_entities) if total_entities else 0.0


def test_report_quality_evaluation():
    """æµ‹è¯•æŠ¥å‘Šè´¨é‡è¯„ä¼°ç³»ç»Ÿ"""
    print("=== æŠ¥å‘Šè´¨é‡è¯„ä¼°ç³»ç»Ÿæµ‹è¯• ===")
    
    # æµ‹è¯•æ•°æ®
    test_report = """
    This study presents a comprehensive analysis of machine learning algorithms 
    for predictive modeling in healthcare applications. The research involved 
    1,200 patients from Stanford University Hospital and achieved 94.2% accuracy 
    using a deep neural network approach. The methodology included data preprocessing, 
    feature selection, and cross-validation techniques. Statistical significance 
    was confirmed with p < 0.001. The results demonstrate significant improvements 
    over traditional methods, with 15% better performance compared to logistic regression.
    """
    
    test_documents = [
        {"content": "Healthcare machine learning study with 1200 patients at Stanford Hospital"},
        {"content": "Deep neural networks achieved 94.2% accuracy in medical prediction tasks"},
        {"content": "Statistical analysis showed p < 0.001 significance in healthcare ML research"}
    ]
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ReportQualityEvaluator()
    
    # æ‰§è¡Œè¯„ä¼°
    metrics, analysis = evaluator.evaluate_report_quality(
        test_report, test_documents, "test_topic"
    )
    
    print(f"æ•´ä½“è¯„åˆ†: {metrics.overall_score:.3f} ({metrics.grade}çº§)")
    print(f"ç›¸å…³æ€§: {metrics.relevance_score:.3f}")
    print(f"ä¿¡æ¯å¯†åº¦: {metrics.information_density:.3f}")
    print(f"è¿è´¯æ€§: {metrics.coherence_score:.3f}")
    print(f"äº‹å®ä¸°å¯Œåº¦: {metrics.factual_richness:.3f}")
    print(f"æŠ€æœ¯æ·±åº¦: {metrics.technical_depth:.3f}")
    print(f"ç»“æ„è´¨é‡: {metrics.structural_quality:.3f}")
    
    print("\næ”¹è¿›å»ºè®®:")
    for i, rec in enumerate(analysis['recommendations'], 1):
        print(f"{i}. {rec}")

if __name__ == "__main__":
    test_report_quality_evaluation() 