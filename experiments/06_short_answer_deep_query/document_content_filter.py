#!/usr/bin/env python3
"""
æ–‡æ¡£å†…å®¹è¿‡æ»¤å’Œæ¸…æ´—æ¨¡å—
ç”¨äºä»åŸå§‹ClueWeb22æ–‡æ¡£ä¸­æå–æœ‰ä»·å€¼çš„å­¦æœ¯å†…å®¹
"""

import re
import logging
from typing import List, Dict, Set
from pathlib import Path

logger = logging.getLogger(__name__)

class DocumentContentFilter:
    """æ–‡æ¡£å†…å®¹è¿‡æ»¤å™¨"""
    
    def __init__(self):
        # å®šä¹‰æ— ä»·å€¼å†…å®¹çš„æ¨¡å¼
        self.noise_patterns = [
            # HTMLæ ‡ç­¾å’ŒCSS
            r'<[^>]+>',
            r'\.css\{[^}]*\}',
            r'@media[^{]*\{[^}]*\}',
            r'font-family:[^;]*;',
            r'background-color:[^;]*;',
            r'margin:[^;]*;',
            r'padding:[^;]*;',
            
            # JavaScript
            r'<script[^>]*>.*?</script>',
            r'function\s+\w+\([^)]*\)\s*\{[^}]*\}',
            r'var\s+\w+\s*=\s*[^;]*;',
            r'document\.\w+',
            
            # å¯¼èˆªå’Œèœå•
            r'(navigation|menu|navbar|sidebar|header|footer)',
            r'(home|about|contact|search|login|register)',
            r'(privacy policy|terms of service|cookie policy)',
            
            # å¹¿å‘Šå’Œå•†ä¸šå†…å®¹
            r'(advertisement|sponsored|affiliate|promotion)',
            r'(buy now|add to cart|purchase|order)',
            r'(discount|sale|offer|deal)',
            
            # ç¤¾äº¤åª’ä½“
            r'(facebook|twitter|instagram|linkedin|youtube)',
            r'(share|like|follow|subscribe)',
            r'(comment|reply|post)',
            
            # æŠ€æœ¯å™ªéŸ³
            r'(cookie|session|cache|database)',
            r'(404|error|not found|page not found)',
            r'(loading|please wait)',
            
            # é‡å¤å­—ç¬¦å’Œæ— æ„ä¹‰ç¬¦å·
            r'[^\w\s.!?,:;()-]{3,}',
            r'(.)\1{5,}',  # è¿ç»­é‡å¤å­—ç¬¦
        ]
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE | re.DOTALL) for pattern in self.noise_patterns]
        
        # å®šä¹‰æœ‰ä»·å€¼å†…å®¹çš„æŒ‡æ ‡
        self.valuable_indicators = [
            # å­¦æœ¯ç ”ç©¶è¯æ±‡
            r'\b(research|study|experiment|analysis|methodology|findings|results|conclusion|data|statistical|significant|hypothesis|theory|model|framework|approach|technique|method|algorithm|procedure|investigation|evaluation|assessment|validation|verification|comparison|correlation|regression|classification|optimization|simulation|empirical|theoretical|quantitative|qualitative)\b',
            
            # æ•°å­—å’Œåº¦é‡
            r'\b\d+\.?\d*\s*(percent|percentage|%|degree|celsius|fahrenheit|meter|kilometer|gram|kilogram|second|minute|hour|day|year|sample|participant|subject|trial|iteration|epoch|accuracy|precision|recall|f1|score|rate|ratio|coefficient|p-value|confidence|interval|standard|deviation|mean|median|variance|correlation)\b',
            
            # æœºæ„å’Œå‡ºç‰ˆ
            r'\b(university|college|institute|laboratory|department|center|journal|conference|proceedings|publication|paper|article|thesis|dissertation|report|patent|standard|specification|guideline)\b',
            
            # æŠ€æœ¯æœ¯è¯­
            r'\b(machine learning|artificial intelligence|deep learning|neural network|natural language processing|computer vision|data mining|big data|cloud computing|internet of things|blockchain|cybersecurity|software engineering|database|algorithm|programming|development|technology|innovation|digital|electronic|computational|automated|intelligent|adaptive|predictive|analytics|optimization|performance|efficiency|scalability|reliability|security|privacy)\b',
        ]
        
        self.valuable_pattern = re.compile('|'.join(self.valuable_indicators), re.IGNORECASE)
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ï¼Œç§»é™¤å™ªéŸ³å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', ' ', text)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤å™ªéŸ³æ¨¡å¼
        for pattern in self.compiled_patterns:
            text = pattern.sub(' ', text)
        
        # å†æ¬¡æ¸…ç†ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_sentences(self, text: str) -> List[str]:
        """æå–æœ‰æ„ä¹‰çš„å¥å­"""
        if not text:
            return []
        
        # æŒ‰å¥å·åˆ†å‰²
        sentences = re.split(r'[.!?]+', text)
        
        valuable_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            
            # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªé•¿çš„å¥å­
            if len(sentence) < 20 or len(sentence) > 500:
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰ä»·å€¼çš„æŒ‡æ ‡
            if self.valuable_pattern.search(sentence):
                valuable_sentences.append(sentence)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—ä¿¡æ¯
            elif re.search(r'\d+', sentence):
                valuable_sentences.append(sentence)
        
        return valuable_sentences
    
    def calculate_content_value_score(self, text: str) -> float:
        """è®¡ç®—å†…å®¹ä»·å€¼åˆ†æ•° (0-1)"""
        if not text:
            return 0.0
        
        # è®¡ç®—æœ‰ä»·å€¼æŒ‡æ ‡çš„å¯†åº¦
        valuable_matches = len(self.valuable_pattern.findall(text))
        total_words = len(text.split())
        
        if total_words == 0:
            return 0.0
        
        # è®¡ç®—å¯†åº¦åˆ†æ•°
        density_score = min(valuable_matches / total_words * 10, 1.0)
        
        # è®¡ç®—é•¿åº¦åˆ†æ•°ï¼ˆé€‚ä¸­é•¿åº¦æ›´å¥½ï¼‰
        length_score = min(len(text) / 1000, 1.0) if len(text) < 2000 else max(1.0 - (len(text) - 2000) / 5000, 0.1)
        
        # è®¡ç®—æ•°å­—ä¿¡æ¯åˆ†æ•°
        number_matches = len(re.findall(r'\d+\.?\d*', text))
        number_score = min(number_matches / total_words * 20, 0.3)
        
        # ç»¼åˆåˆ†æ•°
        final_score = (density_score * 0.6 + length_score * 0.3 + number_score * 0.1)
        
        return min(final_score, 1.0)
    
    def filter_document(self, document: Dict[str, str]) -> Dict[str, str]:
        """è¿‡æ»¤å•ä¸ªæ–‡æ¡£"""
        content = document.get('content', '')
        
        # æ¸…æ´—æ–‡æœ¬
        cleaned_content = self.clean_text(content)
        
        # æå–æœ‰ä»·å€¼çš„å¥å­
        valuable_sentences = self.extract_sentences(cleaned_content)
        
        # é‡æ–°ç»„åˆå†…å®¹
        filtered_content = '. '.join(valuable_sentences)
        
        # è®¡ç®—ä»·å€¼åˆ†æ•°
        value_score = self.calculate_content_value_score(filtered_content)
        
        return {
            'doc_id': document.get('doc_id', ''),
            'source': document.get('source', ''),
            'content': filtered_content,
            'word_count': len(filtered_content.split()),
            'char_count': len(filtered_content),
            'value_score': value_score,
            'valuable_sentences_count': len(valuable_sentences),
            'original_length': len(content)
        }
    
    def extract_key_information(self, text: str) -> Dict[str, List[str]]:
        """æå–å…³é”®ä¿¡æ¯ç±»åˆ«"""
        key_info = {
            'numerical_data': [],
            'technical_terms': [],
            'institutions': [],
            'methodologies': [],
            'findings': []
        }
        
        # æå–æ•°å€¼æ•°æ®
        numerical_patterns = [
            r'\b\d+\.?\d*\s*(?:percent|%|accuracy|precision|recall|f1|score)\b',
            r'\b\d+\.?\d*\s*(?:participants|samples|subjects|trials|epochs|iterations)\b',
            r'\bp\s*[<>=]\s*0\.\d+\b',  # p-values
            r'\b\d+\.?\d*\s*(?:years?|months?|days?|hours?|minutes?)\b'
        ]
        
        for pattern in numerical_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            key_info['numerical_data'].extend(matches)
        
        # æå–æŠ€æœ¯æœ¯è¯­
        tech_terms = re.findall(r'\b(?:algorithm|model|framework|technique|method|approach|system|architecture|neural network|transformer|BERT|GPT|CNN|RNN|LSTM|SVM|random forest|clustering|classification|regression|optimization|deep learning|machine learning|AI|NLP|computer vision)\b', text, re.IGNORECASE)
        key_info['technical_terms'] = list(set(tech_terms))
        
        # æå–æœºæ„ä¿¡æ¯
        institutions = re.findall(r'\b(?:university|college|institute|laboratory|lab|research center|department|Google|Microsoft|OpenAI|Stanford|MIT|Harvard|Berkeley|CMU)\b', text, re.IGNORECASE)
        key_info['institutions'] = list(set(institutions))
        
        # æå–æ–¹æ³•è®º
        methodologies = re.findall(r'\b(?:experiment|study|analysis|evaluation|assessment|validation|comparison|survey|review|investigation|simulation|training|testing|cross-validation|ablation study|hyperparameter tuning)\b', text, re.IGNORECASE)
        key_info['methodologies'] = list(set(methodologies))
        
        # æå–å‘ç°å’Œç»“æœ
        findings = re.findall(r'\b(?:results?|findings?|conclusions?|outcomes?|achievements?|improvements?|performance|accuracy|effectiveness|efficiency|significant|outperform|state-of-the-art|breakthrough|novel|innovative)\b', text, re.IGNORECASE)
        key_info['findings'] = list(set(findings))
        
        return key_info

    def is_high_quality_content(self, content: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ºé«˜è´¨é‡"""
        if not content or len(content.strip()) < 100:
            return False
        
        # è®¡ç®—ä»·å€¼åˆ†æ•°
        value_score = self.calculate_content_value_score(content)
        
        # æ£€æŸ¥åŸºæœ¬è´¨é‡æŒ‡æ ‡
        word_count = len(content.split())
        has_valuable_indicators = bool(self.valuable_pattern.search(content))
        has_numbers = bool(re.search(r'\d+', content))
        
        # ç»¼åˆåˆ¤æ–­
        is_quality = (
            value_score >= 0.05 and  # åŸºæœ¬ä»·å€¼åˆ†æ•°
            word_count >= 50 and     # æœ€å°å­—æ•°
            (has_valuable_indicators or has_numbers)  # åŒ…å«æœ‰ä»·å€¼æŒ‡æ ‡æˆ–æ•°å­—
        )
        
        return is_quality
    
    def extract_clean_content(self, content: str) -> str:
        """æå–æ¸…æ´çš„å†…å®¹"""
        cleaned = self.clean_text(content)
        sentences = self.extract_sentences(cleaned)
        return '. '.join(sentences)

    def filter_documents(self, documents: List[Dict[str, str]], min_value_score: float = 0.1, target_length: int = 50000) -> List[Dict[str, str]]:
        """è¿‡æ»¤æ–‡æ¡£åˆ—è¡¨ - å¢å¼ºç‰ˆï¼Œæ™ºèƒ½é€‰æ‹©æœ€æœ‰ä»·å€¼çš„å†…å®¹"""
        filtered_docs = []
        
        # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å’Œè¯„åˆ†
        for doc in documents:
            filtered_doc = self.filter_document(doc)
            
            # åªä¿ç•™æœ‰ä»·å€¼çš„æ–‡æ¡£
            if (filtered_doc['value_score'] >= min_value_score and 
                filtered_doc['word_count'] >= 20 and
                filtered_doc['valuable_sentences_count'] >= 2):
                
                # æ·»åŠ å…³é”®ä¿¡æ¯æå–
                key_info = self.extract_key_information(filtered_doc['content'])
                filtered_doc['key_information'] = key_info
                filtered_doc['info_density'] = sum(len(v) for v in key_info.values())
                
                filtered_docs.append(filtered_doc)
        
        # ç¬¬äºŒæ­¥ï¼šæŒ‰ç»¼åˆåˆ†æ•°æ’åº
        for doc in filtered_docs:
            # ç»¼åˆåˆ†æ•° = ä»·å€¼åˆ†æ•° * 0.6 + ä¿¡æ¯å¯†åº¦åˆ†æ•° * 0.4
            info_score = min(doc['info_density'] / 20, 1.0)  # å½’ä¸€åŒ–ä¿¡æ¯å¯†åº¦
            doc['combined_score'] = doc['value_score'] * 0.6 + info_score * 0.4
        
        filtered_docs.sort(key=lambda x: x['combined_score'], reverse=True)
        
        # ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½æˆªå–åˆ°ç›®æ ‡é•¿åº¦
        if target_length > 0:
            selected_docs = []
            total_chars = 0
            
            for doc in filtered_docs:
                if total_chars + doc['char_count'] <= target_length:
                    selected_docs.append(doc)
                    total_chars += doc['char_count']
                elif total_chars < target_length * 0.8:  # è‡³å°‘è¾¾åˆ°ç›®æ ‡çš„80%
                    # æˆªå–éƒ¨åˆ†å†…å®¹
                    remaining_chars = target_length - total_chars
                    if remaining_chars > 500:  # è‡³å°‘ç•™500å­—ç¬¦
                        truncated_content = doc['content'][:remaining_chars]
                        # ç¡®ä¿åœ¨å¥å­è¾¹ç•Œæˆªå–
                        last_period = truncated_content.rfind('.')
                        if last_period > remaining_chars * 0.8:
                            truncated_content = truncated_content[:last_period + 1]
                        
                        doc_copy = doc.copy()
                        doc_copy['content'] = truncated_content
                        doc_copy['char_count'] = len(truncated_content)
                        doc_copy['word_count'] = len(truncated_content.split())
                        doc_copy['truncated'] = True
                        selected_docs.append(doc_copy)
                    break
                else:
                    break
            
            return selected_docs
        
        return filtered_docs
    
    def get_filtering_stats(self, original_docs: List[Dict], filtered_docs: List[Dict]) -> Dict:
        """è·å–è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'original_count': len(original_docs),
            'filtered_count': len(filtered_docs),
            'retention_rate': len(filtered_docs) / len(original_docs) if original_docs else 0,
            'original_total_chars': sum(len(doc.get('content', '')) for doc in original_docs),
            'filtered_total_chars': sum(doc['char_count'] for doc in filtered_docs),
            'content_compression_rate': sum(doc['char_count'] for doc in filtered_docs) / sum(len(doc.get('content', '')) for doc in original_docs) if original_docs else 0,
            'avg_value_score': sum(doc['value_score'] for doc in filtered_docs) / len(filtered_docs) if filtered_docs else 0
        }


def test_document_filter():
    """æµ‹è¯•æ–‡æ¡£è¿‡æ»¤å™¨"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_documents = [
        {
            'doc_id': 'test_1',
            'source': 'academic_paper.txt',
            'content': 'This research investigates machine learning algorithms for natural language processing. The study used a dataset of 10,000 samples and achieved 94.2% accuracy using transformer models. The experimental results show significant improvement over baseline methods.'
        },
        {
            'doc_id': 'test_2', 
            'source': 'noise_content.txt',
            'content': '<html><head><style>body{margin:0;padding:0;}</style></head><body><nav>Home About Contact</nav><div>Advertisement: Buy now! 50% discount!</div></body></html>'
        },
        {
            'doc_id': 'test_3',
            'source': 'mixed_content.txt',
            'content': 'Navigation: Home | Products | Contact. The university researchers conducted a controlled experiment with 500 participants. Results showed statistical significance (p<0.05). Footer: Copyright 2023.'
        }
    ]
    
    # æµ‹è¯•è¿‡æ»¤å™¨
    filter = DocumentContentFilter()
    
    print("ğŸ§ª æµ‹è¯•æ–‡æ¡£å†…å®¹è¿‡æ»¤å™¨")
    print("=" * 50)
    
    for doc in test_documents:
        print(f"\nåŸå§‹æ–‡æ¡£ {doc['doc_id']}:")
        print(f"  å†…å®¹: {doc['content'][:100]}...")
        print(f"  é•¿åº¦: {len(doc['content'])} å­—ç¬¦")
        
        filtered = filter.filter_document(doc)
        
        print(f"è¿‡æ»¤å:")
        print(f"  å†…å®¹: {filtered['content'][:100]}...")
        print(f"  é•¿åº¦: {filtered['char_count']} å­—ç¬¦")
        print(f"  ä»·å€¼åˆ†æ•°: {filtered['value_score']:.3f}")
        print(f"  æœ‰ä»·å€¼å¥å­æ•°: {filtered['valuable_sentences_count']}")
    
    # æµ‹è¯•æ‰¹é‡è¿‡æ»¤
    print(f"\nğŸ“Š æ‰¹é‡è¿‡æ»¤ç»“æœ:")
    filtered_docs = filter.filter_documents(test_documents)
    stats = filter.get_filtering_stats(test_documents, filtered_docs)
    
    print(f"  åŸå§‹æ–‡æ¡£æ•°: {stats['original_count']}")
    print(f"  è¿‡æ»¤åæ–‡æ¡£æ•°: {stats['filtered_count']}")
    print(f"  ä¿ç•™ç‡: {stats['retention_rate']:.1%}")
    print(f"  å†…å®¹å‹ç¼©ç‡: {stats['content_compression_rate']:.1%}")
    print(f"  å¹³å‡ä»·å€¼åˆ†æ•°: {stats['avg_value_score']:.3f}")


if __name__ == "__main__":
    test_document_filter() 