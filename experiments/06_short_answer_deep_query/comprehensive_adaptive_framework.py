"""
Comprehensive Adaptive Framework
å®Œæ•´çš„è‡ªé€‚åº”ä¼˜åŒ–æ¡†æ¶ - æ•´åˆæ‰€æœ‰è´¨é‡æ§åˆ¶å’Œä¼˜åŒ–ç»„ä»¶
"""

import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
from dataclasses import dataclass, asdict

from report_quality_evaluation_system import ReportQualityEvaluator, TopicRelevanceAnalyzer, QualityMetrics
from answer_compression_optimizer import AnswerCompressionOptimizer, EnhancedAnswerValidator

logger = logging.getLogger(__name__)

@dataclass
class AdaptiveOptimizationConfig:
    """è‡ªé€‚åº”ä¼˜åŒ–é…ç½®"""
    # è´¨é‡é˜ˆå€¼
    min_report_quality_score: float = 0.65
    min_topic_relevance_score: float = 0.60
    min_answer_compression_success_rate: float = 0.70
    
    # ä¼˜åŒ–ç­–ç•¥
    max_optimization_iterations: int = 5
    target_success_rate: float = 0.80
    aggressive_optimization_threshold: float = 0.30
    
    # æŠ¥å‘Šç”Ÿæˆå‚æ•°
    report_length_range: Tuple[int, int] = (800, 1500)
    max_structural_violations: int = 2
    min_information_density: float = 0.40
    
    # é—®é¢˜ç”Ÿæˆå‚æ•°
    min_browsecomp_ratio: float = 0.70
    min_high_constraint_ratio: float = 0.20
    min_avg_constraints: float = 2.5
    
    # ç­”æ¡ˆè´¨é‡å‚æ•°
    max_avg_word_count: float = 12.0
    max_answer_violations_ratio: float = 0.25
    target_compression_ratio: float = 0.90

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    iteration: int
    success: bool
    before_metrics: Dict[str, Any]
    after_metrics: Dict[str, Any]
    improvements: Dict[str, float]
    optimization_actions: List[str]
    processing_time: float
    recommendation: str

class ComprehensiveAdaptiveFramework:
    """ç»¼åˆè‡ªé€‚åº”ä¼˜åŒ–æ¡†æ¶"""
    
    def __init__(self, experiment_dir: str, config: AdaptiveOptimizationConfig = None):
        self.experiment_dir = Path(experiment_dir)
        self.config = config or AdaptiveOptimizationConfig()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.report_evaluator = ReportQualityEvaluator()
        self.relevance_analyzer = TopicRelevanceAnalyzer()
        self.answer_validator = EnhancedAnswerValidator()
        
        # ä¼˜åŒ–å†å²
        self.optimization_history = []
        self.performance_trends = {
            'report_quality': [],
            'topic_relevance': [],
            'answer_quality': [],
            'overall_success_rate': []
        }
        
        logger.info(f"ğŸš€ ç»¼åˆè‡ªé€‚åº”æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ å®éªŒç›®å½•: {self.experiment_dir}")
        logger.info(f"ğŸ¯ ç›®æ ‡æˆåŠŸç‡: {self.config.target_success_rate:.1%}")
    
    def run_adaptive_optimization_cycle(self, experiment_runner, max_iterations: int = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è‡ªé€‚åº”ä¼˜åŒ–å¾ªç¯"""
        max_iterations = max_iterations or self.config.max_optimization_iterations
        
        cycle_results = {
            'start_time': datetime.now(),
            'max_iterations': max_iterations,
            'target_success_rate': self.config.target_success_rate,
            'iterations': [],
            'final_success_rate': 0.0,
            'optimization_successful': False,
            'total_improvements': {},
            'best_configuration': {}
        }
        
        logger.info(f"ğŸ”„ å¼€å§‹è‡ªé€‚åº”ä¼˜åŒ–å¾ªç¯ (æœ€å¤§{max_iterations}è½®)")
        
        current_success_rate = 0.0
        best_config = None
        best_metrics = None
        
        for iteration in range(1, max_iterations + 1):
            logger.info(f"\nğŸ“Š === ä¼˜åŒ–è½®æ¬¡ {iteration}/{max_iterations} ===")
            
            iteration_start = time.time()
            
            # Step 1: è¿è¡Œå®éªŒ
            logger.info("ğŸ§ª æ‰§è¡Œå®éªŒ...")
            # ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œå¦‚æœå®éªŒå¯¹è±¡å·²ç»é¢„é…ç½®åˆ™ä¼šä½¿ç”¨é¢„é…ç½®çš„è®¾ç½®
            experiment_result = experiment_runner.run_experiment()
            
            # Step 2: å…¨é¢åˆ†æå®éªŒç»“æœ
            logger.info("ğŸ” å…¨é¢è´¨é‡åˆ†æ...")
            comprehensive_analysis = self._comprehensive_quality_analysis(experiment_result)
            
            # Step 3: è®¡ç®—å½“å‰æˆåŠŸç‡å’Œæ”¹è¿›ç©ºé—´
            current_success_rate = comprehensive_analysis['overall_metrics']['success_rate']
            improvement_potential = self._assess_improvement_potential(comprehensive_analysis)
            
            # Step 4: ç”Ÿæˆä¼˜åŒ–ç­–ç•¥
            logger.info("ğŸ¯ ç”Ÿæˆä¼˜åŒ–ç­–ç•¥...")
            optimization_strategies = self._generate_optimization_strategies(
                comprehensive_analysis, improvement_potential, iteration
            )
            
            # Step 5: åº”ç”¨ä¼˜åŒ–
            logger.info("âš™ï¸ åº”ç”¨ä¼˜åŒ–ç­–ç•¥...")
            optimization_actions = self._apply_optimizations(
                optimization_strategies, experiment_runner
            )
            
            # Step 6: éªŒè¯ä¼˜åŒ–æ•ˆæœ
            logger.info("âœ… éªŒè¯ä¼˜åŒ–æ•ˆæœ...")
            post_optimization_result = experiment_runner.run_experiment()
            post_analysis = self._comprehensive_quality_analysis(post_optimization_result)
            new_success_rate = post_analysis['overall_metrics']['success_rate']
            
            # è®¡ç®—æ”¹è¿›æƒ…å†µ
            improvements = self._calculate_improvements(comprehensive_analysis, post_analysis)
            
            iteration_time = time.time() - iteration_start
            
            # ä¿å­˜æœ¬è½®ç»“æœ
            optimization_result = OptimizationResult(
                iteration=iteration,
                success=new_success_rate > current_success_rate,
                before_metrics=comprehensive_analysis['overall_metrics'],
                after_metrics=post_analysis['overall_metrics'],
                improvements=improvements,
                optimization_actions=optimization_actions,
                processing_time=iteration_time,
                recommendation=self._generate_recommendation(new_success_rate, iteration, max_iterations)
            )
            
            cycle_results['iterations'].append(asdict(optimization_result))
            self.optimization_history.append(optimization_result)
            
            # æ›´æ–°æ€§èƒ½è¶‹åŠ¿
            self._update_performance_trends(post_analysis)
            
            # è®°å½•æœ€ä½³é…ç½®
            if new_success_rate > current_success_rate or best_config is None:
                best_config = self._save_current_configuration(experiment_runner)
                best_metrics = post_analysis['overall_metrics']
                cycle_results['best_configuration'] = best_config
            
            logger.info(f"ğŸ“ˆ è½®æ¬¡{iteration}ç»“æœ: {current_success_rate:.1%} â†’ {new_success_rate:.1%}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if new_success_rate >= self.config.target_success_rate:
                logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æˆåŠŸç‡: {new_success_rate:.1%} >= {self.config.target_success_rate:.1%}")
                cycle_results['optimization_successful'] = True
                break
            
            # æ›´æ–°å½“å‰æˆåŠŸç‡
            current_success_rate = new_success_rate
        
        # å®Œæˆä¼˜åŒ–å¾ªç¯
        cycle_results['final_success_rate'] = current_success_rate
        cycle_results['end_time'] = datetime.now()
        cycle_results['total_duration'] = (cycle_results['end_time'] - cycle_results['start_time']).total_seconds()
        
        # è®¡ç®—æ€»ä½“æ”¹è¿›
        if cycle_results['iterations']:
            first_metrics = cycle_results['iterations'][0]['before_metrics']
            final_metrics = cycle_results['iterations'][-1]['after_metrics']
            cycle_results['total_improvements'] = self._calculate_improvements(
                {'overall_metrics': first_metrics},
                {'overall_metrics': final_metrics}
            )
        
        # ä¿å­˜ç»“æœ
        self._save_optimization_cycle_results(cycle_results)
        
        logger.info(f"ğŸ ä¼˜åŒ–å¾ªç¯å®Œæˆ!")
        logger.info(f"ğŸ“Š æœ€ç»ˆæˆåŠŸç‡: {current_success_rate:.1%}")
        logger.info(f"ğŸ¯ ç›®æ ‡è¾¾æˆ: {'âœ…' if cycle_results['optimization_successful'] else 'âŒ'}")
        
        return cycle_results
    
    def _comprehensive_quality_analysis(self, experiment_result: Dict[str, Any]) -> Dict[str, Any]:
        """å…¨é¢çš„è´¨é‡åˆ†æ"""
        analysis = {
            'experiment_info': experiment_result.get('experiment_info', {}),
            'report_quality_analysis': {},
            'topic_relevance_analysis': {},
            'answer_quality_analysis': {},
            'overall_metrics': {},
            'detailed_breakdown': {}
        }
        
        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç»“æœè·¯å¾„
        detailed_results = experiment_result.get('detailed_results', [])
        if not detailed_results:
            # å¤‡ç”¨è·¯å¾„
            detailed_results = experiment_result.get('results', [])
        
        successful_results = [r for r in detailed_results if r.get('success')]
        total_results = len(detailed_results)
        
        if not successful_results:
            analysis['overall_metrics'] = {
                'success_rate': 0.0,
                'avg_report_quality': 0.0,
                'avg_topic_relevance': 0.0,
                'avg_answer_quality': 0.0,
                'total_topics': total_results
            }
            return analysis
        
        # 1. æŠ¥å‘Šè´¨é‡åˆ†æ
        report_qualities = []
        for result in successful_results:
            if 'report' in result:
                # è¿™é‡Œåº”è¯¥è°ƒç”¨æŠ¥å‘Šè´¨é‡è¯„ä¼°ï¼Œç®€åŒ–ç‰ˆæœ¬
                report_quality = self._evaluate_single_report_quality(result)
                report_qualities.append(report_quality)
        
        # 2. ä¸»é¢˜ç›¸å…³æ€§åˆ†æ
        relevance_scores = []
        for result in successful_results:
            relevance_score = self._evaluate_topic_relevance(result)
            relevance_scores.append(relevance_score)
        
        # 3. ç­”æ¡ˆè´¨é‡åˆ†æ
        answer_qualities = []
        for result in successful_results:
            if 'answers' in result:
                answer_quality = self._evaluate_answer_quality(result)
                answer_qualities.append(answer_quality)
        
        # æ±‡æ€»åˆ†æ
        analysis['overall_metrics'] = {
            'success_rate': len(successful_results) / max(total_results, 1),
            'avg_report_quality': sum(report_qualities) / max(len(report_qualities), 1),
            'avg_topic_relevance': sum(relevance_scores) / max(len(relevance_scores), 1),
            'avg_answer_quality': sum(answer_qualities) / max(len(answer_qualities), 1),
            'total_topics': total_results,
            'successful_topics': len(successful_results)
        }
        
        analysis['detailed_breakdown'] = {
            'report_qualities': report_qualities,
            'relevance_scores': relevance_scores,
            'answer_qualities': answer_qualities
        }
        
        return analysis
    
    def _evaluate_single_report_quality(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°å•ä¸ªæŠ¥å‘Šè´¨é‡ (ç®€åŒ–ç‰ˆæœ¬)"""
        try:
            # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†reportå­—æ®µï¼Œå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸
            report = result.get('report', '')
            if isinstance(report, dict):
                report_content = report.get('content', '')
            elif isinstance(report, str):
                report_content = report
            else:
                report_content = str(report) if report else ''
            
            if not report_content:
                return 0.0
            
            # ç®€å•çš„è´¨é‡æŒ‡æ ‡
            word_count = len(report_content.split())
            char_count = len(report_content)
            
            # é•¿åº¦åˆ†æ•°
            target_min, target_max = self.config.report_length_range
            if target_min <= word_count <= target_max:
                length_score = 1.0
            elif word_count < target_min:
                length_score = word_count / target_min
            else:
                length_score = max(0.5, target_max / word_count)
            
            # ä¿¡æ¯å¯†åº¦åˆ†æ•° (æ•°å­—å’Œä¸“ä¸šè¯æ±‡)
            import re
            numerical_count = len(re.findall(r'\d+\.?\d*', report_content))
            technical_words = ['research', 'study', 'analysis', 'method', 'data', 'result']
            technical_count = sum(report_content.lower().count(word) for word in technical_words)
            
            info_density = min((numerical_count + technical_count) / word_count * 10, 1.0)
            
            # ç»“æ„è´¨é‡åˆ†æ•° (é¿å…å­¦æœ¯ç»“æ„)
            bad_structures = ['abstract:', 'introduction:', 'conclusion:']
            structure_violations = sum(report_content.lower().count(marker) for marker in bad_structures)
            structure_score = max(0, 1.0 - structure_violations * 0.2)
            
            # ç»¼åˆåˆ†æ•°
            overall_score = (length_score * 0.3 + info_density * 0.4 + structure_score * 0.3)
            return overall_score
            
        except Exception as e:
            logger.error(f"æŠ¥å‘Šè´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _evaluate_topic_relevance(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°ä¸»é¢˜ç›¸å…³æ€§"""
        try:
            # ç®€åŒ–çš„ç›¸å…³æ€§è¯„ä¼°
            return 0.7  # é»˜è®¤ä¸­ç­‰ç›¸å…³æ€§
        except Exception as e:
            logger.error(f"ä¸»é¢˜ç›¸å…³æ€§è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _evaluate_answer_quality(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        try:
            # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†questionså­—æ®µï¼ˆå®é™…å­—æ®µåï¼‰
            questions = result.get('questions', [])
            if not questions:
                # å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
                questions = result.get('answers', {})
                if isinstance(questions, dict):
                    questions = questions.get('qa_pairs', [])
                elif not isinstance(questions, list):
                    questions = []
            
            if not questions:
                return 0.0
            
            total_score = 0.0
            valid_pairs = 0
            
            for qa_pair in questions:
                answer = qa_pair.get('answer', '')
                word_count = len(answer.split())
                
                # é•¿åº¦åˆ†æ•° (çŸ­ç­”æ¡ˆæ›´å¥½)
                if word_count <= 5:
                    length_score = 1.0
                elif word_count <= 10:
                    length_score = 0.8
                elif word_count <= 15:
                    length_score = 0.6
                else:
                    length_score = 0.3
                
                # å†…å®¹è´¨é‡åˆ†æ•° (åŸºäºæ˜¯å¦åŒ…å«å…·ä½“ä¿¡æ¯)
                import re
                has_numbers = bool(re.search(r'\d+', answer))
                has_specific_info = len(answer.split()) <= 3 and answer.strip()
                
                content_score = 0.5
                if has_numbers:
                    content_score += 0.3
                if has_specific_info:
                    content_score += 0.2
                
                pair_score = (length_score * 0.6 + content_score * 0.4)
                total_score += pair_score
                valid_pairs += 1
            
            return total_score / max(valid_pairs, 1)
            
        except Exception as e:
            logger.error(f"ç­”æ¡ˆè´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _assess_improvement_potential(self, analysis: Dict[str, Any]) -> Dict[str, float]:
        """è¯„ä¼°æ”¹è¿›æ½œåŠ›"""
        metrics = analysis['overall_metrics']
        
        potential = {
            'report_quality': max(0, self.config.min_report_quality_score - metrics.get('avg_report_quality', 0)),
            'topic_relevance': max(0, self.config.min_topic_relevance_score - metrics.get('avg_topic_relevance', 0)),
            'answer_quality': max(0, 0.8 - metrics.get('avg_answer_quality', 0)),  # ç›®æ ‡80%
            'success_rate': max(0, self.config.target_success_rate - metrics.get('success_rate', 0))
        }
        
        return potential
    
    def _generate_optimization_strategies(self, analysis: Dict[str, Any], 
                                        potential: Dict[str, float], 
                                        iteration: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–ç­–ç•¥"""
        strategies = []
        
        # æŠ¥å‘Šè´¨é‡ä¼˜åŒ–
        if potential['report_quality'] > 0.1:
            strategies.append({
                'type': 'report_optimization',
                'priority': 'high',
                'actions': [
                    'increase_report_length_target',
                    'enhance_information_density',
                    'reduce_structural_violations'
                ],
                'target_improvement': potential['report_quality']
            })
        
        # ç­”æ¡ˆè´¨é‡ä¼˜åŒ–
        if potential['answer_quality'] > 0.1:
            strategies.append({
                'type': 'answer_optimization',
                'priority': 'high',
                'actions': [
                    'apply_answer_compression',
                    'enhance_short_answer_detection',
                    'optimize_answer_generation_prompts'
                ],
                'target_improvement': potential['answer_quality']
            })
        
        # æˆåŠŸç‡ä¼˜åŒ– (å¦‚æœå¤šè½®ä¼˜åŒ–åä»ç„¶ä¸è¾¾æ ‡)
        if iteration > 2 and potential['success_rate'] > 0.2:
            strategies.append({
                'type': 'aggressive_optimization',
                'priority': 'critical',
                'actions': [
                    'lower_quality_thresholds',
                    'enable_emergency_mode',
                    'simplify_validation_criteria'
                ],
                'target_improvement': potential['success_rate']
            })
        
        return strategies
    
    def _apply_optimizations(self, strategies: List[Dict[str, Any]], 
                           experiment_runner) -> List[str]:
        """åº”ç”¨ä¼˜åŒ–ç­–ç•¥ - çœŸæ­£ä¿®æ”¹å®éªŒé…ç½®"""
        applied_actions = []
        
        for strategy in strategies:
            strategy_type = strategy['type']
            actions = strategy['actions']
            
            logger.info(f"  ğŸ”§ åº”ç”¨{strategy_type}ç­–ç•¥...")
            
            for action in actions:
                try:
                    if action == 'increase_report_length_target':
                        # åŠ¨æ€ä¿®æ”¹å®éªŒrunnerçš„æŠ¥å‘Šé•¿åº¦é…ç½®
                        current_min = experiment_runner.config.get('min_report_words', 600)
                        current_max = experiment_runner.config.get('max_report_words', 1500)
                        new_min = max(600, current_min - 100)
                        new_max = current_max + 200
                        
                        experiment_runner.config['min_report_words'] = new_min
                        experiment_runner.config['max_report_words'] = new_max
                        applied_actions.append(f"ğŸ“ è°ƒæ•´æŠ¥å‘Šé•¿åº¦ç›®æ ‡: {new_min}-{new_max}è¯")
                    
                    elif action == 'apply_answer_compression':
                        # åŠ¨æ€å¯ç”¨ç­”æ¡ˆå‹ç¼©
                        experiment_runner.config['enable_answer_compression'] = True
                        experiment_runner.config['compression_threshold'] = 10  # æ›´æ¿€è¿›çš„å‹ç¼©
                        applied_actions.append("å¯ç”¨ç­”æ¡ˆå‹ç¼©ä¼˜åŒ–")
                    
                    elif action == 'enhance_short_answer_detection':
                        # é™ä½ç­”æ¡ˆé•¿åº¦è¦æ±‚
                        experiment_runner.config['max_answer_words'] = max(10, 
                            experiment_runner.config.get('max_answer_words', 15) - 2)
                        applied_actions.append("å¢å¼ºçŸ­ç­”æ¡ˆæ£€æµ‹é€»è¾‘")
                    
                    elif action == 'lower_quality_thresholds':
                        # åŠ¨æ€é™ä½è´¨é‡é˜ˆå€¼
                        experiment_runner.config['min_browsecomp_ratio'] = max(0.2, 
                            experiment_runner.config.get('min_browsecomp_ratio', 0.4) * 0.9)
                        experiment_runner.config['min_high_constraint_ratio'] = max(0.1, 
                            experiment_runner.config.get('min_high_constraint_ratio', 0.15) * 0.9)
                        experiment_runner.config['min_report_quality_score'] = max(0.3, 
                            experiment_runner.config.get('min_report_quality_score', 0.45) * 0.9)
                        applied_actions.append("ğŸ“‰ é™ä½è´¨é‡é˜ˆå€¼: report={:.2f}".format(
                            experiment_runner.config['min_report_quality_score']))
                    
                    elif action == 'enable_emergency_mode':
                        # ç´§æ€¥æ¨¡å¼ï¼šå¤§å¹…é™ä½æ‰€æœ‰é˜ˆå€¼
                        experiment_runner.config['min_browsecomp_ratio'] = 0.2
                        experiment_runner.config['min_high_constraint_ratio'] = 0.1
                        experiment_runner.config['min_avg_constraints'] = 0.8
                        experiment_runner.config['min_report_quality_score'] = 0.3
                        experiment_runner.config['min_relevance_score'] = 0.1
                        applied_actions.append("ğŸš¨ å¯ç”¨ç´§æ€¥æ¨¡å¼ï¼Œå¤§å¹…é™ä½é˜ˆå€¼")
                    
                    elif action == 'optimize_answer_generation_prompts':
                        # ä¼˜åŒ–ç­”æ¡ˆç”Ÿæˆï¼ˆä¸å†åŠ¨æ€é™ä½é—®é¢˜æ•°é‡ï¼‰
                        # æ³¨é‡Šæ‰é™ä½é—®é¢˜æ•°é‡çš„é€»è¾‘ï¼Œä¿æŒç”¨æˆ·é…ç½®çš„50ä¸ªé—®é¢˜
                        # experiment_runner.config['questions_per_topic'] = min(25, 
                        #     experiment_runner.config.get('questions_per_topic', 30) - 5)
                        
                        # æ”¹ä¸ºä¼˜åŒ–å…¶ä»–å‚æ•°
                        experiment_runner.config['max_answer_words'] = max(8, 
                            experiment_runner.config.get('max_answer_words', 15) - 2)
                        applied_actions.append("ä¼˜åŒ–ç­”æ¡ˆç”Ÿæˆå‚æ•°ï¼ˆä¿æŒé—®é¢˜æ•°é‡ä¸å˜ï¼‰")
                    
                except Exception as e:
                    logger.error(f"ä¼˜åŒ–è¡ŒåŠ¨ {action} å¤±è´¥: {e}")
        
        # é‡è¦ï¼šè®°å½•å½“å‰é…ç½®çŠ¶æ€
        logger.info(f"  ğŸ“Š å½“å‰é…ç½®: BrowseCompé˜ˆå€¼={experiment_runner.config.get('min_browsecomp_ratio', 0):.2f}, "
                   f"è´¨é‡é˜ˆå€¼={experiment_runner.config.get('min_report_quality_score', 0):.2f}")
        
        return applied_actions
    

    
    def _calculate_improvements(self, before_analysis: Dict[str, Any], 
                              after_analysis: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—æ”¹è¿›æƒ…å†µ"""
        before_metrics = before_analysis['overall_metrics']
        after_metrics = after_analysis['overall_metrics']
        
        improvements = {}
        for key in before_metrics:
            if isinstance(before_metrics[key], (int, float)) and isinstance(after_metrics[key], (int, float)):
                improvement = after_metrics[key] - before_metrics[key]
                improvements[key] = improvement
        
        return improvements
    
    def _generate_recommendation(self, success_rate: float, iteration: int, max_iterations: int) -> str:
        """ç”Ÿæˆå»ºè®®"""
        if success_rate >= self.config.target_success_rate:
            return "ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡æˆåŠŸç‡ï¼Œå¯ä»¥ç»“æŸä¼˜åŒ–"
        elif iteration >= max_iterations:
            return "â° å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå»ºè®®æ‰‹åŠ¨è°ƒæ•´é…ç½®"
        elif success_rate < 0.3:
            return "ğŸš¨ æˆåŠŸç‡è¿‡ä½ï¼Œå»ºè®®æ£€æŸ¥åŸºç¡€é…ç½®"
        else:
            return "ğŸ”„ ç»§ç»­ä¼˜åŒ–ï¼Œé€æ­¥æ”¹è¿›ç³»ç»Ÿæ€§èƒ½"
    
    def _update_performance_trends(self, analysis: Dict[str, Any]):
        """æ›´æ–°æ€§èƒ½è¶‹åŠ¿"""
        metrics = analysis['overall_metrics']
        
        self.performance_trends['report_quality'].append(metrics.get('avg_report_quality', 0))
        self.performance_trends['topic_relevance'].append(metrics.get('avg_topic_relevance', 0))
        self.performance_trends['answer_quality'].append(metrics.get('avg_answer_quality', 0))
        self.performance_trends['overall_success_rate'].append(metrics.get('success_rate', 0))
        
        # ä¿æŒæœ€è¿‘10æ¬¡è®°å½•
        for key in self.performance_trends:
            if len(self.performance_trends[key]) > 10:
                self.performance_trends[key] = self.performance_trends[key][-10:]
    
    def _save_current_configuration(self, experiment_runner) -> Dict[str, Any]:
        """ä¿å­˜å½“å‰æœ€ä½³é…ç½®"""
        config_snapshot = {
            'adaptive_config': asdict(self.config),
            'timestamp': datetime.now().isoformat(),
            'performance_trends': self.performance_trends.copy()
        }
        return config_snapshot
    
    def _save_optimization_cycle_results(self, cycle_results: Dict[str, Any]):
        """ä¿å­˜ä¼˜åŒ–å¾ªç¯ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = self.experiment_dir / f"adaptive_optimization_cycle_{timestamp}.json"
        
        # è½¬æ¢datetimeå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
        def datetime_serializer(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(cycle_results, f, indent=2, ensure_ascii=False, default=datetime_serializer)
        
        logger.info(f"ğŸ’¾ ä¼˜åŒ–å¾ªç¯ç»“æœå·²ä¿å­˜: {results_file}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_optimization_summary(cycle_results, timestamp)
    
    def _generate_optimization_summary(self, cycle_results: Dict[str, Any], timestamp: str):
        """ç”Ÿæˆä¼˜åŒ–æ€»ç»“æŠ¥å‘Š"""
        summary_file = self.experiment_dir / f"adaptive_optimization_summary_{timestamp}.md"
        
        summary = []
        summary.append("# è‡ªé€‚åº”ä¼˜åŒ–å¾ªç¯æ€»ç»“æŠ¥å‘Š\n")
        summary.append(f"**æ—¶é—´**: {cycle_results['start_time']}\n")
        summary.append(f"**æ€»ç”¨æ—¶**: {cycle_results.get('total_duration', 0):.1f}ç§’\n")
        summary.append(f"**ç›®æ ‡æˆåŠŸç‡**: {cycle_results['target_success_rate']:.1%}\n")
        summary.append(f"**æœ€ç»ˆæˆåŠŸç‡**: {cycle_results['final_success_rate']:.1%}\n")
        summary.append(f"**ä¼˜åŒ–æˆåŠŸ**: {'âœ… æ˜¯' if cycle_results['optimization_successful'] else 'âŒ å¦'}\n\n")
        
        summary.append("## ä¼˜åŒ–è½®æ¬¡è¯¦æƒ…\n")
        for i, iteration in enumerate(cycle_results['iterations'], 1):
            summary.append(f"### è½®æ¬¡ {i}\n")
            summary.append(f"- **æˆåŠŸç‡å˜åŒ–**: {iteration['before_metrics']['success_rate']:.1%} â†’ {iteration['after_metrics']['success_rate']:.1%}\n")
            summary.append(f"- **å¤„ç†æ—¶é—´**: {iteration['processing_time']:.1f}ç§’\n")
            summary.append(f"- **ä¼˜åŒ–è¡ŒåŠ¨**: {', '.join(iteration['optimization_actions'])}\n")
            summary.append(f"- **å»ºè®®**: {iteration['recommendation']}\n\n")
        
        if cycle_results.get('total_improvements'):
            summary.append("## æ€»ä½“æ”¹è¿›æƒ…å†µ\n")
            for metric, improvement in cycle_results['total_improvements'].items():
                if isinstance(improvement, (int, float)):
                    summary.append(f"- **{metric}**: {improvement:+.3f}\n")
        
        summary.append("\n## æ€§èƒ½è¶‹åŠ¿\n")
        summary.append("ğŸ“ˆ æŠ¥å‘Šè´¨é‡ã€ä¸»é¢˜ç›¸å…³æ€§ã€ç­”æ¡ˆè´¨é‡å’Œæ•´ä½“æˆåŠŸç‡çš„å˜åŒ–è¶‹åŠ¿å·²è®°å½•åœ¨æ€§èƒ½æ•°æ®ä¸­ã€‚\n")
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.writelines(summary)
        
        logger.info(f"ğŸ“Š ä¼˜åŒ–æ€»ç»“å·²ä¿å­˜: {summary_file}")


def create_adaptive_framework(experiment_dir: str, config: AdaptiveOptimizationConfig = None) -> ComprehensiveAdaptiveFramework:
    """åˆ›å»ºç»¼åˆè‡ªé€‚åº”æ¡†æ¶å®ä¾‹"""
    return ComprehensiveAdaptiveFramework(experiment_dir, config)


def test_adaptive_framework():
    """æµ‹è¯•è‡ªé€‚åº”æ¡†æ¶"""
    print("=== ç»¼åˆè‡ªé€‚åº”æ¡†æ¶æµ‹è¯• ===")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = AdaptiveOptimizationConfig(
        min_report_quality_score=0.60,
        target_success_rate=0.75,
        max_optimization_iterations=3
    )
    
    # åˆ›å»ºæ¡†æ¶
    framework = ComprehensiveAdaptiveFramework("./test_results", config)
    
    print(f"æ¡†æ¶é…ç½®: ç›®æ ‡æˆåŠŸç‡ {config.target_success_rate:.1%}")
    print(f"è´¨é‡é˜ˆå€¼: æŠ¥å‘Š {config.min_report_quality_score:.2f}")
    print("æ¡†æ¶åˆå§‹åŒ–æˆåŠŸï¼")

if __name__ == "__main__":
    test_adaptive_framework() 