#!/usr/bin/env python3
"""
GPT-4o Question-Answer Quality Evaluator
=========================================

ä½¿ç”¨GPT-4oè¯„åˆ¤è‡ªå·±ç”Ÿæˆçš„é—®ç­”å¯¹è´¨é‡çš„ç³»ç»Ÿã€‚
è™½ç„¶å­˜åœ¨"è‡ªå·±è¯„ä»·è‡ªå·±"çš„ä¸»è§‚æ€§ï¼Œä½†å¯ä»¥ä½œä¸ºè´¨é‡æ£€æŸ¥çš„å‚è€ƒç»´åº¦ã€‚

ä½œè€…: Assistant
æ—¥æœŸ: 2025-01-08
ç‰ˆæœ¬: v1.0
"""

import json
import logging
import time
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class QAQualityMetrics:
    """é—®ç­”è´¨é‡è¯„åˆ†æ•°æ®ç±»"""
    question_clarity: float        # é—®é¢˜æ¸…æ™°åº¦ (0-1)
    question_specificity: float    # é—®é¢˜å…·ä½“æ€§ (0-1)
    answer_accuracy: float         # ç­”æ¡ˆå‡†ç¡®æ€§ (0-1)
    answer_completeness: float     # ç­”æ¡ˆå®Œæ•´æ€§ (0-1)
    browsecomp_adherence: float    # BrowseCompæ–¹æ³•è®ºç¬¦åˆåº¦ (0-1)
    overall_score: float           # ç»¼åˆè¯„åˆ† (0-1)
    grade: str                     # ç­‰çº§è¯„å®š (A/B/C/D/F)
    detailed_feedback: str         # è¯¦ç»†åé¦ˆ

class GPT4oQAQualityEvaluator:
    """GPT-4oé—®ç­”è´¨é‡è¯„åˆ¤å™¨"""
    
    def __init__(self, llm_manager):
        """åˆå§‹åŒ–è¯„åˆ¤å™¨"""
        self.llm_manager = llm_manager
        self.evaluation_template = self._create_evaluation_template()
        
    def _create_evaluation_template(self) -> str:
        """åˆ›å»ºè¯„åˆ¤æç¤ºè¯æ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é—®ç­”è´¨é‡è¯„åˆ¤ä¸“å®¶ã€‚è¯·å®¢è§‚è¯„ä»·ä»¥ä¸‹åŸºäºæŠ¥å‘Šå†…å®¹ç”Ÿæˆçš„é—®ç­”å¯¹è´¨é‡ã€‚

**è¯„åˆ¤æ ‡å‡†**ï¼š

1. **é—®é¢˜æ¸…æ™°åº¦** (0-10åˆ†)ï¼šé—®é¢˜è¡¨è¿°æ˜¯å¦æ¸…æ™°æ˜ç¡®ï¼Œæ— æ­§ä¹‰
2. **é—®é¢˜å…·ä½“æ€§** (0-10åˆ†)ï¼šæ˜¯å¦ç¬¦åˆBrowseComp"é«˜çº¦æŸã€å¤šé™å®š"è¦æ±‚
3. **ç­”æ¡ˆå‡†ç¡®æ€§** (0-10åˆ†)ï¼šç­”æ¡ˆæ˜¯å¦å‡†ç¡®å›ç­”äº†é—®é¢˜ï¼ŒåŸºäºæŠ¥å‘Šå†…å®¹
4. **ç­”æ¡ˆå®Œæ•´æ€§** (0-10åˆ†)ï¼šç­”æ¡ˆæ˜¯å¦å®Œæ•´ä½†ç®€æ´ï¼Œç¬¦åˆçŸ­ç­”æ¡ˆè¦æ±‚
5. **BrowseCompç¬¦åˆåº¦** (0-10åˆ†)ï¼šæ˜¯å¦ç¬¦åˆ"éš¾ä»¥æ‰¾åˆ°ä½†å®¹æ˜“éªŒè¯"çš„åŸåˆ™

**å‚è€ƒæŠ¥å‘Š**ï¼š
{report}

**å¾…è¯„åˆ¤çš„é—®ç­”å¯¹**ï¼š
{qa_pairs}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„åˆ¤ç»“æœï¼š
{{
    "evaluations": [
        {{
            "question_index": 1,
            "question_clarity": 8.5,
            "question_specificity": 9.0,
            "answer_accuracy": 7.5,
            "answer_completeness": 8.0,
            "browsecomp_adherence": 8.5,
            "overall_score": 8.3,
            "grade": "B",
            "strengths": ["é—®é¢˜è¡¨è¿°æ¸…æ™°", "çº¦æŸæ˜ç¡®"],
            "weaknesses": ["ç­”æ¡ˆå¯ä»¥æ›´ç²¾ç¡®"],
            "suggestions": ["å»ºè®®ç¼©çŸ­ç­”æ¡ˆé•¿åº¦"]
        }}
    ],
    "overall_assessment": {{
        "avg_question_clarity": 8.2,
        "avg_question_specificity": 8.7,
        "avg_answer_accuracy": 7.8,
        "avg_answer_completeness": 8.1,
        "avg_browsecomp_adherence": 8.4,
        "overall_avg_score": 8.2,
        "overall_grade": "B",
        "general_feedback": "æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–ç­”æ¡ˆç²¾ç¡®åº¦"
    }}
}}

æ³¨æ„ï¼š
- åˆ†æ•°èŒƒå›´0-10ï¼Œå¯¹åº”ç­‰çº§ï¼šA(9-10), B(7-9), C(5-7), D(3-5), F(0-3)
- é‡ç‚¹è¯„åˆ¤æ˜¯å¦ç¬¦åˆ"çŸ­ç­”æ¡ˆæ·±åº¦æŸ¥è¯¢"çš„è¦æ±‚
- è€ƒè™‘BrowseCompæ–¹æ³•è®ºçš„"å€’ç½®é—®é¢˜"ç‰¹å¾
"""

    def evaluate_qa_pairs(self, report: str, qa_pairs: List[Dict[str, Any]], 
                         sample_size: int = 10) -> Dict[str, Any]:
        """è¯„åˆ¤é—®ç­”å¯¹è´¨é‡"""
        
        logger.info(f"ğŸ¤– å¼€å§‹GPT-4oè´¨é‡è¯„åˆ¤ï¼Œæ ·æœ¬æ•°é‡: {min(sample_size, len(qa_pairs))}")
        
        # é‡‡æ ·è¯„åˆ¤ï¼ˆé¿å…è¿‡é•¿è¾“å…¥ï¼‰
        sample_pairs = qa_pairs[:sample_size] if len(qa_pairs) > sample_size else qa_pairs
        
        # æ ¼å¼åŒ–é—®ç­”å¯¹
        formatted_pairs = []
        for i, qa in enumerate(sample_pairs, 1):
            formatted_pairs.append(f"""
{i}. é—®é¢˜: {qa['question']}
   ç­”æ¡ˆ: {qa['answer']}
""")
        
        qa_text = "\n".join(formatted_pairs)
        
        # æ„å»ºè¯„åˆ¤æç¤º
        prompt = self.evaluation_template.format(
            report=report[:2000],  # é™åˆ¶æŠ¥å‘Šé•¿åº¦
            qa_pairs=qa_text
        )
        
        try:
            start_time = time.time()
            api_response = self.llm_manager.generate_text(prompt)
            evaluation_time = time.time() - start_time
            
            if not api_response.success:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {api_response.error}")
            
            response = api_response.content
            
            # è§£æè¯„åˆ¤ç»“æœ
            evaluation_data = self._parse_evaluation_response(response)
            
            # æ·»åŠ å…ƒæ•°æ®
            evaluation_data['meta'] = {
                'sample_size': len(sample_pairs),
                'total_qa_pairs': len(qa_pairs),
                'evaluation_time': evaluation_time,
                'evaluator': 'GPT-4o-self'
            }
            
            logger.info(f"âœ… GPT-4oè¯„åˆ¤å®Œæˆï¼Œæ•´ä½“è¯„åˆ†: {evaluation_data.get('overall_assessment', {}).get('overall_avg_score', 0):.1f}/10")
            
            return evaluation_data
            
        except Exception as e:
            logger.error(f"âŒ GPT-4oè´¨é‡è¯„åˆ¤å¤±è´¥: {e}")
            return self._create_fallback_evaluation(sample_pairs)
    
    def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:
        """è§£æGPT-4oçš„è¯„åˆ¤å“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            else:
                json_str = response.strip()
            
            # æ¸…ç†æ ¼å¼é—®é¢˜
            json_str = json_str.replace('\n', ' ').replace('\r', '')
            json_str = json_str.replace(',}', '}').replace(',]', ']')
            
            evaluation_data = json.loads(json_str)
            
            # éªŒè¯æ•°æ®ç»“æ„
            if 'evaluations' not in evaluation_data or 'overall_assessment' not in evaluation_data:
                raise ValueError("è¯„åˆ¤å“åº”æ ¼å¼ä¸æ­£ç¡®")
            
            return evaluation_data
            
        except Exception as e:
            logger.warning(f"è§£æGPT-4oè¯„åˆ¤å“åº”å¤±è´¥: {e}")
            return self._extract_evaluation_fallback(response)
    
    def _extract_evaluation_fallback(self, response: str) -> Dict[str, Any]:
        """å¤‡ç”¨è¯„åˆ¤ç»“æœæå–"""
        # ç®€å•çš„æ–‡æœ¬åˆ†æå¤‡ç”¨æ–¹æ¡ˆ
        lines = response.lower().split('\n')
        
        # å°è¯•æå–åˆ†æ•°
        scores = []
        for line in lines:
            if any(keyword in line for keyword in ['åˆ†æ•°', 'score', 'è¯„åˆ†', 'åˆ†']):
                # æå–æ•°å­—
                import re
                numbers = re.findall(r'(\d+\.?\d*)', line)
                for num in numbers:
                    try:
                        score = float(num)
                        if 0 <= score <= 10:
                            scores.append(score)
                    except:
                        continue
        
        avg_score = sum(scores) / len(scores) if scores else 6.0
        grade = self._score_to_grade(avg_score)
        
        return {
            'evaluations': [],
            'overall_assessment': {
                'avg_question_clarity': avg_score,
                'avg_question_specificity': avg_score,
                'avg_answer_accuracy': avg_score,
                'avg_answer_completeness': avg_score,
                'avg_browsecomp_adherence': avg_score,
                'overall_avg_score': avg_score,
                'overall_grade': grade,
                'general_feedback': f"åŸºäºæ–‡æœ¬åˆ†æçš„ä¼°ç®—è¯„åˆ†: {avg_score:.1f}/10"
            }
        }
    
    def _create_fallback_evaluation(self, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨è¯„åˆ¤ç»“æœ"""
        return {
            'evaluations': [],
            'overall_assessment': {
                'avg_question_clarity': 6.0,
                'avg_question_specificity': 6.0,
                'avg_answer_accuracy': 6.0,
                'avg_answer_completeness': 6.0,
                'avg_browsecomp_adherence': 6.0,
                'overall_avg_score': 6.0,
                'overall_grade': 'C',
                'general_feedback': 'è¯„åˆ¤ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ä¸­ç­‰è¯„åˆ†'
            },
            'meta': {
                'sample_size': len(qa_pairs),
                'total_qa_pairs': len(qa_pairs),
                'evaluation_time': 0,
                'evaluator': 'fallback'
            }
        }
    
    def _score_to_grade(self, score: float) -> str:
        """åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 9:
            return 'A'
        elif score >= 7:
            return 'B'
        elif score >= 5:
            return 'C'
        elif score >= 3:
            return 'D'
        else:
            return 'F'
    
    def generate_quality_report(self, evaluation_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆè´¨é‡è¯„åˆ¤æŠ¥å‘Š"""
        
        overall = evaluation_data.get('overall_assessment', {})
        meta = evaluation_data.get('meta', {})
        
        report = f"""
# GPT-4oé—®ç­”è´¨é‡è¯„åˆ¤æŠ¥å‘Š

## ğŸ“Š æ€»ä½“è¯„åˆ†

**æ•´ä½“è¯„çº§**: {overall.get('overall_grade', 'N/A')} ({overall.get('overall_avg_score', 0):.1f}/10)

### åˆ†é¡¹è¯„åˆ†
- ğŸ¯ é—®é¢˜æ¸…æ™°åº¦: {overall.get('avg_question_clarity', 0):.1f}/10
- ğŸ” é—®é¢˜å…·ä½“æ€§: {overall.get('avg_question_specificity', 0):.1f}/10  
- âœ… ç­”æ¡ˆå‡†ç¡®æ€§: {overall.get('avg_answer_accuracy', 0):.1f}/10
- ğŸ“ ç­”æ¡ˆå®Œæ•´æ€§: {overall.get('avg_answer_completeness', 0):.1f}/10
- ğŸª BrowseCompç¬¦åˆåº¦: {overall.get('avg_browsecomp_adherence', 0):.1f}/10

## ğŸ’­ æ€»ä½“åé¦ˆ

{overall.get('general_feedback', 'æš‚æ— åé¦ˆ')}

## ğŸ“ˆ è¯„åˆ¤å…ƒæ•°æ®

- è¯„åˆ¤æ ·æœ¬: {meta.get('sample_size', 0)}/{meta.get('total_qa_pairs', 0)} ä¸ªé—®ç­”å¯¹
- è¯„åˆ¤æ—¶é—´: {meta.get('evaluation_time', 0):.1f} ç§’
- è¯„åˆ¤å™¨: {meta.get('evaluator', 'Unknown')}

## ğŸ¤” è‡ªæˆ‘è¯„åˆ¤çš„å±€é™æ€§

1. **ä¸»è§‚æ€§**: GPT-4oè¯„åˆ¤è‡ªå·±ç”Ÿæˆçš„å†…å®¹å¯èƒ½å­˜åœ¨åè§
2. **ä¸€è‡´æ€§**: åŒä¸€ä¸ªæ¨¡å‹çš„è‡ªæˆ‘è¯„åˆ¤å¯èƒ½è¿‡äºå®½æ¾æˆ–ä¸¥æ ¼
3. **å‚è€ƒæ„ä¹‰**: åº”ç»“åˆç®—æ³•è¯„ä¼°å’Œäººå·¥æŠ½æŸ¥ä½¿ç”¨

## ğŸ’¡ å»ºè®®

- å°†æ­¤è¯„åˆ¤ä½œä¸ºè´¨é‡å‚è€ƒï¼Œä¸ä½œä¸ºå”¯ä¸€æ ‡å‡†
- ç»“åˆç®—æ³•æŒ‡æ ‡(BrowseCompæ¯”ä¾‹ã€çº¦æŸæ£€æµ‹ç­‰)ç»¼åˆåˆ¤æ–­
- å®šæœŸè¿›è¡Œäººå·¥æŠ½æŸ¥éªŒè¯è¯„åˆ¤å‡†ç¡®æ€§
"""
        
        return report


def test_gpt4o_evaluator():
    """æµ‹è¯•GPT-4oè¯„åˆ¤å™¨"""
    # æ¨¡æ‹ŸLLMç®¡ç†å™¨
    class MockLLMManager:
        def generate_text(self, prompt):
            class MockResponse:
                success = True
                content = '''```json
{
    "evaluations": [
        {
            "question_index": 1,
            "question_clarity": 8.5,
            "question_specificity": 9.0,
            "answer_accuracy": 7.5,
            "answer_completeness": 8.0,
            "browsecomp_adherence": 8.5,
            "overall_score": 8.3,
            "grade": "B",
            "strengths": ["é—®é¢˜è¡¨è¿°æ¸…æ™°", "çº¦æŸæ˜ç¡®"],
            "weaknesses": ["ç­”æ¡ˆå¯ä»¥æ›´ç²¾ç¡®"],
            "suggestions": ["å»ºè®®ç¼©çŸ­ç­”æ¡ˆé•¿åº¦"]
        }
    ],
    "overall_assessment": {
        "avg_question_clarity": 8.5,
        "avg_question_specificity": 9.0,
        "avg_answer_accuracy": 7.5,
        "avg_answer_completeness": 8.0,
        "avg_browsecomp_adherence": 8.5,
        "overall_avg_score": 8.3,
        "overall_grade": "B",
        "general_feedback": "æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œé—®é¢˜å…·ä½“æ€§å¼ºï¼Œå»ºè®®ä¼˜åŒ–ç­”æ¡ˆç²¾ç¡®åº¦"
    }
}
```'''
            return MockResponse()
    
    # æµ‹è¯•æ•°æ®
    test_report = "è¿™æ˜¯ä¸€ä»½å…³äºäººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•çš„ç ”ç©¶æŠ¥å‘Šã€‚æŠ¥å‘Šæ˜¾ç¤ºï¼Œ2023å¹´AIæŠ€æœ¯åœ¨å¤šä¸ªé¢†åŸŸå–å¾—çªç ´..."
    test_qa_pairs = [
        {
            "question": "According to the report, what percentage of AI breakthroughs occurred in 2023?",
            "answer": "75%"
        },
        {
            "question": "Which specific technology was mentioned as the most promising?",
            "answer": "Large Language Models"
        }
    ]
    
    # è¿è¡Œæµ‹è¯•
    evaluator = GPT4oQAQualityEvaluator(MockLLMManager())
    result = evaluator.evaluate_qa_pairs(test_report, test_qa_pairs)
    
    print("=== GPT-4oè´¨é‡è¯„åˆ¤æµ‹è¯•ç»“æœ ===")
    print(f"æ•´ä½“è¯„åˆ†: {result['overall_assessment']['overall_avg_score']}")
    print(f"æ•´ä½“ç­‰çº§: {result['overall_assessment']['overall_grade']}")
    print(f"åé¦ˆ: {result['overall_assessment']['general_feedback']}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_quality_report(result)
    print("\n=== è´¨é‡æŠ¥å‘Š ===")
    print(report)


if __name__ == "__main__":
    test_gpt4o_evaluator() 