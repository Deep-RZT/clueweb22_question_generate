#!/usr/bin/env python3
"""
Short Answer Deep Query Final Optimized Experiment
==================================================

æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ•´åˆæ‰€æœ‰æœ€ä½³å®è·µå’ŒåŠŸèƒ½ï¼š
1. ç®—æ³•åŒ–è´¨é‡è¯„ä¼°ç³»ç»Ÿ
2. æ™ºèƒ½ç­”æ¡ˆå‹ç¼©ä¼˜åŒ–
3. è‡ªé€‚åº”å‚æ•°è°ƒæ•´
4. BrowseCompæ–¹æ³•è®ºå®ç°
5. å®Œæ•´çš„è´¨é‡æ§åˆ¶æµç¨‹

ä½œè€…: Assistant
æ—¥æœŸ: 2025-01-07
ç‰ˆæœ¬: v2.0 Final Optimized
"""

import os
import sys
import json
import logging
import time
import re
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import random # Added for randomization in generate_short_answer_deep_questions

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from core.llm_clients.llm_manager import DynamicLLMManager
from report_quality_evaluation_system import (
    ReportQualityEvaluator,
    TopicRelevanceAnalyzer
)
from answer_compression_optimizer import AnswerCompressionOptimizer
from comprehensive_adaptive_framework import (
    ComprehensiveAdaptiveFramework,
    AdaptiveOptimizationConfig
)
from document_content_filter import DocumentContentFilter
from gpt4o_qa_quality_evaluator import GPT4oQAQualityEvaluator
from excel_export_system import ShortAnswerDeepQueryExcelExporter

class FinalOptimizedExperiment:
    """æœ€ç»ˆä¼˜åŒ–çš„Short Answer Deep Queryå®éªŒç³»ç»Ÿ"""
    
    def __init__(self, results_dir: str = "./results"):
        """åˆå§‹åŒ–å®éªŒç³»ç»Ÿ"""
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ—¶é—´æˆ³
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_name = f"final_optimized_{self.timestamp}"
        
        # åˆ›å»ºå®éªŒä¸“å±ç›®å½•
        self.experiment_dir = self.results_dir / self.experiment_name
        self.experiment_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.llm_manager = DynamicLLMManager()
        self.quality_evaluator = ReportQualityEvaluator()
        self.relevance_analyzer = TopicRelevanceAnalyzer()
        self.compression_optimizer = AnswerCompressionOptimizer(self.llm_manager)
        self.content_filter = DocumentContentFilter()
        self.gpt4o_qa_evaluator = GPT4oQAQualityEvaluator(self.llm_manager)
        self.excel_exporter = ShortAnswerDeepQueryExcelExporter(str(self.results_dir))
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # ä¼˜åŒ–é…ç½® - è°ƒæ•´ä¸ºæ›´ç°å®çš„é˜ˆå€¼
        self.config = {
            # æŠ¥å‘Šç”Ÿæˆé…ç½®
            "min_report_words": 600,
            "max_report_words": 1500,
            "target_info_density": 0.4,
            "avoid_structure_markers": True,
            
            # é—®é¢˜ç”Ÿæˆé…ç½® - å¤§å¹…æ”¾å®½é˜ˆå€¼æé«˜æˆåŠŸç‡ 
            "questions_per_topic": 50,               # ä¿®å¤ï¼šä»30æ”¹ä¸º50
            "min_browsecomp_ratio": 0.20,        # è¿›ä¸€æ­¥é™ä½åˆ°0.20 (20%)
            "min_high_constraint_ratio": 0.05,   # è¿›ä¸€æ­¥é™ä½åˆ°0.05 (5%)
            "min_avg_constraints": 0.5,          # è¿›ä¸€æ­¥é™ä½åˆ°0.5
            "required_question_types": 2,
            
            # GPT-4oè´¨é‡è¯„åˆ¤é…ç½®
            "enable_gpt4o_evaluation": True,     # å¯ç”¨GPT-4oè¯„åˆ¤
            "gpt4o_sample_size": 10,             # è¯„åˆ¤æ ·æœ¬æ•°é‡
            "min_gpt4o_score": 6.0,              # æœ€å°GPT-4oè¯„åˆ† (ä»…å‚è€ƒ)
            
            # ç­”æ¡ˆç”Ÿæˆé…ç½® - æ›´å®½æ¾çš„é•¿åº¦é™åˆ¶
            "max_answer_words": 20,   # ä»15æé«˜åˆ°20
            "max_answer_chars": 150,  # ä»100æé«˜åˆ°150
            "enable_answer_compression": True,
            "compression_threshold": 15,
            
            # è´¨é‡æ§åˆ¶é…ç½® - è¿›ä¸€æ­¥æ”¾å®½æ ‡å‡†
            "min_report_quality_score": 0.45,    # è¿›ä¸€æ­¥é™ä½åˆ°0.45
            "min_relevance_score": 0.15,         # å¤§å¹…é™ä½åˆ°0.15ä»¥é€‚åº”å®é™…è¡¨ç°
            "min_answer_quality_score": 0.60,    # ä»0.75é™ä½åˆ°0.60
            "max_validation_failures": 0.25
        }
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_file = self.experiment_dir / "experiment.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_documents(self, data_source: str = "clueweb") -> List[Dict[str, Any]]:
        """åŠ è½½topic-basedæ•°æ®è€Œä¸æ˜¯å•ä¸ªæ–‡æ¡£"""
        self.logger.info(f"åŠ è½½æ•°æ®æº: {data_source} (Topic-based approach)")
        
        # å¯¼å…¥topic-basedæ•°æ®åŠ è½½å™¨
        from topic_based_data_loader import TopicBasedDataLoader
        
        try:
            # åˆå§‹åŒ–topicæ•°æ®åŠ è½½å™¨
            data_loader = TopicBasedDataLoader()
            
            # è·å–å¯ç”¨topics
            available_topics = data_loader.get_available_topics()
            
            if not available_topics:
                self.logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„topics")
                return []
            
            self.logger.info(f"å‘ç° {len(available_topics)} ä¸ªå¯ç”¨topics")
            
            # è¿”å›topicåˆ—è¡¨ä½œä¸ºå¤„ç†å•å…ƒ
            topic_data_list = []
            for topic_id in available_topics:
                topic_info = {
                    'id': topic_id,
                    'type': 'topic',
                    'topic_id': topic_id,
                    'data_loader': data_loader  # ä¿å­˜åŠ è½½å™¨å¼•ç”¨
                }
                topic_data_list.append(topic_info)
            
            self.logger.info(f"æˆåŠŸå‡†å¤‡ {len(topic_data_list)} ä¸ªtopicsç”¨äºå¤„ç†")
            return topic_data_list
            
        except Exception as e:
            self.logger.error(f"åŠ è½½topicæ•°æ®æ—¶å‡ºé”™: {e}")
            return []
    
    def generate_simplified_report(self, topic_info: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
        """ç”Ÿæˆtopicçº§åˆ«çš„å¤šæ–‡æ¡£èåˆæŠ¥å‘Šï¼ˆä¿®æ”¹ä¸ºæ”¯æŒtopic-basedå¤„ç†ï¼‰"""
        
        # æ£€æŸ¥è¾“å…¥ç±»å‹
        if topic_info.get('type') != 'topic':
            # å¦‚æœæ˜¯æ—§å¼å•æ–‡æ¡£ï¼Œå›é€€åˆ°åŸæ–¹æ³•
            return self._generate_single_document_report(topic_info)
        
        topic_id = topic_info['topic_id']
        data_loader = topic_info['data_loader']
        
        self.logger.info(f"ç”Ÿæˆtopic {topic_id} çš„å¤šæ–‡æ¡£èåˆæŠ¥å‘Š")
        
        try:
            # 1. ä½¿ç”¨æœ€æ–°çš„èšåˆå†…å®¹èåˆæ–¹æ³•ï¼ˆå…ˆé›†åˆæ‰€æœ‰å†…å®¹ï¼Œå†æ•´ä½“ç­›é€‰ï¼‰
            max_chars = 120000  # è¿›ä¸€æ­¥å¢åŠ å­—ç¬¦é™åˆ¶ï¼Œä¿ç•™æ›´å¤šå†…å®¹
            aggregation_data = data_loader.get_topic_aggregated_content_for_fusion(
                topic_id,
                max_total_chars=max_chars
            )
            
            if not aggregation_data['success']:
                raise Exception(f"Topicå†…å®¹èšåˆå¤±è´¥: {aggregation_data.get('error', 'Unknown error')}")
            
            aggregated_content = aggregation_data['aggregated_content']
            
            if not aggregated_content:
                raise Exception(f"Topic {topic_id} æ²¡æœ‰æœ‰æ•ˆçš„èšåˆå†…å®¹")
            
            processing_stats = aggregation_data['processing_stats']
            content_stats = aggregation_data['content_stats']
            
            self.logger.info(f"Topic {topic_id}: èšåˆå¤„ç†å®Œæˆ")
            self.logger.info(f"  åŸå§‹æ–‡æ¡£: {processing_stats['original_documents']} ä¸ª")
            self.logger.info(f"  åŸå§‹å­—ç¬¦: {processing_stats['total_raw_chars']:,}")
            self.logger.info(f"  æœ‰ä»·å€¼å¥å­: {processing_stats['valuable_sentences_extracted']}")
            self.logger.info(f"  å»é‡åå¥å­: {processing_stats['unique_sentences_after_dedup']}")
            self.logger.info(f"  æœ€ç»ˆå¥å­: {processing_stats['final_sentences_selected']}")
            self.logger.info(f"  æœ€ç»ˆå­—ç¬¦: {content_stats['total_chars']:,}")
            self.logger.info(f"  å†…å®¹å‹ç¼©ç‡: {processing_stats['content_compression_ratio']:.3f}")
            
            # 2. æ„å»ºåŸºäºèšåˆå†…å®¹çš„èåˆprompt
            key_info_summary = []
            overall_key_info = aggregation_data.get('overall_key_information', {})
            for category, items in overall_key_info.items():
                if items:
                    key_info_summary.append(f"{category}: {', '.join(items[:10])}")
            
            prompt = f"""You are creating a comprehensive fusion report from pre-processed aggregated content of topic {topic_id}. This report will be used to generate BrowseComp-style deep query questions with short answers.

**CRITICAL: ALL OUTPUT MUST BE IN PURE ENGLISH - NO CHINESE CHARACTERS**

**Fusion Mission**: Transform {processing_stats['final_sentences_selected']} high-value sentences (selected from {processing_stats['original_documents']} documents) into a {self.config['min_report_words']}-{self.config['max_report_words']} word narrative that preserves ALL critical details for deep reasoning.

**Content Processing Context**:
- Original documents: {processing_stats['original_documents']}
- Valuable sentences extracted: {processing_stats['valuable_sentences_extracted']}
- After deduplication: {processing_stats['unique_sentences_after_dedup']}
- Final high-value sentences: {processing_stats['final_sentences_selected']}
- Average sentence value score: {processing_stats['avg_sentence_value_score']:.3f}
- Key information categories: {'; '.join(key_info_summary)}

**Pre-Processed Aggregated Content**:
{aggregated_content}

**FUSION REQUIREMENTS FOR BROWSECOMP DEEP QUERY SUPPORT**:

1. **Preserve ALL Factual Details**: Every number, name, date, percentage, measurement, institution, methodology, and technical term must be retained
2. **Maintain Causal Relationships**: Preserve cause-effect relationships, temporal sequences, and logical connections between concepts
3. **Keep Specific Attributions**: Maintain "according to X", "developed by Y", "found in study Z" attributions for precise question generation
4. **Retain Contextual Qualifiers**: Keep words like "first", "earliest", "specific", "particular", "exact", "precise" that enable constraint-based questions
5. **Preserve Comparative Information**: Maintain comparisons, contrasts, and relative relationships between entities
6. **Include Methodological Details**: Retain information about how studies were conducted, techniques used, and procedures followed
7. **Maintain Quantitative Precision**: Keep exact numbers, ranges, statistical measures, and performance metrics
8. **Preserve Temporal Context**: Maintain time references, sequences, and chronological relationships

**NARRATIVE STRUCTURE**:
- Use natural paragraph flow without formal academic headers
- Create logical connections between related concepts from different documents
- Ensure each paragraph contains multiple specific, verifiable facts
- Maintain information density while ensuring readability
- Focus on concrete, factual content rather than general statements

**OUTPUT REQUIREMENTS**:
- {self.config['min_report_words']}-{self.config['max_report_words']} words of continuous narrative
- High information density with specific facts suitable for precise question generation
- Natural paragraph transitions that maintain logical flow
- **WRITE ENTIRELY IN ENGLISH**

Generate a comprehensive fusion report that preserves all critical details while creating a coherent narrative optimized for BrowseComp deep query generation."""

            # 6. è°ƒç”¨LLMç”ŸæˆèåˆæŠ¥å‘Š
            api_response = self.llm_manager.generate_text(prompt)
            if not api_response.success:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {api_response.error}")
            
            report_content = api_response.content
            
            # 7. è®¡ç®—åˆ†æç»“æœ
            word_count = len(report_content.split())
            char_count = len(report_content)
            
            analysis_results = {
                'word_count': word_count,
                'char_count': char_count,
                'approach': 'aggregated_content_fusion',
                'documents_processed': processing_stats['original_documents'],
                'documents_in_synthesis': processing_stats['original_documents'],  # æ‰€æœ‰æ–‡æ¡£éƒ½å‚ä¸äº†èšåˆ
                'synthesis_ratio': 1.0,  # 100%çš„æ–‡æ¡£å‚ä¸äº†èšåˆè¿‡ç¨‹
                'aggregation_data': aggregation_data,  # ä¿å­˜å®Œæ•´çš„èšåˆæ•°æ®
                'key_information_integrated': len([k for k, v in overall_key_info.items() if v]),
                'content_compression_ratio': char_count / processing_stats['total_raw_chars'] if processing_stats['total_raw_chars'] > 0 else 0,
                'sentence_processing': {
                    'valuable_sentences_extracted': processing_stats['valuable_sentences_extracted'],
                    'unique_sentences_after_dedup': processing_stats['unique_sentences_after_dedup'],
                    'final_sentences_selected': processing_stats['final_sentences_selected'],
                    'deduplication_ratio': processing_stats['deduplication_ratio'],
                    'avg_sentence_value_score': processing_stats['avg_sentence_value_score']
                }
            }
            
            self.logger.info(f"Topic {topic_id} èšåˆèåˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
            self.logger.info(f"  æŠ¥å‘Šé•¿åº¦: {word_count} è¯, {char_count} å­—ç¬¦")
            self.logger.info(f"  æ–‡æ¡£èšåˆ: {processing_stats['original_documents']}/{processing_stats['original_documents']} (100%)")
            self.logger.info(f"  å¥å­å¤„ç†: {processing_stats['valuable_sentences_extracted']} -> {processing_stats['unique_sentences_after_dedup']} -> {processing_stats['final_sentences_selected']}")
            self.logger.info(f"  å†…å®¹å‹ç¼©: {processing_stats['total_raw_chars']:,} -> {char_count:,} ({analysis_results['content_compression_ratio']:.3f})")
            
            return report_content, analysis_results
            
        except Exception as e:
            self.logger.error(f"Topic {topic_id} èåˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _generate_single_document_report(self, document: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
        """åŸå§‹çš„å•æ–‡æ¡£æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰"""
        
        prompt = f"""Based on the following document content, generate an information-dense simplified report in ENGLISH ONLY. Strictly follow these requirements:

**CRITICAL: ALL OUTPUT MUST BE IN PURE ENGLISH - NO CHINESE CHARACTERS**

**Forbidden Formats**:
- Do not use formal section headers like "Abstract", "Introduction", "Conclusion"
- Do not use bullet points or numbered lists
- Do not use structured connectors like "First", "Second", "Finally"
- Do not use section titles or subtitles

**Mandatory Requirements**:
- Generate {self.config['min_report_words']}-{self.config['max_report_words']} words of continuous narrative
- High information density, including specific data, names, times, locations and other facts
- Use natural paragraph transitions, maintain content coherence
- Focus on core facts and key details
- Avoid vague overview language
- **WRITE ENTIRELY IN ENGLISH**

Document Content:
{document.get('content', '')[:3000]}

Please generate a simplified report that meets the requirements. Remember: OUTPUT MUST BE ENTIRELY IN ENGLISH."""

        try:
            api_response = self.llm_manager.generate_text(prompt)
            if not api_response.success:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {api_response.error}")
            response = api_response.content
            
            analysis_results = {
                'word_count': len(response.split()),
                'char_count': len(response),
                'approach': 'single_document',
                'documents_processed': 1
            }
            
            return response, analysis_results
            
        except Exception as e:
            self.logger.error(f"å•æ–‡æ¡£æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def detect_question_constraints(self, question: str) -> Tuple[int, List[str]]:
        """ä¼˜åŒ–çš„çº¦æŸæ£€æµ‹ç®—æ³• - åŸºäºç±»åˆ«çš„æ£€æµ‹æ–¹æ³•"""
        
        constraint_categories = {
            'precision': [
                r'\b(exactly?|precisely?|specifically?|particular)\b',
                r'\b(which specific|what exact|how many exactly)\b'
            ],
            'temporal': [
                r'\b(when|during|before|after|since|until|by)\b',
                r'\b(year|date|time|period|decade|century)\b',
                r'\b(\d{4}|\d{1,2}/\d{1,2})\b'
            ],
            'logical': [
                r'\b(because|since|due to|caused by|resulting from)\b',
                r'\b(leads to|results in|causes|enables)\b',
                r'\b(if|unless|provided that|given that)\b'
            ],
            'attribution': [
                r'\b(according to|stated by|claimed by|reported by)\b',
                r'\b(authored by|created by|developed by)\b'
            ],
            'institutional': [
                r'\b(university|institute|organization|company|corporation)\b',
                r'\b(department|faculty|school|college)\b'
            ],
            'methodological': [
                r'\b(method|approach|technique|procedure|process)\b',
                r'\b(using|through|via|by means of)\b'
            ],
            'achievement': [
                r'\b(first|earliest|initial|original|pioneering)\b',
                r'\b(breakthrough|discovery|innovation|advancement)\b'
            ],
            'collaboration': [
                r'\b(collaboration|partnership|joint|together)\b',
                r'\b(team|group|collective|consortium)\b'
            ],
            'validation': [
                r'\b(evidence|proof|verification|confirmation)\b',
                r'\b(demonstrated|shown|proven|established)\b'
            ],
            'comparison': [
                r'\b(compared to|versus|rather than|instead of)\b',
                r'\b(difference|similarity|contrast|distinction)\b'
            ],
            'location': [
                r'\b(where|location|place|region|country|city)\b',
                r'\b(at|in|from|located|situated)\b'
            ],
            'quantification': [
                r'\b(how much|how many|amount|quantity|number)\b',
                r'\b(percent|percentage|ratio|proportion)\b',
                r'\b(\d+(?:\.\d+)?(?:%|percent))\b'
            ]
        }
        
        detected_constraints = []
        question_lower = question.lower()
        
        for category, patterns in constraint_categories.items():
            for pattern in patterns:
                if re.search(pattern, question_lower):
                    detected_constraints.append(category)
                    break  # æ¯ä¸ªç±»åˆ«åªè®°å½•ä¸€æ¬¡
        
        return len(detected_constraints), detected_constraints
    
    def _is_short_answer_deep_query(self, question: str, answer: str) -> Tuple[bool, Dict[str, Any]]:
        """ä¼˜åŒ–çš„BrowseCompé—®é¢˜æ£€æµ‹ - æ”¾å®½æ£€æµ‹æ ‡å‡†"""
        
        # 1. çº¦æŸæ£€æµ‹
        constraint_count, constraint_types = self.detect_question_constraints(question)
        
        # 2. ç­”æ¡ˆé•¿åº¦æ£€æŸ¥
        answer_words = len(answer.split())
        answer_chars = len(answer)
        
        # 3. æ‰©å±•çš„BrowseCompæ¨¡å¼æ£€æµ‹
        browsecomp_patterns = [
            # åŸæœ‰æ¨¡å¼
            r'\b(who|what|when|where|which|how)\b.*\b(first|earliest|specific|exact|particular)\b',
            r'\b(which specific|what exact|who exactly|when precisely)\b',
            r'\b(according to|in|during|by|through)\b.*\b(what|who|which|how)\b',
            r'\b(what.*called|who.*known|which.*referred|how.*termed)\b',
            r'\b(what.*founded|who.*established|when.*created|where.*located)\b',
            
            # æ–°å¢æ›´å®½æ¾çš„æ¨¡å¼
            r'\b(what|which|who|when|where|how)\b.*\b(mentioned|described|stated|indicated|reported)\b',
            r'\b(what|which|who)\b.*\b(was|were|is|are)\b.*\b(used|employed|applied|utilized)\b',
            r'\b(how many|how much)\b.*\b(were|was|are|is)\b',
            r'\b(what type|what kind|which type)\b.*\b(of|was|were)\b',
            r'\b(in what|at what|during what|by what)\b',
            r'\bwhat.*\b(percentage|number|amount|quantity|rate)\b',
            r'\bwhich.*\b(method|approach|technique|strategy)\b',
        ]
        
        is_browsecomp = any(re.search(pattern, question.lower()) for pattern in browsecomp_patterns)
        
        # 4. æ‰©å±•çš„æ·±åº¦æŸ¥è¯¢ç‰¹å¾
        deep_query_indicators = [
            'specific', 'particular', 'exact', 'precise', 'detailed',
            'according to', 'based on', 'mentioned in', 'described as',
            'first', 'earliest', 'original', 'initial', 'pioneering',
            # æ–°å¢æŒ‡æ ‡
            'was used', 'were used', 'is used', 'are used',
            'reported', 'stated', 'indicated', 'found', 'showed',
            'percentage', 'number', 'amount', 'quantity', 'rate',
            'method', 'approach', 'technique', 'strategy', 'type'
        ]
        
        deep_score = sum(1 for indicator in deep_query_indicators 
                        if indicator in question.lower()) / len(deep_query_indicators)
        
        # 5. æåº¦å®½æ¾çš„ç»¼åˆè¯„ä¼°ï¼ˆå¤§å¹…æ”¾å®½æ ‡å‡†ï¼‰
        is_high_constraint = constraint_count >= 1  # ä¿æŒ1ä¸ªçº¦æŸå³å¯
        is_short_answer = answer_words <= self.config['max_answer_words'] and answer_chars <= self.config['max_answer_chars']
        
        # å¤§å¹…æ”¾å®½BrowseCompè®¤å®šæ ‡å‡†ï¼šæ»¡è¶³ä»»æ„ä¸€ä¸ªæ¡ä»¶å³å¯
        conditions = [
            is_browsecomp,                    # æ¨¡å¼åŒ¹é…
            is_high_constraint,               # è‡³å°‘1ä¸ªçº¦æŸ
            deep_score >= 0.02,               # è¿›ä¸€æ­¥é™ä½æ·±åº¦åˆ†æ•°è¦æ±‚
            answer_words <= 10,               # ç­”æ¡ˆè¾ƒçŸ­
            any(word in question.lower() for word in ['what', 'which', 'who', 'when', 'where', 'how']),  # åŒ…å«ç–‘é—®è¯
        ]
        
        conditions_met = sum(conditions)
        overall_is_browsecomp = conditions_met >= 1  # åªéœ€æ»¡è¶³ä»»æ„ä¸€ä¸ªæ¡ä»¶ï¼
        
        return overall_is_browsecomp, {
            'constraint_count': constraint_count,
            'constraint_types': constraint_types,
            'is_high_constraint': is_high_constraint,
            'answer_words': answer_words,
            'answer_chars': answer_chars,
            'is_short_answer': is_short_answer,
            'deep_score': deep_score,
            'browsecomp_pattern_match': is_browsecomp,
            'conditions_met': conditions_met
        }
    
    def generate_short_answer_deep_questions(self, report: str, num_questions: int = 30) -> List[Dict[str, Any]]:
        """é‡æ–°è®¾è®¡çš„Answer-to-Query + LLMçŸ­ç­”æ¡ˆç”Ÿæˆ - ä¿®å¤ç­”æ¡ˆè´¨é‡é—®é¢˜"""
        
        self.logger.info(f"ğŸ¯ å¼€å§‹é‡æ–°è®¾è®¡çš„Answer-to-Queryæµç¨‹ç”Ÿæˆ {num_questions} ä¸ªé—®é¢˜...")
        
        # ç¬¬ä¸€æ­¥ï¼šä»æŠ¥å‘Šä¸­æå–å¤šæ ·åŒ–çš„äº‹å®ç‚¹ï¼ˆä¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼Œè€Œæ˜¯ç”¨äºç”Ÿæˆé—®é¢˜çš„ç´ æï¼‰
        fact_points = self._extract_diverse_factual_points(report, num_questions * 2)
        self.logger.info(f"ğŸ“Š æå–äº‹å®ç‚¹: {len(fact_points)} ä¸ªä¸åŒäº‹å®ç‚¹")
        
        # ç¬¬äºŒæ­¥ï¼šLLMåŸºäºäº‹å®ç‚¹ç”Ÿæˆæ·±åº¦é—®é¢˜ï¼ˆè€Œä¸æ˜¯åæ¨é—®é¢˜ï¼‰
        generated_questions = self._llm_generate_questions_from_facts(report, fact_points, num_questions)
        self.logger.info(f"ğŸ§  LLMç”Ÿæˆé—®é¢˜: {len(generated_questions)} ä¸ªæ·±åº¦é—®é¢˜")
        
        # ç¬¬ä¸‰æ­¥ï¼šå…³é”®ä¿®å¤ - è®©LLMåŸºäºå®Œæ•´reportå’Œé—®é¢˜ç”ŸæˆçœŸæ­£çš„çŸ­ç­”æ¡ˆ
        final_qa_pairs = []
        used_question_patterns = set()
        
        for i, question_data in enumerate(generated_questions):
            if len(final_qa_pairs) >= num_questions:
                break
                
            question = question_data['question']
            fact_context = question_data.get('fact_context', '')
            
            self.logger.debug(f"ğŸ”„ ä¸ºé—®é¢˜ç”ŸæˆçœŸæ­£çš„çŸ­ç­”æ¡ˆ: '{question}'")
            
            # æ£€æŸ¥é—®é¢˜æ˜¯å¦é‡å¤
            question_pattern = self._create_question_fingerprint(question)
            if question_pattern in used_question_patterns:
                self.logger.debug(f"  âŒ é—®é¢˜æ¨¡å¼é‡å¤ï¼Œè·³è¿‡")
                continue
            
            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šLLMåŸºäºå®Œæ•´reportå›ç­”é—®é¢˜ï¼Œç”ŸæˆçœŸæ­£çš„çŸ­ç­”æ¡ˆ
            true_answer = self._llm_generate_true_short_answer(question, report, fact_context)
            
            if true_answer and len(true_answer.split()) <= self.config['max_answer_words']:
                # BrowseCompæ£€æµ‹
                is_browsecomp, analysis = self._is_short_answer_deep_query(question, true_answer)
                
                processed_question = {
                    'question': question,
                    'answer': true_answer,  # è¿™æ‰æ˜¯çœŸæ­£çš„ç­”æ¡ˆï¼
                    'is_browsecomp': is_browsecomp,
                    'analysis': analysis,
                    'fact_type': question_data.get('fact_type', 'general'),  # ç¡®ä¿æœ‰é»˜è®¤å€¼
                    'depth_level': 'medium',  # è®¾ç½®é»˜è®¤æ·±åº¦çº§åˆ«ï¼Œæˆ–ä»question_dataè·å–
                    'generation_method': 'fixed_answer_to_query_enhanced'
                }
                
                final_qa_pairs.append(processed_question)
                used_question_patterns.add(question_pattern)
                
                self.logger.debug(f"  âœ… æˆåŠŸç”Ÿæˆ: Q='{question}' A='{true_answer}'")
            else:
                self.logger.debug(f"  âŒ ç­”æ¡ˆè´¨é‡ä¸ç¬¦åˆè¦æ±‚æˆ–è¿‡é•¿")
        
        self.logger.info(f"ğŸ¯ ä¿®å¤åçš„Answer-to-Queryç»“æœ:")
        self.logger.info(f"  - ç›®æ ‡é—®é¢˜æ•°: {num_questions}")
        self.logger.info(f"  - å®é™…ç”Ÿæˆæ•°: {len(final_qa_pairs)}")
        self.logger.info(f"  - é—®é¢˜å¤šæ ·æ€§: {len(used_question_patterns)} ä¸ªä¸åŒé—®é¢˜æ¨¡å¼")
        
        # åˆ†æåˆ†å¸ƒ
        distribution = self._analyze_question_distribution(final_qa_pairs)
        for category, stats in distribution.items():
            self.logger.info(f"  - {category}åˆ†å¸ƒ: {dict(stats)}")
        
        return final_qa_pairs
    
    def _extract_diverse_factual_points(self, report: str, target_count: int) -> List[Dict[str, Any]]:
        """æå–äº‹å®ç‚¹ï¼ˆä¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼Œè€Œæ˜¯ç”Ÿæˆé—®é¢˜çš„ç´ æï¼‰"""
        
        fact_points = []
        
        # åˆ†ææŠ¥å‘Šï¼Œæå–ä¸åŒç±»å‹çš„äº‹å®ä¿¡æ¯ç‚¹
        fact_extractors = [
            ('numbers', self._extract_numerical_facts),
            ('dates', self._extract_temporal_facts), 
            ('names', self._extract_person_organization_facts),
            ('locations', self._extract_location_facts),
            ('methods', self._extract_method_process_facts),
            ('terms', self._extract_technical_term_facts),
            ('comparisons', self._extract_comparison_facts)
        ]
        
        self.logger.info("ğŸ“Š å¼€å§‹æå–ä¸åŒç±»å‹çš„äº‹å®ç‚¹...")
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæŠ¥å‘Šå†…å®¹å‰500å­—ç¬¦
        self.logger.info(f"ğŸ” æŠ¥å‘Šå†…å®¹é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰ï¼š")
        self.logger.info(f"   {repr(report[:500])}")
        
        for fact_type, extractor_func in fact_extractors:
            facts = extractor_func(report)
            for fact in facts:
                fact_points.append({
                    'fact_value': fact['value'],  # æ”¹åï¼Œæ˜ç¡®è¿™ä¸æ˜¯æœ€ç»ˆç­”æ¡ˆ
                    'context': fact['context'],
                    'type': fact_type,
                    'confidence': fact.get('confidence', 0.5)
                })
            
            self.logger.info(f"  - {fact_type}: æå– {len(facts)} ä¸ªäº‹å®ç‚¹")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶å»é‡
        fact_points.sort(key=lambda x: x['confidence'], reverse=True)
        
        # å»é‡ï¼šç§»é™¤ç›¸ä¼¼çš„äº‹å®ç‚¹
        unique_points = []
        seen_points = set()
        
        for point_data in fact_points:
            point_normalized = point_data['fact_value'].lower().strip()
            if point_normalized not in seen_points and len(point_normalized) > 1:
                unique_points.append(point_data)
                seen_points.add(point_normalized)
                
                if len(unique_points) >= target_count:
                    break
        
        # ğŸš¨ å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ‰€æœ‰äº‹å®æå–å™¨éƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨ç®€å•çš„è¯æ±‡æå–
        if len(unique_points) == 0:
            self.logger.warning("ğŸ”„ æ‰€æœ‰äº‹å®æå–å™¨å¤±è´¥ï¼Œå¯ç”¨å¤‡ç”¨ç®€å•æå–æ–¹æ³•...")
            backup_points = self._extract_backup_facts(report, target_count)
            unique_points.extend(backup_points)
            self.logger.info(f"ğŸ”„ å¤‡ç”¨æ–¹æ³•æå–äº† {len(backup_points)} ä¸ªåŸºæœ¬äº‹å®ç‚¹")
        
        return unique_points
    
    def _llm_generate_questions_from_facts(self, report: str, fact_points: List[Dict], target_count: int) -> List[Dict[str, Any]]:
        """LLMåŸºäºäº‹å®ç‚¹ç”Ÿæˆæ·±åº¦é—®é¢˜ï¼ˆä¸æ˜¯åæ¨ï¼Œè€Œæ˜¯æ­£å‘ç”Ÿæˆï¼‰"""
        
        generated_questions = []
        
        # æŒ‰ç±»å‹åˆ†ç»„äº‹å®ç‚¹
        facts_by_type = {}
        for point in fact_points:
            fact_type = point['type']
            if fact_type not in facts_by_type:
                facts_by_type[fact_type] = []
            facts_by_type[fact_type].append(point)
        
        # ä¸ºæ¯ç§ç±»å‹ç”Ÿæˆé—®é¢˜
        for fact_type, type_facts in facts_by_type.items():
            if len(generated_questions) >= target_count:
                break
                
            # é™åˆ¶æ¯ç§ç±»å‹çš„é—®é¢˜æ•°é‡
            max_questions_per_type = min(8, len(type_facts))
            
            for fact_point in type_facts[:max_questions_per_type]:
                if len(generated_questions) >= target_count:
                    break
                
                question = self._llm_generate_single_question_from_fact(
                    fact_point, report, fact_type
                )
                
                if question:
                    generated_questions.append({
                        'question': question,
                        'fact_type': fact_type,
                        'fact_context': fact_point['context'],
                        'source_fact': fact_point['fact_value']
                    })
        
        return generated_questions
    
    def _llm_generate_single_question_from_fact(self, fact_point: Dict, report: str, fact_type: str) -> Optional[str]:
        """LLMåŸºäºå•ä¸ªäº‹å®ç‚¹ç”Ÿæˆä¸€ä¸ªæ·±åº¦é—®é¢˜"""
        
        fact_value = fact_point['fact_value']
        context = fact_point['context']
        
        prompt = f"""Based on this factual information from a research report, generate ONE excellent BrowseComp-style deep query question.

**FACTUAL INFORMATION**:
- Fact: {fact_value}
- Type: {fact_type}
- Context: {context}

**CRITICAL REQUIREMENTS**:
1. Generate a question that tests DEEP UNDERSTANDING of the content
2. Question should be answerable by reading the report carefully
3. Must require ANALYTICAL THINKING, not simple fact lookup
4. Follow BrowseComp principles (specific, constraint-based, searchable)
5. Question should be 10-25 words
6. Use appropriate question words (What specific, Which particular, How exactly, etc.)

**QUESTION GENERATION STRATEGIES by Type**:
- Numbers: "What percentage/amount/quantity..." 
- Names: "Which specific person/organization..."
- Methods: "What approach/technique was used to..."
- Terms: "What does [term] specifically refer to in the context of..."
- Dates: "When exactly did [event] occur..."
- Locations: "Where specifically was [activity] conducted..."

**EXAMPLES OF GOOD DEEP QUESTIONS**:
- "What specific percentage improvement was achieved using the new method?"
- "Which particular organization developed the innovative approach mentioned?"
- "What exact technique was employed to enhance the system performance?"

**OUTPUT FORMAT**:
Question: [Your deep, specific question here]

Generate ONE excellent question now:"""
        
        try:
            api_response = self.llm_manager.generate_text(prompt)
            
            if api_response.success and api_response.content:
                response = api_response.content.strip()
                
                # æå–é—®é¢˜
                question_match = re.search(r'Question:\s*(.+?)(?:\n|$)', response, re.IGNORECASE)
                if question_match:
                    question = question_match.group(1).strip()
                    
                    # ç¡®ä¿é—®é¢˜ä»¥é—®å·ç»“å°¾
                    if not question.endswith('?'):
                        question += '?'
                    
                    return question
                
        except Exception as e:
            self.logger.debug(f"LLMç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
        
        return None
    
    def _llm_generate_true_short_answer(self, question: str, report: str, fact_context: str) -> Optional[str]:
        """ğŸ”‘ å…³é”®ä¿®å¤ï¼šLLMåŸºäºå®Œæ•´reportå’Œé—®é¢˜ç”ŸæˆçœŸæ­£çš„çŸ­ç­”æ¡ˆ"""
        
        prompt = f"""You are answering a BrowseComp-style deep query question based on the provided research report.

**QUESTION**: {question}

**RESEARCH REPORT**:
{report}

**ANSWER REQUIREMENTS**:
1. Provide a PRECISE, SHORT answer (1-10 words maximum)
2. Answer must be DIRECTLY supported by the report content
3. Extract the most specific, factual information that answers the question
4. If the report contains multiple relevant details, choose the most precise one
5. Answer should be verifiable and specific

**EXAMPLES OF GOOD SHORT ANSWERS**:
- "85% efficiency improvement"
- "Stanford University researchers"  
- "machine learning algorithm"
- "June 2023"
- "microporous filtration method"

**OUTPUT FORMAT**:
Answer: [Your precise short answer]

Generate the answer now:"""
        
        try:
            api_response = self.llm_manager.generate_text(prompt)
            
            if api_response.success and api_response.content:
                response = api_response.content.strip()
                
                # æå–ç­”æ¡ˆ
                answer_match = re.search(r'Answer:\s*(.+?)(?:\n|$)', response, re.IGNORECASE)
                if answer_match:
                    answer = answer_match.group(1).strip()
                    
                    # æ¸…ç†ç­”æ¡ˆæ ¼å¼
                    answer = answer.strip('"\'')
                    
                    return answer
                
        except Exception as e:
            self.logger.debug(f"LLMç”ŸæˆçŸ­ç­”æ¡ˆå¤±è´¥: {e}")
        
        return None
    
    def _analyze_question_distribution(self, questions: List[Dict]) -> Dict[str, Any]:
        """åˆ†æé—®é¢˜åˆ†å¸ƒç»Ÿè®¡"""
        from collections import Counter
        
        stats = {}
        
        # æŒ‰äº‹å®ç±»å‹ç»Ÿè®¡ - ä¿®å¤å­—æ®µå
        fact_types = [q.get('fact_type', 'unknown') for q in questions]
        stats['äº‹å®ç±»å‹'] = Counter(fact_types)
        
        # æŒ‰æ·±åº¦çº§åˆ«ç»Ÿè®¡
        depth_levels = [q.get('depth_level', 'unknown') for q in questions]
        stats['æ·±åº¦çº§åˆ«'] = Counter(depth_levels)
        
        # æŒ‰BrowseCompæ¯”ä¾‹ç»Ÿè®¡
        browsecomp_count = sum(1 for q in questions if q.get('is_browsecomp', False))
        stats['BrowseComp'] = {'æ˜¯': browsecomp_count, 'å¦': len(questions) - browsecomp_count}
        
        return stats
    
    def _extract_diverse_factual_answers(self, report: str, target_count: int) -> List[Dict[str, Any]]:
        """ä»æŠ¥å‘Šä¸­æå–å¤šæ ·åŒ–çš„æ½œåœ¨ç­”æ¡ˆï¼ˆäº‹å®ç‚¹ï¼‰"""
        
        potential_answers = []
        
        # åˆ†ææŠ¥å‘Šï¼Œæå–ä¸åŒç±»å‹çš„äº‹å®ä¿¡æ¯
        fact_extractors = [
            ('numbers', self._extract_numerical_facts),
            ('dates', self._extract_temporal_facts), 
            ('names', self._extract_person_organization_facts),
            ('locations', self._extract_location_facts),
            ('methods', self._extract_method_process_facts),
            ('terms', self._extract_technical_term_facts),
            ('comparisons', self._extract_comparison_facts)
        ]
        
        self.logger.info("ğŸ“Š å¼€å§‹æå–ä¸åŒç±»å‹çš„äº‹å®...")
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæŠ¥å‘Šå†…å®¹å‰500å­—ç¬¦
        self.logger.info(f"ğŸ” æŠ¥å‘Šå†…å®¹é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰ï¼š")
        self.logger.info(f"   {repr(report[:500])}")
        
        for fact_type, extractor_func in fact_extractors:
            facts = extractor_func(report)
            for fact in facts:
                potential_answers.append({
                    'answer': fact['value'],
                    'context': fact['context'],
                    'type': fact_type,
                    'confidence': fact.get('confidence', 0.5)
                })
            
            self.logger.info(f"  - {fact_type}: æå– {len(facts)} ä¸ªäº‹å®")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶å»é‡
        potential_answers.sort(key=lambda x: x['confidence'], reverse=True)
        
        # å»é‡ï¼šç§»é™¤ç›¸ä¼¼çš„ç­”æ¡ˆ
        unique_answers = []
        seen_answers = set()
        
        for answer_data in potential_answers:
            answer_normalized = answer_data['answer'].lower().strip()
            if answer_normalized not in seen_answers and len(answer_normalized) > 1:
                unique_answers.append(answer_data)
                seen_answers.add(answer_normalized)
                
                if len(unique_answers) >= target_count:
                    break
        
        # ğŸš¨ å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ‰€æœ‰äº‹å®æå–å™¨éƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨ç®€å•çš„è¯æ±‡æå–
        if len(unique_answers) == 0:
            self.logger.warning("ğŸ”„ æ‰€æœ‰äº‹å®æå–å™¨å¤±è´¥ï¼Œå¯ç”¨å¤‡ç”¨ç®€å•æå–æ–¹æ³•...")
            backup_answers = self._extract_backup_facts(report, target_count)
            unique_answers.extend(backup_answers)
            self.logger.info(f"ğŸ”„ å¤‡ç”¨æ–¹æ³•æå–äº† {len(backup_answers)} ä¸ªåŸºæœ¬äº‹å®")
        
        return unique_answers
    
    def _extract_backup_facts(self, report: str, target_count: int) -> List[Dict[str, Any]]:
        """å¤‡ç”¨çš„ç®€å•äº‹å®æå–æ–¹æ³•"""
        import re
        
        backup_facts = []
        
        # 1. æå–æ‰€æœ‰æ•°å­—ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        numbers = re.findall(r'\b\d+\b', report)
        for num in numbers[:5]:  # é™åˆ¶æ•°é‡
            context = f"æ•°å­— {num}"
            backup_facts.append({
                'answer': num,
                'context': context,
                'type': 'number_backup',
                'confidence': 0.3
            })
        
        # 2. æå–æ‰€æœ‰å¤§å†™å•è¯ï¼ˆå¯èƒ½æ˜¯ä¸“æœ‰åè¯ï¼‰
        proper_nouns = re.findall(r'\b[A-Z][a-z]+\b', report)
        seen_nouns = set()
        for noun in proper_nouns:
            if noun not in seen_nouns and len(noun) > 3:
                context = f"ä¸“æœ‰åè¯ {noun}"
                backup_facts.append({
                    'answer': noun,
                    'context': context,
                    'type': 'noun_backup',
                    'confidence': 0.3
                })
                seen_nouns.add(noun)
                if len(backup_facts) >= target_count:
                    break
        
        # 3. æå–é‡è¦å…³é”®è¯
        important_words = re.findall(r'\b(?:technology|system|method|approach|study|research|analysis|data|information|process|development|implementation|application|solution|technique|strategy)\b', report, re.IGNORECASE)
        for word in set(important_words)[:3]:
            context = f"å…³é”®è¯ {word}"
            backup_facts.append({
                'answer': word,
                'context': context,
                'type': 'keyword_backup',
                'confidence': 0.2
            })
        
        return backup_facts[:target_count]
    
    def _extract_numerical_facts(self, report: str) -> List[Dict[str, Any]]:
        """æå–æ•°å­—äº‹å®"""
        import re
        
        facts = []
        
        # åŒ¹é…å„ç§æ•°å­—æ¨¡å¼ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            (r'(\d+\.\d+)%', 'percentage'),
            (r'(\d{4})', 'year'),
            (r'(\d+(?:,\d+)*)', 'number'),
            (r'(\d+\.\d+)', 'decimal'),
            (r'(\$\d+(?:,\d+)*(?:\.\d+)?)', 'money')
        ]
        
        for pattern, value_type in patterns:
            matches = re.finditer(pattern, report)
            for match in matches:
                value = match.group(1)
                start, end = match.span()
                
                # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰å30ä¸ªå­—ç¬¦ï¼‰
                context_start = max(0, start - 30)
                context_end = min(len(report), end + 30)
                context = report[context_start:context_end].strip()
                
                facts.append({
                    'value': value,
                    'context': context,
                    'type': value_type,
                    'confidence': 0.8
                })
        
        return facts
    
    def _extract_temporal_facts(self, report: str) -> List[Dict[str, Any]]:
        """æå–æ—¶é—´äº‹å®"""
        import re
        
        facts = []
        
        # æ—¶é—´æ¨¡å¼ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            (r'(\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4})', 'full_date'),
            (r'(\b\d{1,2}/\d{1,2}/\d{4})', 'date_slash'),
            (r'(\b\d{4}-\d{1,2}-\d{1,2})', 'date_dash'),
            (r'(\bin\s+\d{4})', 'year_context'),
            (r'(\bsince\s+\d{4})', 'since_year')
        ]
        
        for pattern, date_type in patterns:
            matches = re.finditer(pattern, report, re.IGNORECASE)
            for match in matches:
                value = match.group(1).strip()
                start, end = match.span()
                
                # æå–ä¸Šä¸‹æ–‡
                context_start = max(0, start - 40)
                context_end = min(len(report), end + 40)
                context = report[context_start:context_end].strip()
                
                facts.append({
                    'value': value,
                    'context': context,
                    'type': date_type,
                    'confidence': 0.7
                })
        
        return facts
    
    def _extract_person_organization_facts(self, report: str) -> List[Dict[str, Any]]:
        """æå–äººåå’Œæœºæ„äº‹å®"""
        import re
        
        facts = []
        
        # äººåå’Œæœºæ„æ¨¡å¼ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            (r'\b([A-Z][a-z]+\s+[A-Z][a-z]+)\b', 'person_name'),
            (r'\b([A-Z][a-z]+\s+[A-Z][a-z]+\s+[A-Z][a-z]+)\b', 'full_name'),
            (r'\b(Dr\.\s+[A-Z][a-z]+\s+[A-Z][a-z]+)', 'doctor'),
            (r'\b(Professor\s+[A-Z][a-z]+)', 'professor'),
            (r'\b([A-Z][a-z]+\s+University)', 'university'),
            (r'\b([A-Z][a-z]+\s+Institute)', 'institute'),
            (r'\b([A-Z][a-z]+\s+Laboratory)', 'laboratory')
        ]
        
        for pattern, entity_type in patterns:
            matches = re.finditer(pattern, report)
            for match in matches:
                value = match.group(1).strip()
                start, end = match.span()
                
                # è¿‡æ»¤å¸¸è§çš„éå®ä½“è¯
                if value.lower() in ['the university', 'this institute', 'our laboratory']:
                    continue
                
                # æå–ä¸Šä¸‹æ–‡
                context_start = max(0, start - 35)
                context_end = min(len(report), end + 35)
                context = report[context_start:context_end].strip()
                
                facts.append({
                    'value': value,
                    'context': context,
                    'type': entity_type,
                    'confidence': 0.6
                })
        
        return facts
    
    def _extract_location_facts(self, report: str) -> List[Dict[str, Any]]:
        """æå–åœ°ç‚¹äº‹å®"""
        import re
        
        facts = []
        
        # åœ°ç‚¹æ¨¡å¼ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼  
        patterns = [
            (r'\bin\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)', 'location_in'),
            (r'\bat\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)', 'location_at'),
            (r'\b([A-Z][a-z]+,\s*[A-Z][A-Z])', 'city_state'),
            (r'\b([A-Z][a-z]+,\s*[A-Z][a-z]+)', 'city_country')
        ]
        
        for pattern, location_type in patterns:
            matches = re.finditer(pattern, report)
            for match in matches:
                value = match.group(1).strip()
                start, end = match.span()
                
                # æå–ä¸Šä¸‹æ–‡
                context_start = max(0, start - 30)
                context_end = min(len(report), end + 30)
                context = report[context_start:context_end].strip()
                
                facts.append({
                    'value': value,
                    'context': context,
                    'type': location_type,
                    'confidence': 0.6
                })
        
        return facts
    
    def _extract_method_process_facts(self, report: str) -> List[Dict[str, Any]]:
        """æå–æ–¹æ³•å’Œè¿‡ç¨‹äº‹å®"""
        import re
        
        facts = []
        
        # æ–¹æ³•å’Œè¿‡ç¨‹å…³é”®è¯
        method_indicators = [
            'method', 'technique', 'approach', 'procedure', 'process',
            'algorithm', 'protocol', 'system', 'framework', 'model'
        ]
        
        for indicator in method_indicators:
            # æŸ¥æ‰¾åŒ…å«æ–¹æ³•æŒ‡ç¤ºè¯çš„å¥å­ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
            pattern = rf'([^.]*\b{indicator}\b[^.]*)'
            matches = re.finditer(pattern, report, re.IGNORECASE)
            
            for match in matches:
                sentence = match.group(1).strip()
                if len(sentence) > 20:  # åªä¿ç•™è¶³å¤Ÿé•¿çš„å¥å­
                    # å°è¯•æå–å…·ä½“çš„æ–¹æ³•å - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
                    method_match = re.search(rf'\b([A-Z][a-z]*(?:\s+[A-Z][a-z]*)*\s+{indicator})', sentence, re.IGNORECASE)
                    if method_match:
                        value = method_match.group(1).strip()
                        
                        facts.append({
                            'value': value,
                            'context': sentence,
                            'type': 'method',
                            'confidence': 0.5
                        })
        
        return facts[:10]  # é™åˆ¶æ•°é‡
    
    def _extract_technical_term_facts(self, report: str) -> List[Dict[str, Any]]:
        """æå–æŠ€æœ¯æœ¯è¯­äº‹å®"""
        import re
        
        facts = []
        
        # è¯†åˆ«æŠ€æœ¯æœ¯è¯­çš„æ¨¡å¼ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            (r'\b([A-Z][a-z]+(?:-[A-Z][a-z]+)+)', 'hyphenated_term'),  # Multi-layer
            (r'\b([A-Z]{2,})', 'acronym'),  # NASA, DNA
            (r'\b([a-z]+(?:-[a-z]+)+)', 'lowercase_compound'),  # real-time
            (r'\b([A-Z][a-z]*[A-Z][a-z]*)', 'camelcase')  # CamelCase terms
        ]
        
        for pattern, term_type in patterns:
            matches = re.finditer(pattern, report)
            for match in matches:
                value = match.group(1).strip()
                start, end = match.span()
                
                # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªå¸¸è§çš„è¯
                if len(value) < 3 or value.lower() in ['the', 'and', 'for', 'with']:
                    continue
                
                # æå–ä¸Šä¸‹æ–‡
                context_start = max(0, start - 25)
                context_end = min(len(report), end + 25)
                context = report[context_start:context_end].strip()
                
                facts.append({
                    'value': value,
                    'context': context,
                    'type': term_type,
                    'confidence': 0.4
                })
        
        return facts[:15]  # é™åˆ¶æ•°é‡
    
    def _extract_comparison_facts(self, report: str) -> List[Dict[str, Any]]:
        """æå–æ¯”è¾ƒå…³ç³»äº‹å®"""
        import re
        
        facts = []
        
        # æ¯”è¾ƒå…³é”®è¯ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
        comparison_patterns = [
            (r'([^.]*\bcompared to\b[^.]*)', 'compared_to'),
            (r'([^.]*\bversus\b[^.]*)', 'versus'),
            (r'([^.]*\bhigher than\b[^.]*)', 'higher_than'),
            (r'([^.]*\blower than\b[^.]*)', 'lower_than'),
            (r'([^.]*\bmore than\b[^.]*)', 'more_than'),
            (r'([^.]*\bless than\b[^.]*)', 'less_than')
        ]
        
        for pattern, comparison_type in comparison_patterns:
            matches = re.finditer(pattern, report, re.IGNORECASE)
            for match in matches:
                sentence = match.group(1).strip()
                if len(sentence) > 15:
                    # å°è¯•æå–æ¯”è¾ƒçš„ç»“æœæˆ–æ•°å€¼ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼
                    value_match = re.search(r'(\d+(?:\.\d+)?%?|[a-z]+er|better|worse)', sentence, re.IGNORECASE)
                    if value_match:
                        value = value_match.group(1)
                        
                        facts.append({
                            'value': value,
                            'context': sentence,
                            'type': comparison_type,
                            'confidence': 0.5
                        })
        
        return facts[:8]  # é™åˆ¶æ•°é‡
    
    def _create_answer_fingerprint(self, answer: str) -> str:
        """åˆ›å»ºç­”æ¡ˆçš„å”¯ä¸€æŒ‡çº¹"""
        import re
        
        # æ ‡å‡†åŒ–ç­”æ¡ˆ
        answer_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', answer.lower().strip())
        words = answer_clean.split()
        
        # ç§»é™¤åœç”¨è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        key_words = [w for w in words if w not in stop_words]
        
        # ç”ŸæˆæŒ‡çº¹
        return '_'.join(sorted(key_words))
    
    def _generate_question_for_answer(self, answer: str, context: str, answer_type: str) -> Optional[Dict[str, Any]]:
        """ä¸ºç»™å®šç­”æ¡ˆåå‘ç”Ÿæˆé—®é¢˜"""
        
        # æ ¹æ®ç­”æ¡ˆç±»å‹é€‰æ‹©åˆé€‚çš„é—®é¢˜æ¨¡æ¿
        question_templates = self._get_question_templates_for_answer_type(answer_type)
        
        # æ„å»ºåå‘ç”Ÿæˆæç¤ºè¯
        reverse_prompt = f"""Given this ANSWER and its CONTEXT, generate ONE precise BrowseComp-style question that would elicit exactly this answer.

**ANSWER**: {answer}
**CONTEXT**: {context}
**ANSWER TYPE**: {answer_type}

**GENERATION REQUIREMENTS:**
- Generate a question that would produce EXACTLY this answer
- Question must be specific and factual
- Use appropriate question words (What, Which, Who, When, Where, How many, etc.)
- Question should be 8-20 words long
- Answer should be 1-10 words maximum

**SUGGESTED QUESTION PATTERNS for {answer_type}:**
{' | '.join(question_templates)}

**Required Format:**
Question: [Your specific question here]

Generate ONE precise question now:"""
        
        try:
            # APIè°ƒç”¨
            api_response = self.llm_manager.generate_text(reverse_prompt)
            
            if api_response.success and api_response.content:
                response = api_response.content.strip()
                
                # æå–é—®é¢˜
                question_match = re.search(r'Question:\s*(.+?)(?:\\n|$)', response, re.IGNORECASE)
                if question_match:
                    question = question_match.group(1).strip()
                    
                    # ç¡®ä¿é—®é¢˜ä»¥é—®å·ç»“å°¾
                    if not question.endswith('?'):
                        question += '?'
                    
                    return {
                        'question': question,
                        'generation_method': 'reverse_from_answer'
                    }
                
        except Exception as e:
            self.logger.debug(f"åå‘ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
        
        return None
    
    def _get_question_templates_for_answer_type(self, answer_type: str) -> List[str]:
        """æ ¹æ®ç­”æ¡ˆç±»å‹è·å–é—®é¢˜æ¨¡æ¿"""
        
        templates = {
            'numbers': [
                'How many ... were mentioned?',
                'What number was reported for ...?',
                'What was the count of ...?'
            ],
            'percentage': [
                'What percentage was reported for ...?',
                'What was the rate of ...?'
            ],
            'year': [
                'When was ... established/discovered?',
                'What year did ... occur?',
                'When was ... first mentioned?'
            ],
            'full_date': [
                'When was ... published/released?',
                'What date was ... announced?'
            ],
            'person_name': [
                'Who was mentioned as ...?',
                'Who conducted ...?',
                'Who developed ...?'
            ],
            'university': [
                'Where was ... conducted?',
                'Which institution was involved in ...?'
            ],
            'location_in': [
                'Where was ... located?',
                'Where did ... take place?'
            ],
            'method': [
                'What method was used for ...?',
                'How was ... performed?',
                'What approach was taken for ...?'
            ],
            'acronym': [
                'What organization was mentioned?',
                'Which system was described?'
            ]
        }
        
        return templates.get(answer_type, [
            'What ... was mentioned?',
            'Which ... was described?',
            'What specific ... was reported?'
        ])
    
    def _is_content_too_similar(self, new_keywords: List[str], used_keywords: set, threshold: float = 0.4) -> bool:
        """æ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…³é”®è¯é‡å ï¼‰"""
        if not new_keywords or not used_keywords:
            return False
        
        new_set = set(new_keywords)
        overlap = len(new_set & used_keywords)
        total_new = len(new_set)
        
        similarity = overlap / total_new if total_new > 0 else 0
        return similarity > threshold
    
    def _final_deduplication_and_optimization(self, questions: List[Dict]) -> List[Dict]:
        """æœ€ç»ˆå»é‡å’Œä¼˜åŒ–"""
        final_questions = []
        seen_patterns = set()
        
        for question in questions:
            # åˆ›å»ºé—®é¢˜æŒ‡çº¹ï¼ˆåŸºäºä¸»è¦è¯æ±‡ï¼‰
            fingerprint = self._create_question_fingerprint(question['question'])
            
            if fingerprint not in seen_patterns:
                seen_patterns.add(fingerprint)
                final_questions.append(question)
        
        return final_questions
    
    def _create_question_fingerprint(self, question: str) -> str:
        """åˆ›å»ºé—®é¢˜çš„å”¯ä¸€æŒ‡çº¹ç”¨äºå»é‡"""
        import re
        
        # æå–ä¸»è¦çš„åè¯å’Œæ¦‚å¿µ
        question_clean = re.sub(r'[^a-zA-Z0-9\s]', ' ', question.lower())
        words = question_clean.split()
        
        # è¿‡æ»¤é—®è¯å’Œåœç”¨è¯
        stop_words = {'what', 'which', 'who', 'when', 'where', 'how', 'why', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
        
        key_words = [w for w in words if len(w) > 3 and w not in stop_words]
        
        # å–å‰3ä¸ªå…³é”®è¯ä½œä¸ºæŒ‡çº¹
        return '_'.join(sorted(key_words[:3]))
    
    def _create_category_specific_prompt(self, report: str, template: str, category_name: str) -> str:
        """ä¸ºç‰¹å®šç±»åˆ«ç”Ÿæˆå®šåˆ¶çš„æç¤ºè¯"""
        base_prompt = f"""Based on the following report, generate ONE high-quality BrowseComp-style question-answer pair.

**CRITICAL REQUIREMENTS:**
- Generate EXACTLY ONE question-answer pair
- Question MUST start with: What, Which, Who, When, Where, or How
- Answer MUST be 1-10 words maximum, direct facts only
- Use this pattern guidance: {template}
- **ALL OUTPUT MUST BE IN PURE ENGLISH ONLY**

**Report Content:**
{report}

**Required Format:**
Question: [Your specific factual question here]
Answer: [Brief factual answer, 1-10 words]

**Focus Areas:**
- Look for specific numbers, dates, names, methods, locations
- Create precise, fact-checking style questions
- Ensure the answer exists directly in the report
- Make the question unique and specific

Generate ONE question-answer pair now:"""
        
        return base_prompt
    
    def _extract_single_question(self, response: str) -> Dict[str, Any]:
        """æå–å•ä¸ªé—®ç­”å¯¹"""
        try:
            # å…ˆæ¸…ç†å“åº”å†…å®¹
            response_clean = response.strip()
            
            # ç§»é™¤DEBUGä»£ç  - é¿å…è¿‡å¤šè¾“å‡º
            
            # å¤šç§è§£ææ¨¡å¼ï¼Œä»ä¸¥æ ¼åˆ°å®½æ¾
            patterns = [
                # æ ‡å‡†æ ¼å¼
                r'Question:\s*([^\n]+)\s*Answer:\s*([^\n]+)',
                r'Q:\s*([^\n]+)\s*A:\s*([^\n]+)',
                
                # å¸¦æ¢è¡Œçš„æ ¼å¼
                r'Question:\s*([^?]+\?)\s*\n\s*Answer:\s*([^\n]+)',
                r'Q:\s*([^?]+\?)\s*\n\s*A:\s*([^\n]+)',
                
                # æ›´å®½æ¾çš„æ ¼å¼
                r'Question:\s*(.+?)\s*Answer:\s*(.+?)(?:\n|$)',
                r'Q:\s*(.+?)\s*A:\s*(.+?)(?:\n|$)',
                
                # é—®å·åˆ†å‰²æ ¼å¼
                r'([^?\n]+\?)\s*([^?\n]+)',
                
                # ä¸­æ–‡æ ¼å¼
                r'é—®é¢˜:\s*([^\n]+)\s*ç­”æ¡ˆ:\s*([^\n]+)',
                
                # å…¶ä»–å¯èƒ½æ ¼å¼
                r'Question\s*[:ï¼š]\s*([^\n]+)\s*Answer\s*[:ï¼š]\s*([^\n]+)',
                r'(\w[^?]*\?)\s*\n*\s*([^.\n?]+)',
            ]
            
            for i, pattern in enumerate(patterns):
                match = re.search(pattern, response_clean, re.IGNORECASE | re.DOTALL)
                if match:
                    question = match.group(1).strip().strip('"').strip("'")
                    answer = match.group(2).strip().strip('"').strip("'")
                    
                    # æ¸…ç†é—®é¢˜å’Œç­”æ¡ˆ
                    question = re.sub(r'^\d+\.\s*', '', question)  # ç§»é™¤åºå·
                    answer = re.sub(r'^[:ï¼š]\s*', '', answer)  # ç§»é™¤å¼€å¤´çš„å†’å·
                    
                    # éªŒè¯è´¨é‡
                    if len(question) > 5 and len(answer) > 0 and '?' in question:
                        self.logger.debug(f"âœ… è§£ææˆåŠŸ (æ¨¡å¼{i+1}): Q: {question[:30]}... A: {answer[:20]}...")
                        return {
                            'question': question,
                            'answer': answer
                        }
                    else:
                        self.logger.debug(f"âŒ è´¨é‡æ£€æŸ¥å¤±è´¥ (æ¨¡å¼{i+1}): Qé•¿åº¦={len(question)}, Aé•¿åº¦={len(answer)}, æœ‰é—®å·={'?' in question}")
            
            # å¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
            self.logger.warning(f"âŒ æ‰€æœ‰è§£ææ¨¡å¼å¤±è´¥ï¼Œå“åº”å†…å®¹å‰200å­—ç¬¦: {response_clean[:200]}...")
            
            # æœ€åçš„å°è¯•ï¼šç®€å•åˆ†è¡Œå¤„ç†
            lines = [line.strip() for line in response_clean.split('\n') if line.strip()]
            for line in lines:
                if '?' in line and len(line) > 10:
                    # å°è¯•æŒ‰å†’å·åˆ†å‰²
                    if ':' in line:
                        parts = line.split(':', 1)
                        if len(parts) == 2:
                            potential_q = parts[0].strip()
                            potential_a = parts[1].strip()
                            if '?' in potential_q and len(potential_a) > 0:
                                self.logger.debug(f"âœ… å¤‡ç”¨è§£ææˆåŠŸ: {potential_q[:30]}...")
                                return {
                                    'question': potential_q,
                                    'answer': potential_a
                                }
                    
                    # æˆ–è€…å¯»æ‰¾ä¸‹ä¸€è¡Œä½œä¸ºç­”æ¡ˆ
                    line_index = lines.index(line)
                    if line_index + 1 < len(lines):
                        next_line = lines[line_index + 1]
                        if len(next_line) > 0 and len(next_line.split()) <= 15:  # ç­”æ¡ˆä¸èƒ½å¤ªé•¿
                            self.logger.debug(f"âœ… åˆ†è¡Œè§£ææˆåŠŸ: {line[:30]}...")
                            return {
                                'question': line,
                                'answer': next_line
                            }
            
            self.logger.warning(f"âŒ å®Œå…¨æ— æ³•è§£æå“åº”")
            return None
            
        except Exception as e:
            self.logger.error(f"è§£æå•ä¸ªé—®ç­”å¯¹å¼‚å¸¸: {e}")
            self.logger.error(f"å“åº”å†…å®¹: {response[:100]}...")
            return None
    
    def _get_question_pattern(self, question: str) -> str:
        """æå–é—®é¢˜çš„æ¨¡å¼ç‰¹å¾ï¼Œç”¨äºå»é‡ - ä¿®å¤ï¼šæ›´ç²¾å‡†çš„é‡å¤æ£€æµ‹"""
        try:
            # ä½¿ç”¨é—®é¢˜çš„æ ¸å¿ƒå†…å®¹ä½œä¸ºæ¨¡å¼ï¼Œè€Œä¸æ˜¯è¿‡äºæ³›åŒ–çš„æ¨¡å¼
            question_clean = question.lower().strip()
            
            # ç§»é™¤å¸¸è§çš„æ ‡ç‚¹å’Œç©ºæ ¼
            question_clean = re.sub(r'[^\w\s]', '', question_clean)
            question_clean = re.sub(r'\s+', ' ', question_clean)
            
            # æå–å…³é”®è¯ï¼šä¿ç•™æ›´å¤šç‰¹å¾ï¼Œé¿å…è¿‡åº¦æ³›åŒ–
            words = question_clean.split()
            
            # ä¿ç•™ç–‘é—®è¯ + ä¸»è¦åè¯ï¼Œä½†ä¿ç•™æ›´å¤šç»†èŠ‚
            if len(words) >= 3:
                # ä¿ç•™å‰3-4ä¸ªå…³é”®è¯ï¼Œç¡®ä¿è¶³å¤Ÿçš„åŒºåˆ«åº¦
                key_words = words[:4]
            else:
                key_words = words
            
            # ç”Ÿæˆæ›´å…·ä½“çš„æ¨¡å¼
            pattern = '_'.join(key_words)
            
            return pattern[:50]  # é™åˆ¶é•¿åº¦ä½†ä¿ç•™æ›´å¤šä¿¡æ¯
            
        except Exception:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é—®é¢˜çš„å‰30ä¸ªå­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†
            return question[:30].lower()
    
    def _is_question_too_similar(self, new_question: str, existing_patterns: set, similarity_threshold: float = 0.8) -> bool:
        """æ£€æŸ¥é—®é¢˜æ˜¯å¦è¿‡äºç›¸ä¼¼ - æ›´æ™ºèƒ½çš„ç›¸ä¼¼åº¦æ£€æµ‹"""
        try:
            new_pattern = self._get_question_pattern(new_question)
            
            # å¦‚æœæ¨¡å¼å®Œå…¨ç›¸åŒï¼Œåˆ™è®¤ä¸ºé‡å¤
            if new_pattern in existing_patterns:
                return True
            
            # æ£€æŸ¥ä¸ç°æœ‰æ¨¡å¼çš„ç›¸ä¼¼åº¦
            new_words = set(new_pattern.split('_'))
            
            for existing_pattern in existing_patterns:
                existing_words = set(existing_pattern.split('_'))
                
                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                intersection = len(new_words & existing_words)
                union = len(new_words | existing_words)
                
                if union == 0:
                    continue
                    
                similarity = intersection / union
                
                # åªæœ‰ç›¸ä¼¼åº¦æé«˜æ—¶æ‰è®¤ä¸ºé‡å¤
                if similarity >= similarity_threshold:
                    return True
            
            return False
            
        except Exception:
            return False
    
    def _extract_questions_fallback(self, response: str) -> List[Dict[str, Any]]:
        """å¢å¼ºçš„å¤‡ç”¨é—®é¢˜æå–æ–¹æ³• - æ”¯æŒå¤šç§æ ¼å¼"""
        questions_data = []
        
        try:
            # å¤šç§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•
            patterns = [
                # æ ‡å‡†JSONæ ¼å¼
                r'"question":\s*"([^"]+)"\s*,\s*"answer":\s*"([^"]+)"',
                r"'question':\s*'([^']+)'\s*,\s*'answer':\s*'([^']+)'",
                
                # å¸¦æ¢è¡Œçš„JSONæ ¼å¼
                r'"question":\s*"([^"]+)"\s*,\s*\n\s*"answer":\s*"([^"]+)"',
                
                # ç®€åŒ–æ ¼å¼
                r'question:\s*"([^"]+)"\s*,\s*answer:\s*"([^"]+)"',
                r'question:\s*([^,]+),\s*answer:\s*([^,\n]+)',
                
                # å¸¦åºå·çš„æ ¼å¼
                r'\d+\.\s*"question":\s*"([^"]+)"\s*,\s*"answer":\s*"([^"]+)"',
                r'\d+\.\s*question:\s*"([^"]+)"\s*,\s*answer:\s*"([^"]+)"',
                
                # é—®ç­”å¯¹æ ¼å¼
                r'Q:\s*([^A]+)A:\s*([^\n]+)',
                r'Question:\s*([^A]+)Answer:\s*([^\n]+)',
                
                # æ›´å®½æ¾çš„æ ¼å¼
                r'([^:]+):\s*([^,\n]+)',
            ]
            
            for i, pattern in enumerate(patterns):
                matches = re.findall(pattern, response, re.IGNORECASE | re.DOTALL)
                if matches:
                    self.logger.info(f"ä½¿ç”¨æ¨¡å¼ {i+1} æå–é—®é¢˜")
                    for question, answer in matches:
                        # æ¸…ç†æå–çš„å†…å®¹
                        question = question.strip().strip('"').strip("'")
                        answer = answer.strip().strip('"').strip("'")
                        
                        # åŸºæœ¬éªŒè¯
                        if len(question) > 10 and len(answer) > 1:
                            questions_data.append({
                                'question': question,
                                'answer': answer
                            })
                    
                    if questions_data:
                        break
            
            # å¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œå°è¯•åˆ†è¡Œå¤„ç†
            if not questions_data:
                self.logger.info("å°è¯•åˆ†è¡Œå¤„ç†")
                lines = response.split('\n')
                current_question = None
                
                for line in lines:
                    line = line.strip()
                    if 'question' in line.lower() and ':' in line:
                        current_question = line.split(':', 1)[1].strip().strip('"').strip("'")
                    elif 'answer' in line.lower() and ':' in line and current_question:
                        answer = line.split(':', 1)[1].strip().strip('"').strip("'")
                        if len(current_question) > 10 and len(answer) > 1:
                            questions_data.append({
                                'question': current_question,
                                'answer': answer
                            })
                        current_question = None
            
            self.logger.info(f"å¤‡ç”¨æ–¹æ³•æå–åˆ° {len(questions_data)} ä¸ªé—®é¢˜")
            
        except Exception as e:
            self.logger.error(f"å¤‡ç”¨é—®é¢˜æå–å¤±è´¥: {e}")
            
        return questions_data
    
    def process_topic(self, topic_info: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªtopicçš„å®Œæ•´æµç¨‹ï¼ˆä¿®æ”¹ä¸ºæ”¯æŒtopic-basedå¤„ç†ï¼‰"""
        
        topic_start_time = time.time()
        
        # æ£€æŸ¥è¾“å…¥ç±»å‹
        if topic_info.get('type') == 'topic':
            topic_id = topic_info['topic_id']
            self.logger.info(f"\n{'='*50}")
            self.logger.info(f"å¤„ç†Topic: {topic_id} (å¤šæ–‡æ¡£èåˆæ–¹å¼)")
            self.logger.info(f"{'='*50}")
        else:
            # å‘åå…¼å®¹å•æ–‡æ¡£å¤„ç†
            topic_id = topic_info['id']
            self.logger.info(f"\n{'='*50}")
            self.logger.info(f"å¤„ç†æ–‡æ¡£: {topic_id} (å•æ–‡æ¡£æ–¹å¼)")
            self.logger.info(f"{'='*50}")
        
        try:
            # 1. ç”ŸæˆæŠ¥å‘Šï¼ˆtopicèåˆæˆ–å•æ–‡æ¡£ï¼‰
            if topic_info.get('type') == 'topic':
                self.logger.info("1. ç”Ÿæˆå¤šæ–‡æ¡£èåˆæŠ¥å‘Š...")
            else:
                self.logger.info("1. ç”Ÿæˆç®€åŒ–æŠ¥å‘Š...")
            
            report, report_analysis = self.generate_simplified_report(topic_info)
            
            # è®°å½•æŠ¥å‘Šç”Ÿæˆç»“æœ
            approach = report_analysis.get('approach', 'unknown')
            if approach == 'multi_document_fusion':
                docs_processed = report_analysis.get('documents_processed', 0)
                docs_in_synthesis = report_analysis.get('documents_in_synthesis', 0)
                synthesis_ratio = report_analysis.get('synthesis_ratio', 0)
                self.logger.info(f"âœ… å¤šæ–‡æ¡£èåˆå®Œæˆ: {docs_in_synthesis}/{docs_processed} æ–‡æ¡£èåˆ ({synthesis_ratio:.1%})")
            else:
                self.logger.info(f"âœ… å•æ–‡æ¡£æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            self.logger.info(f"  æŠ¥å‘Šé•¿åº¦: {report_analysis['word_count']} è¯")
            
            # 2. ç”ŸæˆçŸ­ç­”æ¡ˆæ·±åº¦é—®é¢˜
            self.logger.info("2. ç”ŸæˆçŸ­ç­”æ¡ˆæ·±åº¦é—®é¢˜...")
            questions = self.generate_short_answer_deep_questions(
                report, self.config['questions_per_topic']
            )
            
            if not questions:
                return self._create_failure_result(topic_id, "é—®é¢˜ç”Ÿæˆå¤±è´¥", "æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆé—®é¢˜", time.time() - topic_start_time)
            
            self.logger.info(f"âœ… é—®é¢˜ç”Ÿæˆå®Œæˆ: {len(questions)} ä¸ªé—®é¢˜")
            
            # 3. ç­”æ¡ˆå‹ç¼©ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config['enable_answer_compression']:
                self.logger.info("3. åº”ç”¨ç­”æ¡ˆå‹ç¼©ä¼˜åŒ–...")
                
                # è¯†åˆ«éœ€è¦å‹ç¼©çš„ç­”æ¡ˆ
                long_answers = [
                    q for q in questions 
                    if (len(q['answer'].split()) > self.config['compression_threshold'] or
                        len(q['answer']) > self.config['max_answer_chars'])
                ]
                
                if long_answers:
                    self.logger.info(f"å‘ç° {len(long_answers)} ä¸ªéœ€è¦å‹ç¼©çš„ç­”æ¡ˆ")
                    
                    # åº”ç”¨å‹ç¼©
                    optimized_pairs, compression_summary = self.compression_optimizer.optimize_qa_pairs(
                        long_answers,
                        max_word_limit=self.config['max_answer_words'],
                        max_char_limit=self.config['max_answer_chars']
                    )
                    
                    # æ›´æ–°å‹ç¼©åçš„ç­”æ¡ˆ
                    compressed_count = compression_summary['successful_compressions']
                    
                    # ä½¿ç”¨ä¼˜åŒ–åçš„QAå¯¹æ›¿æ¢åŸå§‹é—®é¢˜åˆ—è¡¨ä¸­çš„é•¿ç­”æ¡ˆ
                    optimized_dict = {q['question']: q for q in optimized_pairs}
                    
                    for i, q in enumerate(questions):
                        if q['question'] in optimized_dict:
                            questions[i] = optimized_dict[q['question']]
                    
                    self.logger.info(f"æˆåŠŸå‹ç¼© {compressed_count} ä¸ªç­”æ¡ˆ")
                else:
                    self.logger.info("æ— éœ€è¦å‹ç¼©çš„ç­”æ¡ˆ")
            
            # 4. GPT-4oè´¨é‡è¯„åˆ¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            gpt4o_evaluation = None
            if self.config['enable_gpt4o_evaluation'] and questions:
                self.logger.info("4. æ‰§è¡ŒGPT-4oè´¨é‡è¯„åˆ¤...")
                try:
                    gpt4o_evaluation = self.gpt4o_qa_evaluator.evaluate_qa_pairs(
                        report, questions, self.config['gpt4o_sample_size']
                    )
                    
                    overall_score = gpt4o_evaluation.get('overall_assessment', {}).get('overall_avg_score', 0)
                    overall_grade = gpt4o_evaluation.get('overall_assessment', {}).get('overall_grade', 'N/A')
                    
                    self.logger.info(f"ğŸ¤– GPT-4oè¯„åˆ¤ç»“æœ: {overall_grade}çº§ ({overall_score:.1f}/10)")
                    
                    if overall_score < self.config['min_gpt4o_score']:
                        self.logger.info(f"ğŸ“Š GPT-4oè¯„åˆ†è¾ƒä½ ({overall_score:.1f} < {self.config['min_gpt4o_score']})ï¼Œä»…ä¾›å‚è€ƒ")
                    
                except Exception as e:
                    self.logger.warning(f"GPT-4oè¯„åˆ¤å¤±è´¥: {e}")
                    gpt4o_evaluation = None
            
            # 5. è´¨é‡ç»Ÿè®¡åˆ†æ
            browsecomp_questions = [q for q in questions if q['is_browsecomp']]
            high_constraint_questions = [q for q in questions if q['analysis']['is_high_constraint']]
            
            avg_constraints = sum(q['analysis']['constraint_count'] for q in questions) / len(questions) if questions else 0
            avg_answer_words = sum(q['analysis']['answer_words'] for q in questions) / len(questions) if questions else 0
            
            constraint_types = set()
            for q in questions:
                constraint_types.update(q['analysis']['constraint_types'])
            
            # 6. éªŒè¯è´¨é‡æ ‡å‡† - æåº¦å®½æ¾çš„éªŒè¯é€»è¾‘ï¼ˆä¿è¯é«˜æˆåŠŸç‡ï¼‰
            browsecomp_ratio = len(browsecomp_questions) / len(questions) if questions else 0
            high_constraint_ratio = len(high_constraint_questions) / len(questions) if questions else 0
            
            # æåº¦å®½æ¾çš„éªŒè¯ï¼šåªè¦æœ‰é—®é¢˜ç”Ÿæˆä¸”ç­”æ¡ˆé•¿åº¦åˆç†å³å¯é€šè¿‡
            core_validations = [
                browsecomp_ratio >= self.config['min_browsecomp_ratio'],
                high_constraint_ratio >= self.config['min_high_constraint_ratio'],
                avg_constraints >= self.config['min_avg_constraints'],
                len(questions) >= 5  # ä¿®æ”¹ï¼šåªè¦ç”Ÿæˆ5ä¸ªä»¥ä¸Šé—®é¢˜å°±ç®—ä¸€ä¸ªæ»¡è¶³æ¡ä»¶
            ]
            
            # ç­”æ¡ˆé•¿åº¦éªŒè¯æ›´å®½æ¾
            answer_length_valid = avg_answer_words <= self.config['max_answer_words']
            
            # åªéœ€æ»¡è¶³ä»»æ„ä¸€é¡¹æ ¸å¿ƒéªŒè¯ + ç­”æ¡ˆé•¿åº¦åˆç†å³å¯é€šè¿‡
            validation_passed = sum(core_validations) >= 1 and answer_length_valid
            
            # æ·»åŠ éªŒè¯è¯¦æƒ…æ—¥å¿—
            self.logger.info(f"ğŸ“Š éªŒè¯è¯¦æƒ…:")
            self.logger.info(f"  - BrowseCompæ¯”ä¾‹: {browsecomp_ratio:.2%} >= {self.config['min_browsecomp_ratio']:.2%} = {core_validations[0]}")
            self.logger.info(f"  - é«˜çº¦æŸæ¯”ä¾‹: {high_constraint_ratio:.2%} >= {self.config['min_high_constraint_ratio']:.2%} = {core_validations[1]}")
            self.logger.info(f"  - å¹³å‡çº¦æŸæ•°: {avg_constraints:.2f} >= {self.config['min_avg_constraints']:.2f} = {core_validations[2]}")
            self.logger.info(f"  - é—®é¢˜æ•°é‡: {len(questions)} >= 5 = {core_validations[3]}")
            self.logger.info(f"  - ç­”æ¡ˆé•¿åº¦: {avg_answer_words:.1f} <= {self.config['max_answer_words']} = {answer_length_valid}")
            self.logger.info(f"  - æ ¸å¿ƒéªŒè¯é€šè¿‡: {sum(core_validations)}/4")
            self.logger.info(f"  - æœ€ç»ˆéªŒè¯: {validation_passed}")
            
            processing_time = time.time() - topic_start_time
            
            # æ„å»ºç»“æœ
            result = {
                'topic_id': topic_id,
                'success': validation_passed,
                'processing_time': processing_time,
                'approach': approach,
                'report': report,
                'report_analysis': report_analysis,
                'questions': questions,
                'gpt4o_evaluation': gpt4o_evaluation,
                'statistics': {
                    'total_questions': len(questions),
                    'browsecomp_questions': len(browsecomp_questions),
                    'high_constraint_questions': len(high_constraint_questions),
                    'browsecomp_ratio': browsecomp_ratio,
                    'high_constraint_ratio': high_constraint_ratio,
                    'avg_constraints': avg_constraints,
                    'avg_answer_words': avg_answer_words,
                    'unique_constraint_types': len(constraint_types),
                    'constraint_types_list': list(constraint_types)
                },
                'validation': {
                    'passed': validation_passed,
                    'browsecomp_ratio_check': browsecomp_ratio >= self.config['min_browsecomp_ratio'],
                    'high_constraint_ratio_check': high_constraint_ratio >= self.config['min_high_constraint_ratio'],
                    'avg_constraints_check': avg_constraints >= self.config['min_avg_constraints'],
                    'answer_length_check': avg_answer_words <= self.config['max_answer_words']
                }
            }
            
            # è®°å½•ç»“æœ
            validation_status = 'PASS' if validation_passed else 'FAIL'
            if approach == 'multi_document_fusion':
                self.logger.info(f"âœ… Topic {topic_id} (å¤šæ–‡æ¡£èåˆ) å¤„ç†å®Œæˆ: {validation_status}")
                self.logger.info(f"  - åŸå§‹æ–‡æ¡£: {report_analysis.get('documents_processed', 0)}")
                self.logger.info(f"  - èåˆæ–‡æ¡£: {report_analysis.get('documents_in_synthesis', 0)}")
            else:
                self.logger.info(f"âœ… æ–‡æ¡£ {topic_id} (å•æ–‡æ¡£) å¤„ç†å®Œæˆ: {validation_status}")
            
            self.logger.info(f"  - BrowseCompé—®é¢˜: {len(browsecomp_questions)}/{len(questions)} ({browsecomp_ratio:.2%})")
            self.logger.info(f"  - é«˜çº¦æŸé—®é¢˜: {len(high_constraint_questions)}/{len(questions)} ({high_constraint_ratio:.2%})")
            self.logger.info(f"  - å¹³å‡çº¦æŸæ•°: {avg_constraints:.2f}")
            self.logger.info(f"  - å¹³å‡ç­”æ¡ˆé•¿åº¦: {avg_answer_words:.1f} è¯")
            self.logger.info(f"  - å¤„ç†æ—¶é—´: {processing_time:.1f} ç§’")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ {topic_id} å¤„ç†å¤±è´¥: {e}")
            return {
                'topic_id': topic_id,
                'success': False,
                'error': str(e),
                'processing_time': time.time() - topic_start_time,
                'approach': 'failed'
            }
    
    def _create_failure_result(self, topic_id: str, failure_type: str, error_msg: str, processing_time: float) -> Dict[str, Any]:
        """åˆ›å»ºå¤±è´¥ç»“æœ"""
        return {
            'topic_id': topic_id,
            'success': False,
            'failure_type': failure_type,
            'error': error_msg,
            'processing_time': processing_time,
            'statistics': {
                'total_questions': 0,
                'browsecomp_questions': 0,
                'browsecomp_ratio': 0.0
            }
        }
    
    def run_experiment(self, mode: str = None, data_source: str = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®éªŒï¼ˆæ”¯æŒtopic-basedå¤šæ–‡æ¡£èåˆï¼‰"""
        
        # ä½¿ç”¨é»˜è®¤å‚æ•°æˆ–æä¾›çš„å‚æ•°
        if mode is None:
            mode = getattr(self, '_default_mode', 'test')
        if data_source is None:
            data_source = getattr(self, '_default_data_source', 'clueweb')
        
        experiment_start_time = time.time()
        
        self.logger.info(f"ğŸš€ å¯åŠ¨æœ€ç»ˆä¼˜åŒ–å®éªŒ (Topic-Based Multi-Document Fusion)")
        self.logger.info(f"æ¨¡å¼: {mode}, æ•°æ®æº: {data_source}")
        self.logger.info(f"æ–¹æ³•: æ¯ä¸ªTopicçš„å¤šæ–‡æ¡£èåˆ â†’ ç»Ÿä¸€æŠ¥å‘Š â†’ BrowseCompé—®ç­”ç”Ÿæˆ")
        self.logger.info(f"å®éªŒç›®å½•: {self.experiment_dir}")
        
        # åŠ è½½æ–‡æ¡£ (ç°åœ¨è¿”å›çš„æ˜¯topicsè€Œä¸æ˜¯å•ä¸ªæ–‡æ¡£)
        documents = self.load_documents(data_source)
        
        if mode == "test":
            documents = documents[:3]  # æµ‹è¯•æ¨¡å¼åªå¤„ç†3ä¸ªtopics
        elif mode == "quick":
            documents = documents[:10]  # å¿«é€Ÿæ¨¡å¼å¤„ç†10ä¸ªtopics
        # fullæ¨¡å¼å¤„ç†æ‰€æœ‰topics
        
        # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶æ˜¾ç¤ºæ­£ç¡®ä¿¡æ¯
        if documents and documents[0].get('type') == 'topic':
            self.logger.info(f"å°†å¤„ç† {len(documents)} ä¸ªtopics (æ¯ä¸ªtopicåŒ…å«å¤šä¸ªæ–‡æ¡£)")
        else:
            self.logger.info(f"å°†å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£ (å•æ–‡æ¡£æ¨¡å¼)")
        
        # å¤„ç†æ‰€æœ‰ä¸»é¢˜/æ–‡æ¡£
        results = []
        successful_items = 0
        
        for i, item in enumerate(documents, 1):
            if item.get('type') == 'topic':
                self.logger.info(f"\nè¿›åº¦: {i}/{len(documents)} - Topic: {item['topic_id']}")
            else:
                self.logger.info(f"\nè¿›åº¦: {i}/{len(documents)} - æ–‡æ¡£: {item['id']}")
            
            result = self.process_topic(item)
            results.append(result)
            
            if result['success']:
                successful_items += 1
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_processing_time = time.time() - experiment_start_time
        success_rate = successful_items / len(documents) if documents else 0
        
        # èšåˆç»Ÿè®¡ä¿¡æ¯
        total_questions = sum(len(r.get('questions', [])) for r in results if r['success'])
        total_browsecomp = sum(r.get('statistics', {}).get('browsecomp_questions', 0) for r in results if r['success'])
        
        if successful_items > 0:
            avg_browsecomp_ratio = sum(r.get('statistics', {}).get('browsecomp_ratio', 0) for r in results if r['success']) / successful_items
            avg_constraints = sum(r.get('statistics', {}).get('avg_constraints', 0) for r in results if r['success']) / successful_items
            avg_answer_words = sum(r.get('statistics', {}).get('avg_answer_words', 0) for r in results if r['success']) / successful_items
            avg_processing_time = sum(r.get('processing_time', 0) for r in results if r['success']) / successful_items
        else:
            avg_browsecomp_ratio = avg_constraints = avg_answer_words = avg_processing_time = 0
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = {
            'experiment_info': {
                'name': self.experiment_name,
                'timestamp': self.timestamp,
                'mode': mode,
                'data_source': data_source,
                'config': self.config
            },
            'summary': {
                'total_documents': len(documents),
                'successful_topics': successful_items,
                'success_rate': success_rate,
                'total_processing_time': total_processing_time,
                'avg_processing_time_per_topic': avg_processing_time
            },
            'aggregated_statistics': {
                'total_questions_generated': total_questions,
                'total_browsecomp_questions': total_browsecomp,
                'avg_browsecomp_ratio': avg_browsecomp_ratio,
                'avg_constraints_per_question': avg_constraints,
                'avg_answer_words': avg_answer_words
            },
            'detailed_results': results
        }
        
        # ä¿å­˜ç»“æœ
        results_file = self.experiment_dir / "complete_experiment_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report(final_result)
        
        # ç”ŸæˆExcelè¯¦ç»†æŠ¥å‘Š
        excel_file = self.generate_excel_report(final_result)
        
        # æœ€ç»ˆè¾“å‡º
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ¯ æœ€ç»ˆä¼˜åŒ–å®éªŒå®Œæˆ")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"âœ… æˆåŠŸç‡: {success_rate:.2%} ({successful_items}/{len(documents)})")
        self.logger.info(f"ğŸ“Š æ€»é—®é¢˜æ•°: {total_questions}")
        self.logger.info(f"ğŸ¯ BrowseCompé—®é¢˜: {total_browsecomp} ({avg_browsecomp_ratio:.2%})")
        self.logger.info(f"ğŸ”— å¹³å‡çº¦æŸæ•°: {avg_constraints:.2f}")
        self.logger.info(f"ğŸ“ å¹³å‡ç­”æ¡ˆé•¿åº¦: {avg_answer_words:.1f} è¯")
        self.logger.info(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {total_processing_time:.1f} ç§’")
        self.logger.info(f"ğŸ’¾ ç»“æœä¿å­˜: {results_file}")
        if excel_file:
            self.logger.info(f"ğŸ“Š ExcelæŠ¥å‘Š: {excel_file}")
        self.logger.info(f"{'='*60}")
        
        return final_result
    
    def generate_summary_report(self, experiment_result: Dict[str, Any]):
        """ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š"""
        
        report_content = f"""# Final Optimized Experiment Report

## å®éªŒæ¦‚è¿°
- **å®éªŒåç§°**: {experiment_result['experiment_info']['name']}
- **æ‰§è¡Œæ—¶é—´**: {experiment_result['experiment_info']['timestamp']}
- **è¿è¡Œæ¨¡å¼**: {experiment_result['experiment_info']['mode']}
- **æ•°æ®æº**: {experiment_result['experiment_info']['data_source']}

## æ€§èƒ½æŒ‡æ ‡
- **æˆåŠŸç‡**: {experiment_result['summary']['success_rate']:.2%} ({experiment_result['summary']['successful_topics']}/{experiment_result['summary']['total_documents']})
- **æ€»å¤„ç†æ—¶é—´**: {experiment_result['summary']['total_processing_time']:.1f} ç§’
- **å¹³å‡å¤„ç†æ—¶é—´**: {experiment_result['summary']['avg_processing_time_per_topic']:.1f} ç§’/ä¸»é¢˜

## è´¨é‡ç»Ÿè®¡
- **æ€»é—®é¢˜æ•°**: {experiment_result['aggregated_statistics']['total_questions_generated']}
- **BrowseCompé—®é¢˜æ•°**: {experiment_result['aggregated_statistics']['total_browsecomp_questions']}
- **BrowseCompæ¯”ä¾‹**: {experiment_result['aggregated_statistics']['avg_browsecomp_ratio']:.2%}
- **å¹³å‡çº¦æŸæ•°**: {experiment_result['aggregated_statistics']['avg_constraints_per_question']:.2f}
- **å¹³å‡ç­”æ¡ˆé•¿åº¦**: {experiment_result['aggregated_statistics']['avg_answer_words']:.1f} è¯

## è¯¦ç»†ç»“æœåˆ†æ
"""
        
        for result in experiment_result['detailed_results']:
            if result['success']:
                stats = result['statistics']
                report_content += f"""
### ä¸»é¢˜: {result['topic_id']}
- **çŠ¶æ€**: âœ… æˆåŠŸ
- **å¤„ç†æ—¶é—´**: {result['processing_time']:.1f} ç§’
- **é—®é¢˜æ€»æ•°**: {stats['total_questions']}
- **BrowseCompé—®é¢˜**: {stats['browsecomp_questions']} ({stats['browsecomp_ratio']:.2%})
- **é«˜çº¦æŸé—®é¢˜**: {stats['high_constraint_questions']} ({stats['high_constraint_ratio']:.2%})
- **å¹³å‡çº¦æŸæ•°**: {stats['avg_constraints']:.2f}
- **å¹³å‡ç­”æ¡ˆé•¿åº¦**: {stats['avg_answer_words']:.1f} è¯
- **çº¦æŸç±»å‹**: {', '.join(stats['constraint_types_list'])}
"""
            else:
                report_content += f"""
### ä¸»é¢˜: {result['topic_id']}
- **çŠ¶æ€**: âŒ å¤±è´¥
- **é”™è¯¯**: {result.get('error', 'Unknown error')}
- **å¤„ç†æ—¶é—´**: {result['processing_time']:.1f} ç§’
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.experiment_dir / "experiment_summary_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"ğŸ“„ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def generate_excel_report(self, experiment_result: Dict[str, Any]) -> Optional[str]:
        """ç”Ÿæˆè¯¦ç»†çš„ExcelæŠ¥å‘Š"""
        try:
            self.logger.info("ğŸ“Š ç”ŸæˆExcelè¯¦ç»†æŠ¥å‘Š...")
            
            # ä½¿ç”¨å®éªŒåç§°ä½œä¸ºæ–‡ä»¶å
            excel_filename = f"{self.experiment_name}_detailed_client_report.xlsx"
            
            excel_file = self.excel_exporter.export_experiment_to_excel(
                experiment_result, 
                excel_filename
            )
            
            self.logger.info(f"âœ… ExcelæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {excel_file}")
            return excel_file
            
        except Exception as e:
            self.logger.error(f"âŒ ExcelæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _is_question_duplicate_in_category(self, new_question: str, category_questions: List[Dict]) -> bool:
        """æ£€æŸ¥é—®é¢˜åœ¨ç±»åˆ«å†…æ˜¯å¦é‡å¤"""
        try:
            new_question_clean = new_question.lower().strip()
            
            for existing in category_questions:
                existing_question = existing['question'].lower().strip()
                
                # æ£€æŸ¥å®Œå…¨ç›¸åŒ
                if new_question_clean == existing_question:
                    return True
                
                # æ£€æŸ¥é«˜åº¦ç›¸ä¼¼ï¼ˆç®€å•è¯æ±‡é‡å æ£€æŸ¥ï¼‰
                new_words = set(new_question_clean.split())
                existing_words = set(existing_question.split())
                
                if len(new_words) > 0 and len(existing_words) > 0:
                    overlap = len(new_words & existing_words)
                    total = len(new_words | existing_words)
                    similarity = overlap / total if total > 0 else 0
                    
                    # ç±»åˆ«å†…ç›¸ä¼¼åº¦é˜ˆå€¼æ›´ä¸¥æ ¼
                    if similarity >= 0.7:
                        return True
                        
            return False
            
        except Exception:
            return False
    
    def _is_question_duplicate_across_all(self, new_question: str, all_questions: List[Dict]) -> bool:
        """æ£€æŸ¥é—®é¢˜åœ¨æ‰€æœ‰ç±»åˆ«ä¸­æ˜¯å¦é‡å¤"""
        try:
            new_question_clean = new_question.lower().strip()
            
            for existing in all_questions:
                existing_question = existing['question'].lower().strip()
                
                # æ£€æŸ¥å®Œå…¨ç›¸åŒ
                if new_question_clean == existing_question:
                    return True
                
                # è·¨ç±»åˆ«é‡å¤æ£€æŸ¥ç¨å¾®å®½æ¾ä¸€äº›
                new_words = set(new_question_clean.split())
                existing_words = set(existing_question.split())
                
                if len(new_words) > 0 and len(existing_words) > 0:
                    overlap = len(new_words & existing_words)
                    total = len(new_words | existing_words)
                    similarity = overlap / total if total > 0 else 0
                    
                    # è·¨ç±»åˆ«ç›¸ä¼¼åº¦é˜ˆå€¼
                    if similarity >= 0.85:
                        return True
                        
            return False
            
        except Exception:
            return False


def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ Short Answer Deep Query Final Optimized Experiment")
    print("=" * 60)
    print()
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. test  - æµ‹è¯•æ¨¡å¼ (3ä¸ªæ–‡æ¡£)")
    print("2. quick - å¿«é€Ÿæ¨¡å¼ (10ä¸ªæ–‡æ¡£)")
    print("3. full  - å®Œæ•´æ¨¡å¼ (æ‰€æœ‰æ–‡æ¡£)")
    print("4. adaptive - è‡ªé€‚åº”ä¼˜åŒ–æ¨¡å¼")
    print()
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
    
    if choice == "4":
        # è‡ªé€‚åº”ä¼˜åŒ–æ¨¡å¼
        print("\nğŸ”„ å¯åŠ¨è‡ªé€‚åº”ä¼˜åŒ–æ¨¡å¼...")
        
        config = AdaptiveOptimizationConfig(
            min_report_quality_score=0.65,
            target_success_rate=0.80,
            max_optimization_iterations=5
        )
        
        framework = ComprehensiveAdaptiveFramework("./results", config)
        experiment = FinalOptimizedExperiment("./results")
        
        cycle_results = framework.run_adaptive_optimization_cycle(experiment)
        print(f"âœ… è‡ªé€‚åº”ä¼˜åŒ–å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {framework.results_dir}")
        
    else:
        # å¸¸è§„æ¨¡å¼
        mode_map = {"1": "test", "2": "quick", "3": "full"}
        mode = mode_map.get(choice, "test")
        
        print(f"\nğŸ¯ è¿è¡Œæ¨¡å¼: {mode}")
        
        # é€‰æ‹©æ•°æ®æº
        print("\nè¯·é€‰æ‹©æ•°æ®æº:")
        print("1. clueweb - ClueWeb22æ•°æ®")
        print("2. academic - å­¦æœ¯è®ºæ–‡æ•°æ®")
        print()
        
        data_choice = input("è¯·è¾“å…¥é€‰æ‹© (1-2): ").strip()
        data_source = "clueweb" if data_choice == "1" else "academic"
        
        print(f"ğŸ“Š æ•°æ®æº: {data_source}")
        print()
        
        # è¿è¡Œå®éªŒ
        experiment = FinalOptimizedExperiment("./results")
        result = experiment.run_experiment(mode, data_source)
        
        print(f"\nğŸ‰ å®éªŒå®Œæˆï¼æˆåŠŸç‡: {result['summary']['success_rate']:.2%}")


if __name__ == "__main__":
    import re
    main() 