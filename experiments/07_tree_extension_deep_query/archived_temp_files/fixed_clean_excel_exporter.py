#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆç®€æ´Excelå¯¼å‡ºå™¨ - ä¿®å¤åˆ†æ”¯ç±»å‹è¯†åˆ«å’Œç³…åˆé—®é¢˜å¤„ç†
1. ç³…åˆåçš„é—®ç­”å¯¹ç»“æœ - å¦‚æœæ˜¯å ä½ç¬¦ï¼Œåˆ™æ˜¾ç¤º"æœªç”ŸæˆçœŸæ­£çš„ç»¼åˆé—®é¢˜"
2. è¿‡ç¨‹ä¸­æ‰€æœ‰é—®ç­”å¯¹ï¼ˆæ¯å±‚ï¼‰- æ­£ç¡®è¯†åˆ«root/series/parallelåˆ†æ”¯ç±»å‹
3. è½¨è¿¹æ•°æ®
4. æ•ˆç‡æ•°æ®
"""

import json
import re
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)

class FixedCleanExcelExporter:
    """ä¿®å¤ç‰ˆç®€æ´Excelå¯¼å‡ºå™¨ - 4ä¸ªæ ¸å¿ƒå·¥ä½œè¡¨"""
    
    def __init__(self, output_dir: str = "results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def export_clean_excel(self, json_file: Path) -> Optional[Path]:
        """ç”Ÿæˆä¿®å¤ç‰ˆç®€æ´Excelæ–‡ä»¶"""
        try:
            print(f"ğŸ”„ è¯»å–JSONæ–‡ä»¶: {json_file.name}")
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“Š è§£ææ•°æ®...")
            # è§£ææ•°æ®
            parsed_data = self._parse_data(data)
            
            # ç”ŸæˆExcelæ–‡ä»¶å
            excel_file = self.output_dir / f"FIXED_{json_file.stem}.xlsx"
            
            print(f"ğŸ“‹ ç”Ÿæˆ4ä¸ªä¿®å¤çš„æ ¸å¿ƒå·¥ä½œè¡¨...")
            # å¯¼å‡ºExcel - åªæœ‰4ä¸ªå·¥ä½œè¡¨
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # 1. ç³…åˆåçš„é—®ç­”å¯¹ç»“æœ
                self._write_final_composite_qa(parsed_data, writer)
                
                # 2. è¿‡ç¨‹ä¸­æ‰€æœ‰é—®ç­”å¯¹ï¼ˆæ¯å±‚ï¼‰
                self._write_all_process_qa(parsed_data, writer)
                
                # 3. è½¨è¿¹æ•°æ®
                self._write_trajectory_data(parsed_data, writer)
                
                # 4. æ•ˆç‡æ•°æ®
                self._write_efficiency_data(parsed_data, writer)
            
            print(f"âœ… ä¿®å¤ç‰ˆç®€æ´Excelå·²ç”Ÿæˆ: {excel_file.name}")
            return excel_file
            
        except Exception as e:
            logger.error(f"ä¿®å¤ç‰ˆExcelå¯¼å‡ºå¤±è´¥: {e}")
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            return None
    
    def _parse_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£ææ•°æ®"""
        parsed = {
            'session_info': self._extract_session_info(data),
            'composite_qa': [],      # ç³…åˆåçš„é—®ç­”å¯¹
            'all_process_qa': [],    # æ‰€æœ‰è¿‡ç¨‹ä¸­çš„é—®ç­”å¯¹
            'trajectories': [],      # è½¨è¿¹æ•°æ®
            'efficiency_data': []    # æ•ˆç‡æ•°æ®
        }
        
        processing_results = data.get('processing_results', {})
        processed_docs = processing_results.get('processed_documents', [])
        
        print(f"ğŸ“‹ è§£æå¤„ç†ç»“æœ...")
        
        for doc_idx, doc in enumerate(processed_docs):
            doc_id = doc.get('doc_id', f'Unknown_{doc_idx}')
            reasoning_trees = doc.get('reasoning_trees', [])
            trajectory_records = doc.get('trajectory_records', [])
            
            print(f"  ğŸ“„ å¤„ç†æ–‡æ¡£: {doc_id} ({len(reasoning_trees)} æ¨ç†æ ‘)")
            
            # è§£ææ¨ç†æ ‘
            for tree_idx, tree_str in enumerate(reasoning_trees):
                if isinstance(tree_str, str):
                    # æå–æ ‘ID
                    tree_id_match = re.search(r"tree_id='([^']+)'", tree_str)
                    tree_id = tree_id_match.group(1) if tree_id_match else f'{doc_id}_tree_{tree_idx}'
                    
                    # æå–final_composite_queryï¼ˆç³…åˆåçš„é—®ç­”å¯¹ï¼‰
                    composite_match = re.search(r"final_composite_query='([^']*)'", tree_str)
                    final_composite = composite_match.group(1) if composite_match else 'N/A'
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å ä½ç¬¦æˆ–æ— æ•ˆå†…å®¹
                    is_placeholder = (
                        not final_composite or 
                        final_composite == 'N/A' or 
                        final_composite == 'Logical reasoning chain question requiring genuine step-by-step solving' or
                        len(final_composite) < 30
                    )
                    
                    if not is_placeholder:
                        # æ‰¾æ ¹ç­”æ¡ˆ
                        root_answer = self._extract_root_answer(tree_str)
                        
                        parsed['composite_qa'].append({
                            'doc_id': doc_id,
                            'tree_id': tree_id,
                            'composite_question': final_composite,
                            'target_answer': root_answer,
                            'question_length': len(final_composite),
                            'tree_index': tree_idx,
                            'is_valid': True
                        })
                    else:
                        # è®°å½•æ— æ•ˆçš„ç³…åˆé—®é¢˜
                        root_answer = self._extract_root_answer(tree_str)
                        parsed['composite_qa'].append({
                            'doc_id': doc_id,
                            'tree_id': tree_id,
                            'composite_question': 'âŒ æœªç”ŸæˆçœŸæ­£çš„ç»¼åˆé—®é¢˜ï¼ˆä»…ä¸ºå ä½ç¬¦ï¼‰',
                            'target_answer': root_answer,
                            'question_length': 0,
                            'tree_index': tree_idx,
                            'is_valid': False
                        })
                    
                    # æå–æ‰€æœ‰å±‚çº§çš„é—®ç­”å¯¹
                    process_qa = self._extract_all_qa_from_tree(tree_str, doc_id, tree_id, tree_idx)
                    parsed['all_process_qa'].extend(process_qa)
            
            # è§£æè½¨è¿¹è®°å½•
            for traj in trajectory_records:
                if isinstance(traj, dict):
                    traj_info = self._parse_trajectory(traj, doc_id)
                    if traj_info:
                        parsed['trajectories'].append(traj_info)
            
            # ç”Ÿæˆæ–‡æ¡£çº§æ•ˆç‡æ•°æ®
            valid_composites = sum(1 for comp in parsed['composite_qa'] if comp['doc_id'] == doc_id and comp['is_valid'])
            doc_efficiency = {
                'doc_id': doc_id,
                'processing_time': doc.get('processing_time', 0),
                'trees_generated': len(reasoning_trees),
                'valid_composite_questions': valid_composites,
                'placeholder_composite_questions': len(reasoning_trees) - valid_composites,
                'success': len(reasoning_trees) > 0
            }
            parsed['efficiency_data'].append(doc_efficiency)
        
        valid_composites = sum(1 for comp in parsed['composite_qa'] if comp['is_valid'])
        total_composites = len(parsed['composite_qa'])
        
        print(f"ğŸ“Š è§£æå®Œæˆ:")
        print(f"   ç³…åˆé—®ç­”å¯¹: {total_composites} ä¸ª (æœ‰æ•ˆ: {valid_composites}, å ä½ç¬¦: {total_composites - valid_composites})")
        print(f"   è¿‡ç¨‹é—®ç­”å¯¹: {len(parsed['all_process_qa'])} ä¸ª")
        print(f"   è½¨è¿¹è®°å½•: {len(parsed['trajectories'])} æ¡")
        
        return parsed
    
    def _extract_all_qa_from_tree(self, tree_str: str, doc_id: str, tree_id: str, tree_idx: int) -> List[Dict[str, Any]]:
        """æå–æ¨ç†æ ‘ä¸­æ‰€æœ‰å±‚çº§çš„é—®ç­”å¯¹ï¼Œä¿®å¤åˆ†æ”¯ç±»å‹è¯†åˆ«"""
        qa_pairs = []
        
        # æŸ¥æ‰¾æ‰€æœ‰èŠ‚ç‚¹ID
        node_ids = re.findall(r"'([^']+)': QuestionTreeNode\(", tree_str)
        
        for node_idx, node_id in enumerate(node_ids):
            try:
                # å®šä½èŠ‚ç‚¹
                node_start = tree_str.find(f"'{node_id}': QuestionTreeNode(")
                if node_start == -1:
                    continue
                
                # ç›´æ¥å–ä¸€ä¸ªè¶³å¤Ÿé•¿çš„æ®µè½ï¼ŒåŒ…å«å®Œæ•´çš„èŠ‚ç‚¹ä¿¡æ¯
                node_section = tree_str[node_start:node_start + 1500]
                
                # æå–é—®é¢˜å’Œç­”æ¡ˆ
                query_match = re.search(r"query_text='([^']+)'", node_section)
                answer_match = re.search(r"answer='([^']+)'", node_section)
                
                if query_match and answer_match:
                    query_text = query_match.group(1)
                    answer = answer_match.group(1)
                    
                    # æå–å±‚çº§
                    layer_match = re.search(r"layer_level=(\d+)", node_section)
                    layer = int(layer_match.group(1)) if layer_match else 0
                    
                    # æå–éªŒè¯çŠ¶æ€
                    validation_match = re.search(r"validation_passed=(\w+)", node_section)
                    validation_passed = validation_match.group(1) == 'True' if validation_match else False
                    
                    # ä¿®å¤åˆ†æ”¯ç±»å‹è¯†åˆ« - æŒ‰ä¼˜å…ˆçº§åŒ¹é…
                    branch_type = self._identify_branch_type(node_id)
                    
                    qa_pairs.append({
                        'doc_id': doc_id,
                        'tree_id': tree_id,
                        'tree_index': tree_idx,
                        'node_id': node_id,
                        'layer': layer,
                        'branch_type': branch_type,
                        'question': query_text,
                        'answer': answer,
                        'validation_passed': validation_passed
                    })
                    
            except Exception as e:
                print(f"    âš ï¸ èŠ‚ç‚¹è§£æå¤±è´¥: {node_id} - {e}")
                continue
        
        # æŒ‰å±‚çº§æ’åº
        qa_pairs.sort(key=lambda x: (x['layer'], x['node_id']))
        print(f"    âœ… æå–äº† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
        
        return qa_pairs
    
    def _identify_branch_type(self, node_id: str) -> str:
        """æ­£ç¡®è¯†åˆ«åˆ†æ”¯ç±»å‹ - æŒ‰ä¼˜å…ˆçº§åŒ¹é…"""
        # æŒ‰ç…§æœ€å…·ä½“çš„ç‰¹å¾ä¼˜å…ˆåŒ¹é…
        if '_parallel' in node_id:
            return 'parallel'
        elif '_series' in node_id:
            return 'series'
        elif '_root' in node_id or node_id.endswith('_root'):
            return 'root'
        else:
            return 'unknown'
    
    def _extract_root_answer(self, tree_str: str) -> str:
        """æå–æ ¹ç­”æ¡ˆ"""
        try:
            # æŸ¥æ‰¾rootèŠ‚ç‚¹
            root_match = re.search(r"_root.*?answer='([^']+)'", tree_str)
            if root_match:
                return root_match.group(1)
            return 'N/A'
        except:
            return 'N/A'
    
    def _parse_trajectory(self, traj: Dict[str, Any], doc_id: str) -> Dict[str, Any]:
        """è§£æè½¨è¿¹è®°å½•"""
        return {
            'doc_id': doc_id,
            'step': traj.get('step', 'N/A'),
            'step_id': traj.get('step_id', 0),
            'query_text': traj.get('query_text', 'N/A'),
            'answer': traj.get('answer', 'N/A'),
            'validation_passed': traj.get('validation_passed', False),
            'keyword_count': traj.get('keyword_count', 0),
            'layer': traj.get('layer', 0),
            'tree_id': traj.get('tree_id', 'N/A'),
            'timestamp': traj.get('timestamp', 0)
        }
    
    def _extract_session_info(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ä¼šè¯ä¿¡æ¯"""
        summary = data.get('summary', {})
        return {
            'session_id': data.get('session_id', 'N/A'),
            'total_docs': summary.get('total_documents_attempted', 0),
            'successful_docs': summary.get('successful_documents', 0),
            'total_trees': summary.get('total_reasoning_trees', 0),
            'success_rate': summary.get('success_rate', 0),
            'total_time': data.get('total_processing_time', 0)
        }
    
    # Excelå·¥ä½œè¡¨ç”Ÿæˆå‡½æ•°
    def _write_final_composite_qa(self, data: Dict[str, Any], writer):
        """1. ç³…åˆåçš„é—®ç­”å¯¹ç»“æœ - æ˜¾ç¤ºçœŸå®çŠ¶æ€"""
        valid_count = sum(1 for comp in data['composite_qa'] if comp.get('is_valid', True))
        total_count = len(data['composite_qa'])
        placeholder_count = total_count - valid_count
        
        print(f"  ğŸ“‹ ç”Ÿæˆç³…åˆåé—®ç­”å¯¹å·¥ä½œè¡¨ (æ€»æ•°: {total_count}, æœ‰æ•ˆ: {valid_count}, å ä½ç¬¦: {placeholder_count})")
        
        if not data['composite_qa']:
            # åˆ›å»ºç©ºè¡¨
            empty_data = [['æ–‡æ¡£ID', 'æ¨ç†æ ‘ID', 'ç³…åˆåçš„ç»¼åˆé—®é¢˜', 'ç›®æ ‡ç­”æ¡ˆ', 'é—®é¢˜çŠ¶æ€', 'é—®é¢˜é•¿åº¦', 'æ ‘ç´¢å¼•']]
            df = pd.DataFrame(empty_data[1:], columns=empty_data[0])
        else:
            composite_data = []
            for idx, comp in enumerate(data['composite_qa']):
                status = 'âœ… æœ‰æ•ˆ' if comp.get('is_valid', True) else 'âŒ å ä½ç¬¦'
                composite_data.append({
                    'åºå·': idx + 1,
                    'æ–‡æ¡£ID': comp['doc_id'],
                    'æ¨ç†æ ‘ID': comp['tree_id'],
                    'ç³…åˆåçš„ç»¼åˆé—®é¢˜': comp['composite_question'],
                    'ç›®æ ‡ç­”æ¡ˆ': comp['target_answer'],
                    'é—®é¢˜çŠ¶æ€': status,
                    'é—®é¢˜é•¿åº¦': comp['question_length'],
                    'æ ‘ç´¢å¼•': comp['tree_index']
                })
            
            df = pd.DataFrame(composite_data)
        
        df.to_excel(writer, sheet_name='1-ç³…åˆåé—®ç­”å¯¹', index=False)
    
    def _write_all_process_qa(self, data: Dict[str, Any], writer):
        """2. è¿‡ç¨‹ä¸­æ‰€æœ‰é—®ç­”å¯¹ï¼ˆæ¯å±‚ï¼‰- æ­£ç¡®çš„åˆ†æ”¯ç±»å‹"""
        print(f"  ğŸ“‹ ç”Ÿæˆè¿‡ç¨‹é—®ç­”å¯¹å·¥ä½œè¡¨ ({len(data['all_process_qa'])} ä¸ª)")
        
        # ç»Ÿè®¡åˆ†æ”¯ç±»å‹
        branch_stats = {}
        for qa in data['all_process_qa']:
            branch_type = qa['branch_type']
            branch_stats[branch_type] = branch_stats.get(branch_type, 0) + 1
        
        print(f"    åˆ†æ”¯ç±»å‹ç»Ÿè®¡: {branch_stats}")
        
        if not data['all_process_qa']:
            # åˆ›å»ºç©ºè¡¨
            empty_data = [['æ–‡æ¡£ID', 'æ¨ç†æ ‘ID', 'èŠ‚ç‚¹ID', 'å±‚çº§', 'åˆ†æ”¯ç±»å‹', 'é—®é¢˜', 'ç­”æ¡ˆ', 'éªŒè¯çŠ¶æ€']]
            df = pd.DataFrame(empty_data[1:], columns=empty_data[0])
        else:
            process_data = []
            for idx, qa in enumerate(data['all_process_qa']):
                process_data.append({
                    'åºå·': idx + 1,
                    'æ–‡æ¡£ID': qa['doc_id'],
                    'æ¨ç†æ ‘ID': qa['tree_id'],
                    'æ ‘ç´¢å¼•': qa['tree_index'],
                    'èŠ‚ç‚¹ID': qa['node_id'],
                    'å±‚çº§': qa['layer'],
                    'åˆ†æ”¯ç±»å‹': qa['branch_type'],
                    'é—®é¢˜': qa['question'],
                    'ç­”æ¡ˆ': qa['answer'],
                    'éªŒè¯çŠ¶æ€': 'âœ… é€šè¿‡' if qa['validation_passed'] else 'âŒ å¤±è´¥'
                })
            
            df = pd.DataFrame(process_data)
        
        df.to_excel(writer, sheet_name='2-è¿‡ç¨‹ä¸­æ‰€æœ‰é—®ç­”å¯¹', index=False)
    
    def _write_trajectory_data(self, data: Dict[str, Any], writer):
        """3. è½¨è¿¹æ•°æ®"""
        print(f"  ğŸ“‹ ç”Ÿæˆè½¨è¿¹æ•°æ®å·¥ä½œè¡¨ ({len(data['trajectories'])} æ¡)")
        
        if not data['trajectories']:
            # åˆ›å»ºç©ºè¡¨
            empty_data = [['æ–‡æ¡£ID', 'æ­¥éª¤', 'æ­¥éª¤ID', 'å±‚çº§', 'é—®é¢˜', 'ç­”æ¡ˆ', 'éªŒè¯çŠ¶æ€', 'å…³é”®è¯æ•°é‡']]
            df = pd.DataFrame(empty_data[1:], columns=empty_data[0])
        else:
            traj_data = []
            for idx, traj in enumerate(data['trajectories']):
                traj_data.append({
                    'åºå·': idx + 1,
                    'æ–‡æ¡£ID': traj['doc_id'],
                    'æ­¥éª¤': traj['step'],
                    'æ­¥éª¤ID': traj['step_id'],
                    'å±‚çº§': traj['layer'],
                    'é—®é¢˜': traj['query_text'],
                    'ç­”æ¡ˆ': traj['answer'],
                    'éªŒè¯çŠ¶æ€': 'âœ… é€šè¿‡' if traj['validation_passed'] else 'âŒ å¤±è´¥',
                    'å…³é”®è¯æ•°é‡': traj['keyword_count'],
                    'æ¨ç†æ ‘ID': traj['tree_id'],
                    'æ—¶é—´æˆ³': traj['timestamp']
                })
            
            df = pd.DataFrame(traj_data)
        
        df.to_excel(writer, sheet_name='3-è½¨è¿¹æ•°æ®', index=False)
    
    def _write_efficiency_data(self, data: Dict[str, Any], writer):
        """4. æ•ˆç‡æ•°æ® - åŒ…å«ç³…åˆé—®é¢˜è´¨é‡ç»Ÿè®¡"""
        print(f"  ğŸ“‹ ç”Ÿæˆæ•ˆç‡æ•°æ®å·¥ä½œè¡¨")
        
        session_info = data['session_info']
        valid_composites = sum(1 for comp in data['composite_qa'] if comp.get('is_valid', True))
        total_composites = len(data['composite_qa'])
        placeholder_composites = total_composites - valid_composites
        
        # æ•´ä½“æ•ˆç‡æ•°æ®
        overall_data = [
            ['é¡¹ç›®', 'æ•°å€¼', 'å•ä½'],
            ['ä¼šè¯ID', session_info['session_id'], ''],
            ['æ€»å¤„ç†æ—¶é—´', f"{session_info['total_time']:.2f}", 'ç§’'],
            ['å¤„ç†æ–‡æ¡£æ•°', session_info['total_docs'], 'ä¸ª'],
            ['æˆåŠŸæ–‡æ¡£æ•°', session_info['successful_docs'], 'ä¸ª'],
            ['æˆåŠŸç‡', f"{session_info['success_rate']:.1%}", ''],
            ['ç”Ÿæˆæ¨ç†æ ‘', session_info['total_trees'], 'ä¸ª'],
            ['', '', ''],
            ['ç³…åˆé—®é¢˜è´¨é‡', '', ''],
            ['æ€»ç³…åˆé—®ç­”å¯¹', total_composites, 'ä¸ª'],
            ['æœ‰æ•ˆç³…åˆé—®é¢˜', valid_composites, 'ä¸ª'],
            ['å ä½ç¬¦é—®é¢˜', placeholder_composites, 'ä¸ª'],
            ['ç³…åˆé—®é¢˜æœ‰æ•ˆç‡', f"{(valid_composites/max(total_composites, 1))*100:.1f}", '%'],
            ['', '', ''],
            ['è¿‡ç¨‹æ•°æ®', '', ''],
            ['è¿‡ç¨‹é—®ç­”å¯¹', len(data['all_process_qa']), 'ä¸ª'],
            ['è½¨è¿¹è®°å½•', len(data['trajectories']), 'æ¡'],
            ['', '', ''],
            ['å¹³å‡æ•ˆç‡', '', ''],
            ['å¹³å‡æ—¶é—´/æ–‡æ¡£', f"{session_info['total_time']/max(session_info['total_docs'], 1):.2f}", 'ç§’/æ–‡æ¡£'],
            ['å¹³å‡æ¨ç†æ ‘/æ–‡æ¡£', f"{session_info['total_trees']/max(session_info['successful_docs'], 1):.1f}", 'ä¸ª/æ–‡æ¡£'],
            ['å¹³å‡é—®ç­”å¯¹/æ¨ç†æ ‘', f"{len(data['all_process_qa'])/max(session_info['total_trees'], 1):.1f}", 'ä¸ª/æ ‘']
        ]
        
        # æ–‡æ¡£çº§æ•ˆç‡æ•°æ®
        if data['efficiency_data']:
            overall_data.extend([
                ['', '', ''],
                ['æ–‡æ¡£çº§è¯¦ç»†æ•°æ®', '', ''],
                ['æ–‡æ¡£ID', 'å¤„ç†æ—¶é—´', 'æ¨ç†æ ‘æ•°é‡', 'æœ‰æ•ˆç³…åˆé—®é¢˜', 'å ä½ç¬¦ç³…åˆé—®é¢˜']
            ])
            
            for eff in data['efficiency_data']:
                overall_data.append([
                    eff['doc_id'],
                    f"{eff['processing_time']:.1f}ç§’",
                    f"{eff['trees_generated']}ä¸ª",
                    f"{eff['valid_composite_questions']}ä¸ª",
                    f"{eff['placeholder_composite_questions']}ä¸ª"
                ])
        
        df = pd.DataFrame(overall_data)
        df.to_excel(writer, sheet_name='4-æ•ˆç‡æ•°æ®', index=False, header=False)

def main():
    """ä¸»å‡½æ•°"""
    results_dir = Path("results")
    
    json_files = list(results_dir.glob("agent_reasoning_production_*.json"))
    
    if not json_files:
        print("âŒ æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        return
    
    exporter = FixedCleanExcelExporter()
    
    for json_file in json_files:
        print(f"\nğŸš€ ç”Ÿæˆä¿®å¤ç‰ˆç®€æ´Excel: {json_file.name}")
        excel_file = exporter.export_clean_excel(json_file)
        
        if excel_file:
            print(f"âœ… ä¿®å¤ç‰ˆç®€æ´Excelå·²ç”Ÿæˆ: {excel_file.name}")
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {json_file.name}")

if __name__ == "__main__":
    main() 