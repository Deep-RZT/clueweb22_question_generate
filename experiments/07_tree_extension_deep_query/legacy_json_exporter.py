#!/usr/bin/env python3
"""
æ—§æ ¼å¼JSONå¯¼å‡ºå™¨ - ä¸“é—¨å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„AgentReasoningTreeæ•°æ®
å¤„ç† agent_reasoning_production_en0028_20250722_122412.json è¿™ç±»æ—§æ ¼å¼æ–‡ä»¶
"""

import json
import pandas as pd
import re
import openai
import os
from typing import Dict, List, Any, Optional
from pathlib import Path

class LegacyJsonExporter:
    """æ—§æ ¼å¼JSONå¯¼å‡ºå™¨ - å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„æ¨ç†æ ‘"""
    
    def __init__(self):
        self.data = {
            'documents_processed': 0,
            'total_reasoning_trees': 0,
            'total_processing_time': 0.0
        }
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = None
        try:
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = openai.OpenAI(api_key=api_key)
                print("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œå°†è°ƒç”¨GPT-4oç”Ÿæˆè‡ªç„¶è¯­è¨€æ•´åˆé—®é¢˜")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°OpenAI API keyï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        except Exception as e:
            print(f"âš ï¸ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def export_to_excel(self, json_file_path: str, output_dir: str = "results") -> str:
        """å¯¼å‡ºæ—§æ ¼å¼JSONåˆ°Excel"""
        try:
            print(f"ğŸ”„ å¼€å§‹å¤„ç†æ—§æ ¼å¼JSONæ–‡ä»¶: {json_file_path}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
            
            # è¯»å–JSONæ–‡ä»¶
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è§£ææ—§æ ¼å¼æ•°æ®
            parsed_data = self._parse_legacy_data(data)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            input_filename = Path(json_file_path).stem
            excel_filename = f"legacy_export_{input_filename}.xlsx"
            excel_path = output_path / excel_filename
            
            # å†™å…¥Excel
            self._write_to_excel(parsed_data, str(excel_path))
            
            print(f"âœ… Excelå¯¼å‡ºå®Œæˆ: {excel_path}")
            return str(excel_path)
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            raise

    def _parse_legacy_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£ææ—§æ ¼å¼çš„å­—ç¬¦ä¸²JSONæ•°æ®"""
        parsed = {
            'efficiency_data': [],   # æ–‡æ¡£å¤„ç†æ•ˆç‡ç»Ÿè®¡
            'composite_qa': [],      # ç³…åˆåçš„é—®ç­”å¯¹
            'all_process_qa': [],    # æ‰€æœ‰è¿‡ç¨‹ä¸­çš„é—®ç­”å¯¹
            'trajectories': [],      # è½¨è¿¹æ•°æ®
        }
        
        print(f"ğŸ“Š è§£ææ•°æ®ç»“æ„: {list(data.keys())}")
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„ç»“æœ
        if 'results' in data:
            for doc_result in data['results']:
                doc_id = doc_result.get('document_id', 'unknown')
                print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc_id}")
                
                # å¤„ç†æ¨ç†æ ‘
                reasoning_trees = doc_result.get('reasoning_trees', [])
                for tree_idx, tree_str in enumerate(reasoning_trees):
                    if isinstance(tree_str, str):  # å­—ç¬¦ä¸²æ ¼å¼æ•°æ®
                        print(f"ğŸŒ³ è§£ææ¨ç†æ ‘ {tree_idx + 1}: å­—ç¬¦ä¸²æ ¼å¼")
                        
                        # æå–é—®ç­”å¯¹
                        qa_pairs = self._extract_all_qa_from_tree(tree_str, doc_id, tree_idx)
                        parsed['all_process_qa'].extend(qa_pairs)
                        
                        # ç”Ÿæˆç³…åˆé—®é¢˜å’Œç­”æ¡ˆ
                        root_answer = self._extract_root_answer(tree_str)
                        composite_qa = self._generate_composite_questions(qa_pairs, root_answer, doc_id, tree_idx)
                        
                        if composite_qa:
                            parsed['composite_qa'].append({
                                'doc_id': doc_id,
                                'tree_id': f"tree_{tree_idx}",
                                'tree_index': tree_idx,
                                'target_answer': root_answer,
                                'nested_question': composite_qa.get('nested_cumulative_question', 'âŒ åµŒå¥—ç´¯ç§¯å‹é—®é¢˜ç”Ÿæˆå¤±è´¥'),
                                'natural_question': composite_qa.get('llm_integrated_question', 'âŒ LLMæ•´åˆå‹é—®é¢˜ç”Ÿæˆå¤±è´¥'),
                                'nested_answer': composite_qa.get('nested_cumulative_answer', 'âŒ åµŒå¥—ç´¯ç§¯å‹ç­”æ¡ˆç”Ÿæˆå¤±è´¥'),
                                'natural_answer': composite_qa.get('llm_integrated_answer', 'âŒ LLMæ•´åˆå‹ç­”æ¡ˆç”Ÿæˆå¤±è´¥'),
                                'debug_info': composite_qa.get('debug_info', '')
                            })
                
                # æ·»åŠ æ•ˆç‡æ•°æ®
                parsed['efficiency_data'].append({
                    'document_id': doc_id,
                    'processing_time': doc_result.get('processing_time', 0),
                    'reasoning_trees_count': len(reasoning_trees),
                    'nested_questions_count': sum(1 for qa in parsed['composite_qa'] if qa['doc_id'] == doc_id and 'âŒ' not in qa['nested_question']),
                    'llm_questions_count': sum(1 for qa in parsed['composite_qa'] if qa['doc_id'] == doc_id and 'âŒ' not in qa['natural_question']),
                    'total_valid_questions': sum(1 for qa in parsed['composite_qa'] if qa['doc_id'] == doc_id and 'âŒ' not in qa['nested_question'] and 'âŒ' not in qa['natural_question']),
                    'success': doc_result.get('success', True)
                })
                
                # å¤„ç†è½¨è¿¹æ•°æ®
                if 'trajectories' in doc_result:
                    parsed['trajectories'].extend(self._parse_trajectories(doc_result['trajectories'], doc_id))
        
        print(f"âœ… è§£æå®Œæˆ: {len(parsed['composite_qa'])} ä¸ªç³…åˆé—®ç­”, {len(parsed['all_process_qa'])} ä¸ªè¿‡ç¨‹é—®ç­”")
        return parsed

    def _extract_all_qa_from_tree(self, tree_str: str, doc_id: str, tree_idx: int) -> List[Dict[str, Any]]:
        """ä»å­—ç¬¦ä¸²æ ¼å¼æ¨ç†æ ‘ä¸­æå–æ‰€æœ‰é—®ç­”å¯¹"""
        qa_pairs = []
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æŸ¥è¯¢å’Œç­”æ¡ˆ
            # åŒ¹é… PreciseQuery ç»“æ„
            query_pattern = r"PreciseQuery\(\s*query_id='([^']+)'.*?query_text='([^']+)'.*?answer='([^']+)'.*?\)"
            matches = re.findall(query_pattern, tree_str, re.DOTALL)
            
            for query_id, query_text, answer in matches:
                # æ¨æ–­å±‚çº§
                layer_level = self._infer_layer_from_node_id(query_id)
                
                # ç¡®å®šåˆ†æ”¯ç±»å‹
                branch_type = self._identify_branch_type(query_id)
                
                qa_pairs.append({
                    'document_id': doc_id,
                    'tree_id': f"tree_{tree_idx}",
                    'node_id': query_id,
                    'layer': layer_level,
                    'branch_type': branch_type,
                    'query_text': query_text,
                    'answer': answer,
                    'generation_method': 'extracted_from_tree',
                    'validation_passed': True  # å‡è®¾ä»æ ‘ä¸­æå–çš„éƒ½æ˜¯æœ‰æ•ˆçš„
                })
                
            # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not qa_pairs:
                print(f"âš ï¸ æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç®€å•æ–‡æœ¬æå– for tree_{tree_idx}")
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šçš„æå–é€»è¾‘
                
        except Exception as e:
            print(f"âŒ æå–é—®ç­”å¯¹å¤±è´¥ for tree_{tree_idx}: {e}")
            
        return qa_pairs

    def _identify_branch_type(self, node_id: str) -> str:
        """è¯†åˆ«åˆ†æ”¯ç±»å‹"""
        if 'root' in node_id.lower():
            return 'root'
        elif 'series' in node_id.lower():
            return 'series'
        elif 'parallel' in node_id.lower():
            return 'parallel'
        else:
            return 'unknown'

    def _infer_layer_from_node_id(self, node_id: str) -> int:
        """ä»èŠ‚ç‚¹IDæ¨æ–­å±‚çº§"""
        try:
            # æ ¹æ®node_idæ¨¡å¼æ¨æ–­å±‚çº§
            if '_root' in node_id or node_id.endswith('_root'):
                return 0  # æ ¹èŠ‚ç‚¹æ˜¯ç¬¬0å±‚
            elif '_series1' in node_id or '_parallel1' in node_id:
                return 1  # ç¬¬ä¸€å±‚æ‰©å±•
            elif '_series2' in node_id or '_parallel2' in node_id:
                return 2  # ç¬¬äºŒå±‚æ‰©å±•
            elif 'series' in node_id and not any(x in node_id for x in ['series1', 'series2']):
                return 1  # é»˜è®¤seriesæ˜¯ç¬¬ä¸€å±‚
            elif 'parallel' in node_id and not any(x in node_id for x in ['parallel1', 'parallel2']):
                return 1  # é»˜è®¤parallelæ˜¯ç¬¬ä¸€å±‚
            else:
                # å°è¯•ä»æ•°å­—åç¼€æ¨æ–­
                parts = node_id.split('_')
                for part in reversed(parts):
                    if part.isdigit():
                        return min(int(part), 2)  # æœ€å¤§å±‚çº§æ˜¯2
                return 0  # é»˜è®¤è¿”å›0
        except Exception:
            return 0

    def _extract_root_answer(self, tree_str: str) -> str:
        """æå–æ ¹ç­”æ¡ˆ"""
        try:
            # æŸ¥æ‰¾æ ¹èŠ‚ç‚¹çš„ç­”æ¡ˆ
            root_pattern = r"root.*?answer='([^']+)'"
            match = re.search(root_pattern, tree_str, re.DOTALL | re.IGNORECASE)
            if match:
                return match.group(1)
            
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªç­”æ¡ˆ
            first_answer_pattern = r"answer='([^']+)'"
            match = re.search(first_answer_pattern, tree_str)
            if match:
                return match.group(1)
                
            return "æœªæ‰¾åˆ°æ ¹ç­”æ¡ˆ"
        except Exception as e:
            print(f"âŒ æå–æ ¹ç­”æ¡ˆå¤±è´¥: {e}")
            return "æå–å¤±è´¥"

    def _generate_composite_questions(self, qa_pairs: List[Dict], root_answer: str, doc_id: str, tree_idx: int) -> Dict[str, str]:
        """ç”ŸæˆåŒæ ¼å¼åµŒå¥—é—®é¢˜å’Œç­”æ¡ˆ"""
        if not qa_pairs:
            return self._fallback_generation(root_answer, "æ— é—®ç­”å¯¹æ•°æ®")
        
        try:
            # æŒ‰å±‚çº§åˆ†ç»„
            queries_by_layer = {}
            answers_by_layer = {}
            
            for qa in qa_pairs:
                layer = qa['layer']
                if layer not in queries_by_layer:
                    queries_by_layer[layer] = []
                    answers_by_layer[layer] = []
                queries_by_layer[layer].append(qa['query_text'])
                answers_by_layer[layer].append(qa['answer'])
            
            debug_info = f"æå–åˆ° {len(queries_by_layer)} å±‚æ•°æ®: {list(queries_by_layer.keys())}"
            print(f"ğŸ“Š {debug_info}")
            
            # ç”ŸæˆåµŒå¥—é—®é¢˜å’Œç­”æ¡ˆ
            if len(queries_by_layer) == 1:
                # åªæœ‰ä¸€å±‚æ•°æ®ï¼Œä½¿ç”¨ç®€åŒ–ç”Ÿæˆ
                layer = list(queries_by_layer.keys())[0]
                queries = queries_by_layer[layer]
                answers = answers_by_layer[layer]
                
                nested_question = f"({', '.join(queries)})" if len(queries) > 1 else queries[0]
                nested_answer = f"({', '.join(answers)})" if len(answers) > 1 else answers[0]
                
                llm_question = self._generate_single_layer_llm_question(queries)
                llm_answer = self._generate_single_layer_llm_answer(answers, root_answer)
                
                return {
                    'nested_cumulative_question': nested_question,
                    'nested_cumulative_answer': nested_answer,
                    'llm_integrated_question': llm_question,
                    'llm_integrated_answer': llm_answer,
                    'debug_info': debug_info + " (å•å±‚ç®€åŒ–ç”Ÿæˆ)"
                }
            else:
                # å¤šå±‚æ•°æ®ï¼Œæ­£å¸¸ç”Ÿæˆ
                nested_question = self._generate_nested_cumulative_from_layers(queries_by_layer)
                nested_answer = self._generate_nested_cumulative_answer_from_layers(answers_by_layer)
                
                llm_question = self._generate_llm_integrated_question(queries_by_layer)
                llm_answer = self._generate_llm_integrated_answer(answers_by_layer, root_answer)
                
                return {
                    'nested_cumulative_question': nested_question,
                    'nested_cumulative_answer': nested_answer,
                    'llm_integrated_question': llm_question,
                    'llm_integrated_answer': llm_answer,
                    'debug_info': debug_info + " (å¤šå±‚æ­£å¸¸ç”Ÿæˆ)"
                }
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç³…åˆé—®é¢˜å¤±è´¥: {e}")
            return self._fallback_generation(root_answer, f"å¼‚å¸¸: {str(e)}")

    def _generate_single_layer_llm_question(self, queries: List[str]) -> str:
        """ä¸ºå•å±‚æ•°æ®ç”ŸæˆLLMæ•´åˆé—®é¢˜"""
        if not self.openai_client:
            return f"æ•´åˆé—®é¢˜: {', '.join(queries)}"
        
        try:
            if len(queries) == 1:
                return queries[0]
            
            combined_queries = "; ".join(queries)
            prompt = f"""Create a natural reasoning question based on this sub-question to determine the answer.

Sub-questions: {combined_queries}

Create a single, clear question that logically combines these elements."""

            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You create clear reasoning questions."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200,
                timeout=30
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ LLMé—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return f"æ•´åˆé—®é¢˜: {', '.join(queries)}"

    def _generate_single_layer_llm_answer(self, answers: List[str], root_answer: str) -> str:
        """ä¸ºå•å±‚æ•°æ®ç”ŸæˆLLMæ•´åˆç­”æ¡ˆ"""
        if not self.openai_client:
            return f"æ•´åˆç­”æ¡ˆ: {', '.join(answers)} â†’ {root_answer}"
        
        try:
            if len(answers) == 1:
                return answers[0]
            
            combined_answers = "; ".join(answers)
            prompt = f"""Create a natural reasoning answer that shows how the sub-answer leads to the final answer.

Sub-answers: {combined_answers}
Final answer: {root_answer}

Show the logical connection."""

            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You create clear logical explanations."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200,
                timeout=30
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return f"æ•´åˆç­”æ¡ˆ: {', '.join(answers)} â†’ {root_answer}"

    def _generate_nested_cumulative_from_layers(self, queries_by_layer: Dict[int, List[str]]) -> str:
        """ç”ŸæˆåµŒå¥—ç´¯ç§¯å‹é—®é¢˜"""
        try:
            if not queries_by_layer:
                return "(ç©ºé—®é¢˜æ ‘)"
            
            # è·å–æ‰€æœ‰å±‚çº§ï¼Œä»æ·±åˆ°æµ…æ’åº
            layers = sorted(queries_by_layer.keys(), reverse=True)
            
            # å¼€å§‹æ„å»ºåµŒå¥—ç»“æ„
            nested_parts = []
            
            for layer in layers:
                queries = queries_by_layer[layer]
                if queries:
                    if len(queries) == 1:
                        nested_parts.append(queries[0])
                    else:
                        # å¤šä¸ªæŸ¥è¯¢ç”¨æ‹¬å·ç»„åˆ
                        combined = f"({', '.join(queries)})"
                        nested_parts.append(combined)
            
            if not nested_parts:
                return "(æ— æœ‰æ•ˆé—®é¢˜)"
            
            # æ„å»ºæœ€ç»ˆçš„åµŒå¥—ç»“æ„ï¼šä»æœ€æ·±å±‚åˆ°æœ€æµ…å±‚
            if len(nested_parts) == 1:
                return nested_parts[0]
            elif len(nested_parts) == 2:
                return f"({nested_parts[1]}, {nested_parts[0]})"
            else:
                # 3å±‚æˆ–æ›´å¤šï¼š(root, (middle, deepest))
                deepest = nested_parts[0]
                middle = nested_parts[1]
                root = nested_parts[2] if len(nested_parts) > 2 else ""
                
                if root:
                    return f"({root}, ({middle}, {deepest}))"
                else:
                    return f"({middle}, {deepest})"
                    
        except Exception as e:
            print(f"âŒ åµŒå¥—é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return "(åµŒå¥—ç”Ÿæˆå¤±è´¥)"

    def _generate_nested_cumulative_answer_from_layers(self, answers_by_layer: Dict[int, List[str]]) -> str:
        """ç”ŸæˆåµŒå¥—ç´¯ç§¯å‹ç­”æ¡ˆ"""
        try:
            # è·å–æ‰€æœ‰å±‚çº§ï¼Œä»æ·±åˆ°æµ…æ’åº
            layers = sorted(answers_by_layer.keys(), reverse=True)
            
            # å¼€å§‹æ„å»ºåµŒå¥—ç­”æ¡ˆç»“æ„
            nested_parts = []
            
            for layer in layers:
                answers = answers_by_layer[layer]
                if answers:
                    if len(answers) == 1:
                        nested_parts.append(answers[0])
                    else:
                        # å¤šä¸ªç­”æ¡ˆç”¨æ‹¬å·ç»„åˆ
                        combined = f"({', '.join(answers)})"
                        nested_parts.append(combined)
            
            if not nested_parts:
                return "(æ— æœ‰æ•ˆç­”æ¡ˆ)"
            
            # æ„å»ºæœ€ç»ˆçš„åµŒå¥—ç»“æ„
            if len(nested_parts) == 1:
                return nested_parts[0]
            elif len(nested_parts) == 2:
                return f"({nested_parts[1]}, {nested_parts[0]})"
            else:
                # 3å±‚æˆ–æ›´å¤š
                deepest = nested_parts[0]
                middle = nested_parts[1]
                root = nested_parts[2] if len(nested_parts) > 2 else ""
                
                if root:
                    return f"({root}, ({middle}, {deepest}))"
                else:
                    return f"({middle}, {deepest})"
                    
        except Exception as e:
            print(f"âŒ åµŒå¥—ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return "(åµŒå¥—ç­”æ¡ˆç”Ÿæˆå¤±è´¥)"

    def _generate_llm_integrated_question(self, queries_by_layer: Dict[int, List[str]]) -> str:
        """ç”ŸæˆLLMæ•´åˆå‹é—®é¢˜"""
        if not self.openai_client:
            # ä¸ä½¿ç”¨LLMçš„å¤‡é€‰æ–¹æ¡ˆ
            all_queries = []
            for layer in sorted(queries_by_layer.keys()):
                all_queries.extend(queries_by_layer[layer])
            return f"æ•´åˆé—®é¢˜: {'; '.join(all_queries)}"
        
        try:
            # å°†æ‰€æœ‰å±‚çº§çš„é—®é¢˜æ”¶é›†èµ·æ¥ï¼ŒæŒ‰å±‚çº§æ’åˆ—
            structured_queries = []
            for layer in sorted(queries_by_layer.keys()):
                queries = queries_by_layer[layer]
                if queries:
                    structured_queries.append(f"Layer {layer}: {'; '.join(queries)}")
            
            context = "\n".join(structured_queries)
            
            """è°ƒç”¨OpenAI GPT-3.5ç”Ÿæˆè‡ªç„¶è¯­è¨€æ•´åˆé—®é¢˜"""
            integration_prompt = f"""**TASK: Create a natural language reasoning question that integrates multiple sub-questions while maintaining their logical order.**

Available sub-questions by layer:
{context}

**REQUIREMENTS:**
1. Create ONE comprehensive question that logically combines all sub-questions
2. Maintain the reasoning flow from different layers
3. Make it readable and coherent in natural language
4. Keep the logical dependency structure

**OUTPUT:** A single natural language question."""

            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are an expert at creating natural language reasoning questions that integrate multiple sub-questions while maintaining logical flow."},
                    {"role": "user", "content": integration_prompt}
                ],
                temperature=0.4,
                max_tokens=300,
                timeout=30
            )
            
            generated_question = response.choices[0].message.content.strip()
            
            # æ¸…ç†å’ŒéªŒè¯ç”Ÿæˆçš„é—®é¢˜
            if generated_question and len(generated_question) > 10:
                return generated_question
            else:
                # é™çº§å¤„ç†
                return self._generate_fallback_integrated_question(queries_by_layer)
                
        except Exception as e:
            print(f"âŒ LLMæ•´åˆé—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_integrated_question(queries_by_layer)

    def _generate_llm_integrated_answer(self, answers_by_layer: Dict[int, List[str]], root_answer: str) -> str:
        """ç”ŸæˆLLMæ•´åˆå‹ç­”æ¡ˆ"""
        if not self.openai_client:
            # ä¸ä½¿ç”¨LLMçš„å¤‡é€‰æ–¹æ¡ˆ
            all_answers = []
            for layer in sorted(answers_by_layer.keys()):
                all_answers.extend(answers_by_layer[layer])
            return f"æ•´åˆç­”æ¡ˆ: {'; '.join(all_answers)} â†’ {root_answer}"
        
        try:
            # å°†æ‰€æœ‰å±‚çº§çš„ç­”æ¡ˆæ”¶é›†èµ·æ¥
            structured_answers = []
            for layer in sorted(answers_by_layer.keys()):
                answers = answers_by_layer[layer]
                if answers:
                    structured_answers.append(f"Layer {layer}: {'; '.join(answers)}")
            
            context = "\n".join(structured_answers)
            
            """è°ƒç”¨OpenAIç”Ÿæˆè‡ªç„¶è¯­è¨€æ•´åˆç­”æ¡ˆ"""
            answer_prompt = f"""**TASK: Create a natural language integrated answer that shows the reasoning flow.**

Layer answers available:
{context}

Final target answer: {root_answer}

**REQUIREMENTS:**
1. Show how the answers from different layers lead to the final answer
2. Create a logical reasoning chain
3. Keep it concise but clear (under 200 words)
4. End with the final answer: {root_answer}

**OUTPUT:** A natural language answer showing the reasoning chain."""

            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are an expert at creating clear, logical answer explanations."},
                    {"role": "user", "content": answer_prompt}
                ],
                temperature=0.4,
                max_tokens=300,
                timeout=30
            )
            
            generated_answer = response.choices[0].message.content.strip()
            
            if generated_answer and len(generated_answer) > 10:
                return generated_answer
            else:
                return self._generate_fallback_integrated_answer(answers_by_layer, root_answer)
                
        except Exception as e:
            print(f"âŒ LLMæ•´åˆç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_integrated_answer(answers_by_layer, root_answer)

    def _generate_fallback_integrated_question(self, queries_by_layer: Dict[int, List[str]]) -> str:
        """é™çº§ç‰ˆæœ¬çš„æ•´åˆé—®é¢˜ç”Ÿæˆ"""
        all_queries = []
        for layer in sorted(queries_by_layer.keys()):
            all_queries.extend(queries_by_layer[layer])
        return f"Based on these interconnected questions: {'; '.join(all_queries)}, what is the comprehensive answer?"

    def _fallback_generation(self, root_answer: str, debug_info: str) -> Dict[str, str]:
        """é™çº§ç”Ÿæˆæ–¹æ³•"""
        return {
            'nested_cumulative_question': "(æå–å¤±è´¥)",
            'nested_cumulative_answer': root_answer,
            'llm_integrated_question': "(æå–å¤±è´¥)",
            'llm_integrated_answer': root_answer,
            'debug_info': debug_info + " (é™çº§ç”Ÿæˆ)"
        }

    def _parse_trajectories(self, trajectories: List[Dict], doc_id: str) -> List[Dict]:
        """è§£æè½¨è¿¹è®°å½•"""
        parsed_trajectories = []
        for traj in trajectories:
            parsed_trajectories.append({
                'document_id': doc_id,
                'step': traj.get('step', ''),
                'query_id': traj.get('query_id', ''),
                'query_text': traj.get('query_text', ''),
                'answer': traj.get('answer', ''),
                'minimal_keywords': ', '.join(traj.get('minimal_keywords', [])),
                'generation_method': traj.get('generation_method', ''),
                'validation_passed': traj.get('validation_passed', True)
            })
        return parsed_trajectories

    def _write_to_excel(self, data: Dict[str, Any], excel_path: str):
        """å†™å…¥Excelå·¥ä½œè¡¨"""
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Sheet1: æ–‡æ¡£å¤„ç†æ•ˆç‡ç»Ÿè®¡
            self._write_efficiency_stats(data['efficiency_data'], writer)
            
            # Sheet2: æ‰€æœ‰è¿‡ç¨‹ä¸­çš„é—®ç­”å¯¹
            self._write_all_process_qa(data['all_process_qa'], writer)
            
            # Sheet3: æ¨ç†è½¨è¿¹è®°å½•
            self._write_trajectories(data['trajectories'], writer)
            
            # Sheet4: ç³…åˆåçš„ç»¼åˆé—®ç­”ï¼ˆå››åˆ—æ ¼å¼ï¼‰
            self._write_final_composite_qa(data['composite_qa'], writer)

    def _write_efficiency_stats(self, efficiency_data: List[Dict], writer):
        """å†™å…¥æ–‡æ¡£å¤„ç†æ•ˆç‡ç»Ÿè®¡"""
        if not efficiency_data:
            return
        
        df = pd.DataFrame(efficiency_data)
        df.columns = ['æ–‡æ¡£ID', 'å¤„ç†æ—¶é—´(ç§’)', 'ç”Ÿæˆæ¨ç†æ ‘æ•°', 'æœ‰æ•ˆåµŒå¥—é—®é¢˜æ•°', 'æœ‰æ•ˆLLMé—®é¢˜æ•°', 'æ€»æœ‰æ•ˆé—®é¢˜æ•°', 'å¤„ç†æˆåŠŸ']
        
        df.to_excel(writer, sheet_name='Sheet1-æ–‡æ¡£å¤„ç†æ•ˆç‡ç»Ÿè®¡', index=False)

    def _write_all_process_qa(self, all_process_qa: List[Dict], writer):
        """å†™å…¥æ‰€æœ‰è¿‡ç¨‹ä¸­çš„é—®ç­”å¯¹"""
        if not all_process_qa:
            return
        
        df = pd.DataFrame(all_process_qa)
        df.columns = [
            'æ–‡æ¡£ID', 'æ¨ç†æ ‘ID', 'èŠ‚ç‚¹ID', 'å±‚çº§', 'åˆ†æ”¯ç±»å‹',
            'é—®é¢˜', 'ç­”æ¡ˆ', 'ç”Ÿæˆæ–¹æ³•', 'éªŒè¯é€šè¿‡'
        ]
        
        df.to_excel(writer, sheet_name='Sheet2-æ‰€æœ‰è¿‡ç¨‹ä¸­çš„é—®ç­”å¯¹', index=False)

    def _write_trajectories(self, trajectories: List[Dict], writer):
        """å†™å…¥æ¨ç†è½¨è¿¹è®°å½•"""
        if not trajectories:
            return
        
        df = pd.DataFrame(trajectories)
        df.columns = [
            'æ–‡æ¡£ID', 'æ­¥éª¤', 'æŸ¥è¯¢ID', 'æŸ¥è¯¢æ–‡æœ¬', 'ç­”æ¡ˆ',
            'æœ€å°å…³é”®è¯', 'ç”Ÿæˆæ–¹æ³•', 'éªŒè¯é€šè¿‡'
        ]
        
        df.to_excel(writer, sheet_name='Sheet3-æ¨ç†è½¨è¿¹è®°å½•', index=False)

    def _write_final_composite_qa(self, composite_qa: List[Dict], writer):
        """å†™å…¥ç³…åˆåçš„ç»¼åˆé—®ç­”ï¼ˆå››åˆ—æ ¼å¼ï¼‰"""
        if not composite_qa:
            return
        
        # å‡†å¤‡æ•°æ®
        export_data = []
        for idx, comp in enumerate(composite_qa):
            nested_status = "âœ…" if 'âŒ' not in comp['nested_question'] else "âŒ"
            llm_status = "âœ…" if 'âŒ' not in comp['natural_question'] else "âŒ"
            
            export_data.append({
                'åºå·': idx + 1,
                'æ–‡æ¡£ID': comp['doc_id'],
                'æ¨ç†æ ‘ID': comp['tree_id'],
                'åµŒå¥—é—®é¢˜': comp['nested_question'],
                'è‡ªç„¶é—®é¢˜': comp['natural_question'],
                'åµŒå¥—ç­”æ¡ˆ': comp['nested_answer'],
                'è‡ªç„¶ç­”æ¡ˆ': comp['natural_answer'],
                'æœ€ç»ˆç­”æ¡ˆ': comp['target_answer'],
                'åµŒå¥—çŠ¶æ€': nested_status,
                'è‡ªç„¶çŠ¶æ€': llm_status,
                'è°ƒè¯•ä¿¡æ¯': comp.get('debug_info', ''),
                'æ ‘ç´¢å¼•': comp['tree_index']
            })
        
        df = pd.DataFrame(export_data)
        df.to_excel(writer, sheet_name='Sheet4-ç³…åˆåçš„ç»¼åˆé—®ç­”', index=False)


def main():
    """ä¸»å‡½æ•° - å¤„ç†æŒ‡å®šçš„æ—§æ ¼å¼JSONæ–‡ä»¶"""
    exporter = LegacyJsonExporter()
    
    # å¤„ç†æŒ‡å®šçš„JSONæ–‡ä»¶
    json_file = "agent_reasoning_production_en0028_20250722_122412.json"
    
    if Path(json_file).exists():
        excel_path = exporter.export_to_excel(json_file)
        print(f"ğŸ‰ å¤„ç†å®Œæˆï¼ŒExcelæ–‡ä»¶å·²ç”Ÿæˆ: {excel_path}")
    else:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")

if __name__ == "__main__":
    main() 