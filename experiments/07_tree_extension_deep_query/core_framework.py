#!/usr/bin/env python3
"""
Agentæ·±åº¦æ¨ç†æµ‹è¯•æ¡†æ¶ (Agent Depth Reasoning Test Framework)
åŸºäºæ–°çš„6æ­¥è®¾è®¡ï¼Œä¸ºæ™ºèƒ½Agentæ„å»ºæ·±åº¦æ¨ç†æµ‹è¯•é¢˜

æ ¸å¿ƒç†å¿µï¼š
- ä¸ºæ™ºèƒ½Agentå‡ºé¢˜ï¼Œæµ‹è¯•å…¶æ·±åº¦æ¨ç†èƒ½åŠ›
- é˜²æ­¢æ™®é€šLLMç›´æ¥è·å–ç­”æ¡ˆ
- é€šè¿‡å¤šå±‚çº§é—®é¢˜æ ‘è®­ç»ƒAgenté€æ­¥æ¨ç†
- æœ€ç»ˆç”ŸæˆåµŒå¥—å¼ç»¼åˆé—®é¢˜

è®¾è®¡æµç¨‹ï¼š
Step1: æå–3ä¸ªShort Answerï¼Œæ„å»ºæœ€å°ç²¾ç¡®é—®é¢˜
Step2: æå–Root Queryçš„æœ€å°å…³é”®è¯
Step3: é’ˆå¯¹æ¯ä¸ªå…³é”®è¯åšSeriesæ·±åº¦æ‰©å±•
Step4: é’ˆå¯¹æ‰€æœ‰å…³é”®è¯åšParallelæ¨ªå‘æ‰©å±•  
Step5: é‡å¤æ„å»ºæœ€å¤š3å±‚é—®é¢˜æ ‘
Step6: ç³…åˆæ‰€æœ‰å±‚çº§ç”Ÿæˆæœ€ç»ˆç»¼åˆé—®é¢˜
"""

import logging
import json
import time
import re
from typing import List, Dict, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from pathlib import Path
import uuid

# å¯¼å…¥å¾ªç¯é—®é¢˜å¤„ç†å™¨å’Œå¹¶è¡ŒéªŒè¯å™¨
from utils.circular_problem_handler import CircularProblemHandler
from utils.parallel_keyword_validator import create_parallel_validator

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class ShortAnswer:
    """çŸ­ç­”æ¡ˆæ•°æ®ç»“æ„"""
    answer_text: str
    answer_type: str  # noun, number, name, date, location
    confidence: float
    extraction_source: str
    document_position: int
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'answer_text': self.answer_text,
            'answer_type': self.answer_type,
            'confidence': self.confidence,
            'extraction_source': self.extraction_source,
            'document_position': self.document_position
        }

@dataclass
class MinimalKeyword:
    """æœ€å°ç²¾ç¡®å…³é”®è¯"""
    keyword: str
    keyword_type: str  # proper_noun, number, technical_term, date
    uniqueness_score: float  # å”¯ä¸€æ€§åˆ†æ•°
    necessity_score: float   # å¿…è¦æ€§åˆ†æ•°ï¼ˆç§»é™¤åæ˜¯å¦è¿˜èƒ½ç¡®å®šç­”æ¡ˆï¼‰
    extraction_context: str
    position_in_query: int
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'keyword': self.keyword,
            'keyword_type': self.keyword_type,
            'uniqueness_score': self.uniqueness_score,
            'necessity_score': self.necessity_score,
            'extraction_context': self.extraction_context,
            'position_in_query': self.position_in_query
        }

@dataclass
class PreciseQuery:
    """ç²¾ç¡®é—®é¢˜ç»“æ„"""
    query_id: str
    query_text: str
    answer: str
    minimal_keywords: List[MinimalKeyword]
    generation_method: str  # web_search, llm_analysis
    validation_passed: bool
    layer_level: int  # 0=root, 1=first_extension, 2=second_extension
    parent_query_id: Optional[str] = None
    extension_type: str = "root"  # root, series, parallel
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'query_id': self.query_id,
            'query_text': self.query_text,
            'answer': self.answer,
            'minimal_keywords': [kw.to_dict() for kw in self.minimal_keywords],
            'generation_method': self.generation_method,
            'validation_passed': self.validation_passed,
            'layer_level': self.layer_level,
            'parent_query_id': self.parent_query_id,
            'extension_type': self.extension_type
        }

@dataclass
class QuestionTreeNode:
    """é—®é¢˜æ ‘èŠ‚ç‚¹"""
    node_id: str
    query: PreciseQuery
    parent_id: Optional[str]
    children_ids: List[str] = field(default_factory=list)
    layer: int = 0
    branch_type: str = "root"  # root, series, parallel
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'node_id': self.node_id,
            'query': self.query.to_dict(),
            'parent_id': self.parent_id,
            'children_ids': self.children_ids,
            'layer': self.layer,
            'branch_type': self.branch_type
        }

@dataclass
class AgentReasoningTree:
    """Agentæ¨ç†æµ‹è¯•æ ‘"""
    tree_id: str
    root_node: QuestionTreeNode
    all_nodes: Dict[str, QuestionTreeNode] = field(default_factory=dict)
    max_layers: int = 3
    final_composite_query: Any = ""  # æ”¯æŒå­—ç¬¦ä¸²æˆ–å­—å…¸æ ¼å¼
    trajectory_records: List[Dict] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
        return {
            'tree_id': self.tree_id,
            'root_node': self.root_node.to_dict(),
            'all_nodes': {k: v.to_dict() for k, v in self.all_nodes.items()},
            'max_layers': self.max_layers,
            'final_composite_query': self.final_composite_query,
            'trajectory_records': self.trajectory_records
        }

class AgentDepthReasoningFramework:
    """Agentæ·±åº¦æ¨ç†æµ‹è¯•æ¡†æ¶ä¸»ç±»"""
    
    def __init__(self, api_client=None, search_client=None):
        self.api_client = api_client
        self.search_client = search_client
        self.max_short_answers = 3
        self.max_tree_layers = 3
        self.trajectory_records = []
        
        # åˆå§‹åŒ–å¾ªç¯é—®é¢˜å¤„ç†å™¨
        self.circular_handler = CircularProblemHandler()
        
        # åˆå§‹åŒ–å¹¶è¡Œå…³é”®è¯éªŒè¯å™¨
        self.parallel_validator = create_parallel_validator(api_client, max_workers=3) if api_client else None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'documents_processed': 0,
            'short_answers_extracted': 0,
            'root_queries_generated': 0,
            'minimal_keywords_found': 0,
            'series_extensions_created': 0,
            'parallel_extensions_created': 0,
            'final_composite_queries': 0,
            'total_reasoning_trees': 0
        }
    
    def set_api_client(self, api_client):
        """è®¾ç½®APIå®¢æˆ·ç«¯"""
        self.api_client = api_client
    
    def set_search_client(self, search_client):
        """è®¾ç½®æœç´¢å®¢æˆ·ç«¯"""
        self.search_client = search_client
    
    def process_document_for_agent_reasoning(self, document_content: str, document_id: str) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼Œç”ŸæˆAgentæ¨ç†æµ‹è¯•æ•°æ®
        
        Args:
            document_content: æ–‡æ¡£å†…å®¹
            document_id: æ–‡æ¡£ID
            
        Returns:
            å®Œæ•´çš„Agentæ¨ç†æµ‹è¯•ç»“æœ
        """
        logger.info(f"ğŸ¯ å¼€å§‹ä¸ºAgentç”Ÿæˆæ·±åº¦æ¨ç†æµ‹è¯•é¢˜: {document_id}")
        start_time = time.time()
        
        try:
            # Step 1: æå–Short Answerå¹¶æ„å»ºæœ€å°ç²¾ç¡®é—®é¢˜
            logger.info("ğŸ“ Step 1: æå–Short Answerå¹¶æ„å»ºRoot Query")
            root_queries = self._step1_extract_short_answers_and_build_root_queries(
                document_content, document_id
            )
            
            if not root_queries:
                return self._create_error_result(document_id, "Step 1 failed: No root queries generated")
            
            # ä¸ºæ¯ä¸ªRoot Queryæ„å»ºæ¨ç†æ ‘
            reasoning_trees = []
            for i, root_query in enumerate(root_queries):
                logger.info(f"ğŸŒ³ æ„å»ºæ¨ç†æ ‘ {i+1}/{len(root_queries)}")
                
                # Step 2: æå–Root Queryçš„æœ€å°å…³é”®è¯
                logger.info("ğŸ“ Step 2: æå–Root Queryæœ€å°å…³é”®è¯")
                minimal_keywords = self._step2_extract_minimal_keywords(root_query)
                
                if not minimal_keywords:
                    logger.warning(f"è·³è¿‡Root Query {root_query.query_id}: æ— æ³•æå–æœ€å°å…³é”®è¯")
                    continue
                
                # æ ¹æ®å…³é”®è¯æ•°é‡å†³å®šæ ‘ç»“æ„
                if len(minimal_keywords) == 1:
                    logger.info("ğŸ”— å•å…³é”®è¯æ¨¡å¼: æ„å»º2å±‚Seriesæ ‘")
                    tree = self._build_single_keyword_tree(root_query, minimal_keywords[0])
                else:
                    logger.info(f"ğŸŒ å¤šå…³é”®è¯æ¨¡å¼: æ„å»º3å±‚Series+Parallelæ ‘ ({len(minimal_keywords)}ä¸ªå…³é”®è¯)")
                    tree = self._build_multi_keyword_tree(root_query, minimal_keywords)
                
                if tree:
                    # Step 6: ç”Ÿæˆæœ€ç»ˆç»¼åˆé—®é¢˜
                    logger.info("ğŸ“ Step 6: ç”Ÿæˆæœ€ç»ˆç»¼åˆé—®é¢˜")
                    composite_queries = self._step6_generate_composite_query(tree)
                    tree.final_composite_query = composite_queries  # ç°åœ¨æ˜¯å­—å…¸æ ¼å¼
                    reasoning_trees.append(tree.to_dict())  # è½¬æ¢ä¸ºå­—å…¸å­˜å‚¨
                    self.stats['total_reasoning_trees'] += 1
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            self.stats['documents_processed'] += 1
            
            # è®°å½•å¾ªç¯å¤„ç†å™¨ç»Ÿè®¡
            self.circular_handler.log_statistics()
            
            result = self._create_success_result(
                document_id, reasoning_trees, processing_time
            )
            
            # æ·»åŠ å¾ªç¯å¤„ç†å™¨ç»Ÿè®¡åˆ°ç»“æœä¸­
            result['circular_handler_stats'] = self.circular_handler.get_statistics()
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {document_id}: {e}")
            return self._create_error_result(document_id, str(e))
    
    def _step1_extract_short_answers_and_build_root_queries(
        self, document_content: str, document_id: str
    ) -> List[PreciseQuery]:
        """
        Step 1: æå–Short Answerå¹¶æ„å»ºæœ€å°ç²¾ç¡®é—®é¢˜
        
        è¦æ±‚ï¼š
        - æœ€å¤š3ä¸ªæ˜ç¡®ä¸”å”¯ä¸€çš„Short Answer
        - ä¸ºæ¯ä¸ªShort Answeræ„å»ºæ˜ç¡®é—®é¢˜ï¼ˆä½¿ç”¨Webæœç´¢+LLMï¼‰
        - é—®é¢˜åŒ…å«è‡³å°‘ä¸¤ä¸ªå…³é”®è¯ç¡®ä¿ç­”æ¡ˆå”¯ä¸€
        - ç§»é™¤éå¿…è¦å…³é”®è¯ï¼Œä¿ç•™æœ€å°ç²¾ç¡®é—®é¢˜
        """
        if not self.api_client:
            return []
        
        try:
            # 1.1 æå–Short Answer
            short_answers = self._extract_unique_short_answers(document_content)
            logger.info(f"æå–åˆ° {len(short_answers)} ä¸ªShort Answer")
            
            if not short_answers:
                return []
            
            # 1.2 ä¸ºæ¯ä¸ªShort Answeræ„å»ºRoot Query
            root_queries = []
            for i, short_answer in enumerate(short_answers[:self.max_short_answers]):
                logger.info(f"ä¸ºShort Answer '{short_answer.answer_text}' æ„å»ºRoot Query")
                
                # ä½¿ç”¨Webæœç´¢å¢å¼ºé—®é¢˜ç”Ÿæˆ
                root_query = self._build_minimal_precise_query(
                    short_answer, document_content, f"{document_id}_root_{i}"
                )
                
                if root_query and root_query.validation_passed:
                    root_queries.append(root_query)
                    self.stats['root_queries_generated'] += 1
                    
                    # è·å–æœç´¢ä¸Šä¸‹æ–‡ç”¨äºè½¨è¿¹è®°å½•
                    search_context = ""
                    if self.search_client:
                        try:
                            # ç›´æ¥è°ƒç”¨search_clientï¼Œwrapperä¼šå¤„ç†API key
                            search_results = self.search_client(f"{short_answer.answer_text} definition characteristics")
                            
                            # åªæœ‰æˆåŠŸè·å–åˆ°çœŸå®æœç´¢ç»“æœæ‰ä½¿ç”¨ï¼Œå¦åˆ™ç•™ç©º
                            if (search_results and 
                                search_results.get('status') == 'success' and 
                                search_results.get('results')):
                                search_context = " ".join([
                                    result.get('content', '')[:200] 
                                    for result in search_results['results'][:2]
                                ])
                        except Exception:
                            search_context = ""
                    
                    # è®°å½•è½¨è¿¹ - ä½¿ç”¨å¢å¼ºçš„è®°å½•åŠŸèƒ½
                    try:
                        self._record_detailed_trajectory_enhanced(
                            'step1_root_query_generation',
                            layer_level=0,
                            keywords=[kw.keyword for kw in root_query.minimal_keywords],
                            current_question=root_query.query_text,
                            current_answer=root_query.answer,
                            query_id=root_query.query_id,
                            extension_type='root',
                            generation_method=root_query.generation_method,
                            validation_results={'validation_passed': root_query.validation_passed},
                            short_answer_info={
                                'answer_text': short_answer.answer_text,
                                'answer_type': short_answer.answer_type,
                                'confidence': short_answer.confidence
                            },
                            web_search_context=search_context,
                            uniqueness_verified=root_query.validation_passed
                        )
                    except Exception as e:
                        logger.warning(f"å¢å¼ºè½¨è¿¹è®°å½•å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰è®°å½•: {e}")
                        # å›é€€åˆ°åŸæœ‰è®°å½•æ–¹æ³•
                        self._record_trajectory({
                            'step': 'step1_root_query_generation',
                            'query_id': root_query.query_id,
                            'query_text': root_query.query_text,
                            'answer': root_query.answer,
                            'minimal_keywords': [kw.keyword for kw in root_query.minimal_keywords],
                            'keyword_count': len(root_query.minimal_keywords),
                            'validation_passed': root_query.validation_passed
                        })
            
            logger.info(f"âœ… Step 1å®Œæˆ: ç”Ÿæˆ {len(root_queries)} ä¸ªRoot Query")
            return root_queries
            
        except Exception as e:
            logger.error(f"Step 1æ‰§è¡Œå¤±è´¥: {e}")
            return []
    
    def _step2_extract_minimal_keywords(self, root_query: PreciseQuery) -> List[MinimalKeyword]:
        """
        Step 2: æå–Root Queryçš„æœ€å°å…³é”®è¯
        
        è¦æ±‚ï¼š
        - æå–èƒ½ç¡®å®šç­”æ¡ˆçš„æœ€å°æ•°é‡å…³é”®è¯
        - æ•°å­—ã€åè¯ç­‰å¯ä½œä¸ºç²¾ç¡®ç­”æ¡ˆçš„å…³é”®è¯
        - éªŒè¯æ¯ä¸ªå…³é”®è¯çš„å¿…è¦æ€§
        """
        if not self.api_client:
            return []
        
        try:
            logger.info(f"åˆ†æRoot Queryçš„æœ€å°å…³é”®è¯: {root_query.query_text}")
            
            # 2.1 åˆæ­¥æå–å…³é”®è¯
            candidate_keywords = self._extract_candidate_keywords(
                root_query.query_text, root_query.answer
            )
            
            # 2.2 éªŒè¯å…³é”®è¯å¿…è¦æ€§ï¼ˆç§»é™¤æµ‹è¯•ï¼‰
            minimal_keywords = self._validate_keyword_necessity(
                root_query.query_text, root_query.answer, candidate_keywords
            )
            
            # 2.3 å…³é”®è¯å”¯ä¸€æ€§è¯„åˆ†
            for keyword in minimal_keywords:
                keyword.uniqueness_score = self._calculate_keyword_uniqueness(
                    keyword.keyword, root_query.answer
                )
            
            logger.info(f"âœ… æå–åˆ° {len(minimal_keywords)} ä¸ªæœ€å°å…³é”®è¯: {[kw.keyword for kw in minimal_keywords]}")
            self.stats['minimal_keywords_found'] += len(minimal_keywords)
            
            # åªæœ‰å½“æœ‰å…³é”®è¯æ—¶æ‰è®°å½•è½¨è¿¹
            if minimal_keywords:
                self._record_trajectory({
                    'step': 'step2_minimal_keywords',
                    'root_query_id': root_query.query_id,
                    'minimal_keywords': [{
                        'keyword': kw.keyword,
                        'type': kw.keyword_type,
                        'uniqueness_score': kw.uniqueness_score,
                        'necessity_score': kw.necessity_score
                    } for kw in minimal_keywords],
                    'total_count': len(minimal_keywords)
                })
            else:
                # è®°å½•å…³é”®è¯æå–å¤±è´¥çš„æƒ…å†µ
                self._record_trajectory({
                    'step': 'step2_minimal_keywords_failed',
                    'root_query_id': root_query.query_id,
                    'error': 'No minimal keywords extracted',
                    'total_count': 0
                })
            
            return minimal_keywords
            
        except Exception as e:
            logger.error(f"Step 2æ‰§è¡Œå¤±è´¥: {e}")
            return []
    
    def _build_single_keyword_tree(
        self, root_query: PreciseQuery, keyword: MinimalKeyword
    ) -> Optional[AgentReasoningTree]:
        """
        æ„å»ºå•å…³é”®è¯æ¨ç†æ ‘ï¼ˆ2å±‚Seriesï¼‰
        
        ç»“æ„:
        Root â†’ Series1 â†’ Series2
        """
        try:
            tree_id = f"single_{root_query.query_id}_{int(time.time())}"
            
            # åˆ›å»ºæ ¹èŠ‚ç‚¹
            root_node = QuestionTreeNode(
                node_id=f"{tree_id}_root",
                query=root_query,
                parent_id=None,
                layer=0,
                branch_type="root"
            )
            
            # åˆ›å»ºæ¨ç†æ ‘
            tree = AgentReasoningTree(
                tree_id=tree_id,
                root_node=root_node,
                max_layers=2  # å•å…³é”®è¯åªéœ€2å±‚
            )
            tree.all_nodes[root_node.node_id] = root_node
            
            # Step 3: ç¬¬ä¸€å±‚Seriesæ‰©å±•
            series1_query = self._step3_create_series_extension(
                root_query, keyword, layer=1, tree_id=tree_id
            )
            
            if series1_query:
                series1_node = QuestionTreeNode(
                    node_id=f"{tree_id}_series1",
                    query=series1_query,
                    parent_id=root_node.node_id,
                    layer=1,
                    branch_type="series"
                )
                tree.all_nodes[series1_node.node_id] = series1_node
                root_node.children_ids.append(series1_node.node_id)
                
                # ç¬¬äºŒå±‚Seriesæ‰©å±•
                series1_keywords = self._step2_extract_minimal_keywords(series1_query)
                if series1_keywords:
                    series2_query = self._step3_create_series_extension(
                        series1_query, series1_keywords[0], layer=2, tree_id=tree_id
                    )
                    
                    if series2_query:
                        series2_node = QuestionTreeNode(
                            node_id=f"{tree_id}_series2",
                            query=series2_query,
                            parent_id=series1_node.node_id,
                            layer=2,
                            branch_type="series"
                        )
                        tree.all_nodes[series2_node.node_id] = series2_node
                        series1_node.children_ids.append(series2_node.node_id)
            
            logger.info(f"âœ… å•å…³é”®è¯æ¨ç†æ ‘æ„å»ºå®Œæˆ: {len(tree.all_nodes)} ä¸ªèŠ‚ç‚¹")
            return tree
            
        except Exception as e:
            logger.error(f"å•å…³é”®è¯æ ‘æ„å»ºå¤±è´¥: {e}")
            return None
    
    def _build_multi_keyword_tree(
        self, root_query: PreciseQuery, keywords: List[MinimalKeyword]
    ) -> Optional[AgentReasoningTree]:
        """
        æ„å»ºå¤šå…³é”®è¯æ¨ç†æ ‘ï¼ˆ3å±‚Series+Parallelï¼‰
        
        ç»“æ„:
        Root â†’ Series1/Parallel1 â†’ Series2/Parallel2
        """
        try:
            tree_id = f"multi_{root_query.query_id}_{int(time.time())}"
            
            # åˆ›å»ºæ ¹èŠ‚ç‚¹
            root_node = QuestionTreeNode(
                node_id=f"{tree_id}_root",
                query=root_query,
                parent_id=None,
                layer=0,
                branch_type="root"
            )
            
            # åˆ›å»ºæ¨ç†æ ‘
            tree = AgentReasoningTree(
                tree_id=tree_id,
                root_node=root_node,
                max_layers=3
            )
            tree.all_nodes[root_node.node_id] = root_node
            
            # Step 3: é’ˆå¯¹æ¯ä¸ªå…³é”®è¯åˆ›å»ºSeriesæ‰©å±•
            # Step 4: é’ˆå¯¹æ‰€æœ‰å…³é”®è¯åˆ›å»ºParallelæ‰©å±•
            first_layer_nodes = []
            
            # Seriesæ‰©å±•ï¼ˆé€‰æ‹©ç¬¬ä¸€ä¸ªå…³é”®è¯ï¼‰
            series1_query = self._step3_create_series_extension(
                root_query, keywords[0], layer=1, tree_id=tree_id
            )
            if series1_query:
                series1_node = QuestionTreeNode(
                    node_id=f"{tree_id}_series1",
                    query=series1_query,
                    parent_id=root_node.node_id,
                    layer=1,
                    branch_type="series"
                )
                tree.all_nodes[series1_node.node_id] = series1_node
                root_node.children_ids.append(series1_node.node_id)
                first_layer_nodes.append(series1_node)
                self.stats['series_extensions_created'] += 1
            
            # Parallelæ‰©å±•ï¼ˆæ‰€æœ‰å…³é”®è¯ï¼‰
            parallel_queries = self._step4_create_parallel_extensions(
                root_query, keywords, layer=1, tree_id=tree_id
            )
            for i, parallel_query in enumerate(parallel_queries):
                parallel_node = QuestionTreeNode(
                    node_id=f"{tree_id}_parallel1_{i}",
                    query=parallel_query,
                    parent_id=root_node.node_id,
                    layer=1,
                    branch_type="parallel"
                )
                tree.all_nodes[parallel_node.node_id] = parallel_node
                root_node.children_ids.append(parallel_node.node_id)
                first_layer_nodes.append(parallel_node)
                self.stats['parallel_extensions_created'] += 1
            
            # Step 5: é‡å¤è¿‡ç¨‹æ„å»ºç¬¬äºŒå±‚
            for node in first_layer_nodes:
                self._build_second_layer_extensions(tree, node)
            
            logger.info(f"âœ… å¤šå…³é”®è¯æ¨ç†æ ‘æ„å»ºå®Œæˆ: {len(tree.all_nodes)} ä¸ªèŠ‚ç‚¹")
            return tree
            
        except Exception as e:
            logger.error(f"å¤šå…³é”®è¯æ ‘æ„å»ºå¤±è´¥: {e}")
            return None
    
    def _step3_create_series_extension(
        self, parent_query: PreciseQuery, keyword: MinimalKeyword, 
        layer: int, tree_id: str
    ) -> Optional[PreciseQuery]:
        """
        Step 3: åˆ›å»ºSeriesæ·±åº¦æ‰©å±•
        
        è¦æ±‚ï¼š
        - é’ˆå¯¹å•ä¸ªå…³é”®è¯ç”Ÿæˆæ–°é—®é¢˜
        - ä½¿ç”¨Webæœç´¢+LLMåˆ†æ
        - æ–°é—®é¢˜ä¸èƒ½ä¸Rooté—®é¢˜æœ‰ä»»ä½•å…³è”
        """
        try:
            logger.info(f"åˆ›å»ºSeriesæ‰©å±• (Layer {layer}): å…³é”®è¯ '{keyword.keyword}'")
            
            # ä½¿ç”¨æ™ºèƒ½Webæœç´¢è·å–å…³é”®è¯ç›¸å…³ä¿¡æ¯ (é›†æˆå¾ªç¯æ£€æµ‹)
            search_context = self._smart_web_search_for_keyword(
                keyword.keyword, parent_query.query_text, parent_query.answer
            )
            
            # ç”Ÿæˆæ— å…³è”çš„æ–°é—®é¢˜
            extension_query = self._generate_unrelated_query(
                keyword, search_context, layer, f"{tree_id}_series_{layer}"
            )
            
            if extension_query:
                # éªŒè¯æ— å…³è”æ€§ - ä½¿ç”¨å¢å¼ºçš„ä¸¥æ ¼éªŒè¯
                validation_passed = False
                try:
                    # ä¼˜å…ˆä½¿ç”¨æ–°çš„ä¸¥æ ¼æ— å…³è”éªŒè¯
                    parent_questions = [parent_query.query_text]
                    if self._validate_strict_no_correlation(parent_questions, extension_query.query_text, layer):
                        logger.info(f"âœ… Seriesæ‰©å±•æˆåŠŸ (ä¸¥æ ¼éªŒè¯é€šè¿‡): {extension_query.query_text}")
                        validation_passed = True
                    else:
                        logger.warning("Seriesæ‰©å±•å¤±è´¥: ä¸¥æ ¼éªŒè¯æœªé€šè¿‡ - å­˜åœ¨å…³è”")
                except Exception as e:
                    logger.warning(f"ä¸¥æ ¼éªŒè¯å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰éªŒè¯: {e}")
                    # å›é€€åˆ°åŸæœ‰éªŒè¯æ–¹æ³•
                    if self._validate_no_correlation(parent_query.query_text, extension_query.query_text):
                        logger.info(f"âœ… Seriesæ‰©å±•æˆåŠŸ (åŸæœ‰éªŒè¯é€šè¿‡): {extension_query.query_text}")
                        validation_passed = True
                    else:
                        logger.warning("Seriesæ‰©å±•å¤±è´¥: ä¸çˆ¶é—®é¢˜å­˜åœ¨å…³è”")
                
                # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦ä¼šæš´éœ²æ ¹ç­”æ¡ˆ
                root_answer = self._extract_root_answer_from_tree_id(tree_id)
                if root_answer and validation_passed:
                    exposure_safe = self._validate_no_root_answer_exposure(
                        extension_query.query_text, root_answer, layer
                    )
                    if not exposure_safe:
                        logger.warning(f"Seriesæ‰©å±•é—®é¢˜å¯èƒ½æš´éœ²æ ¹ç­”æ¡ˆï¼Œéœ€è¦é‡æ–°è®¾è®¡: {extension_query.query_text}")
                        validation_passed = False
                
                # è®°å½•Seriesæ‰©å±•è½¨è¿¹
                try:
                    self._record_detailed_trajectory_enhanced(
                        step=f"step3_series_extension_layer_{layer}",
                        layer_level=layer,
                        current_keywords=[keyword.keyword],
                        keyword_count=1,
                        parent_question=parent_query.query_text,
                        parent_answer=parent_query.answer,
                        parent_keywords=[kw.keyword for kw in parent_query.minimal_keywords],
                        current_question=extension_query.query_text,
                        current_answer=extension_query.answer,
                        generation_method=extension_query.generation_method,
                        validation_results={'validation_passed': validation_passed},
                        no_correlation_verified=validation_passed,
                        tree_id=tree_id,
                        query_id=extension_query.query_id,
                        extension_type="series"
                    )
                except Exception as e:
                    logger.error(f"è®°å½•Seriesæ‰©å±•è½¨è¿¹å¤±è´¥: {e}")
                
                if validation_passed:
                    return extension_query
                else:
                    return None
            
            return None
            
        except Exception as e:
            logger.error(f"Step 3æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def _step4_create_parallel_extensions(
        self, root_query: PreciseQuery, keywords: List[MinimalKeyword], 
        layer: int, tree_id: str
    ) -> List[PreciseQuery]:
        """
        Step 4: åˆ›å»ºParallelæ¨ªå‘æ‰©å±•
        
        è¦æ±‚ï¼š
        - é’ˆå¯¹æ‰€æœ‰å…³é”®è¯åˆ†åˆ«ç”Ÿæˆé—®é¢˜
        - æ¯ä¸ªé—®é¢˜éƒ½ä¸èƒ½ä¸Rooté—®é¢˜å…³è”
        - ç”Ÿæˆnä¸ªä¸åŒçš„æœ€ç²¾ç¡®é—®é¢˜
        """
        try:
            logger.info(f"åˆ›å»ºParallelæ‰©å±• (Layer {layer}): {len(keywords)} ä¸ªå…³é”®è¯")
            
            parallel_queries = []
            for i, keyword in enumerate(keywords):
                # ä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆç‹¬ç«‹é—®é¢˜ (é›†æˆå¾ªç¯æ£€æµ‹)
                search_context = self._smart_web_search_for_keyword(
                    keyword.keyword, root_query.query_text, root_query.answer
                )
                
                extension_query = self._generate_unrelated_query(
                    keyword, search_context, layer, f"{tree_id}_parallel_{layer}_{i}"
                )
                
                if extension_query:
                    # éªŒè¯ä¸Rooté—®é¢˜æ— å…³è” - ä½¿ç”¨å¢å¼ºçš„ä¸¥æ ¼éªŒè¯
                    validation_passed = False
                    try:
                        # ä¼˜å…ˆä½¿ç”¨æ–°çš„ä¸¥æ ¼æ— å…³è”éªŒè¯
                        parent_questions = [root_query.query_text]
                        if self._validate_strict_no_correlation(parent_questions, extension_query.query_text, layer):
                            parallel_queries.append(extension_query)
                            logger.info(f"âœ… Parallelæ‰©å±• {i+1} (ä¸¥æ ¼éªŒè¯é€šè¿‡): {extension_query.query_text}")
                            validation_passed = True
                        else:
                            logger.warning(f"Parallelæ‰©å±• {i+1} å¤±è´¥: ä¸¥æ ¼éªŒè¯æœªé€šè¿‡ - å­˜åœ¨å…³è”")
                    except Exception as e:
                        logger.warning(f"Parallelæ‰©å±• {i+1} ä¸¥æ ¼éªŒè¯å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰éªŒè¯: {e}")
                        # å›é€€åˆ°åŸæœ‰éªŒè¯æ–¹æ³•
                        if self._validate_no_correlation(root_query.query_text, extension_query.query_text):
                            parallel_queries.append(extension_query)
                            logger.info(f"âœ… Parallelæ‰©å±• {i+1} (åŸæœ‰éªŒè¯é€šè¿‡): {extension_query.query_text}")
                            validation_passed = True
                        else:
                            logger.warning(f"Parallelæ‰©å±• {i+1} å¤±è´¥: ä¸Rooté—®é¢˜å­˜åœ¨å…³è”")
                    
                    # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦ä¼šæš´éœ²æ ¹ç­”æ¡ˆ
                    if validation_passed:
                        exposure_safe = self._validate_no_root_answer_exposure(
                            extension_query.query_text, root_query.answer, layer
                        )
                        if not exposure_safe:
                            logger.warning(f"Parallelæ‰©å±•é—®é¢˜å¯èƒ½æš´éœ²æ ¹ç­”æ¡ˆï¼Œéœ€è¦é‡æ–°è®¾è®¡: {extension_query.query_text}")
                            validation_passed = False
                            # ä»å·²æ·»åŠ çš„åˆ—è¡¨ä¸­ç§»é™¤è¿™ä¸ªé—®é¢˜
                            if extension_query in parallel_queries:
                                parallel_queries.remove(extension_query)
                    
                    # è®°å½•Parallelæ‰©å±•è½¨è¿¹
                    try:
                        self._record_detailed_trajectory_enhanced(
                            step=f"step4_parallel_extension_layer_{layer}_{i}",
                            layer_level=layer,
                            current_keywords=[keyword.keyword],
                            keyword_count=1,
                            parent_question=root_query.query_text,
                            parent_answer=root_query.answer,
                            parent_keywords=[kw.keyword for kw in root_query.minimal_keywords],
                            current_question=extension_query.query_text,
                            current_answer=extension_query.answer,
                            generation_method=extension_query.generation_method,
                            validation_results={'validation_passed': validation_passed},
                            no_correlation_verified=validation_passed,
                            tree_id=tree_id,
                            query_id=extension_query.query_id,
                            extension_type="parallel",
                            parallel_index=i
                        )
                    except Exception as e:
                        logger.error(f"è®°å½•Parallelæ‰©å±•è½¨è¿¹å¤±è´¥: {e}")
            
            logger.info(f"âœ… å®ŒæˆParallelæ‰©å±•: {len(parallel_queries)}/{len(keywords)} ä¸ªé—®é¢˜")
            return parallel_queries
            
        except Exception as e:
            logger.error(f"Step 4æ‰§è¡Œå¤±è´¥: {e}")
            return []
    
    def _step6_generate_composite_query(self, tree: AgentReasoningTree) -> Dict[str, str]:
        """
        Step 6: ç”Ÿæˆæœ€ç»ˆç»¼åˆé—®é¢˜å’Œç­”æ¡ˆ - åŒæ ¼å¼è¾“å‡º
        
        ç”Ÿæˆä¸¤ç§ç±»å‹çš„ç³…åˆé—®é¢˜å’Œå¯¹åº”ç­”æ¡ˆï¼š
        1. åµŒå¥—ç´¯ç§¯å‹ï¼šæ— LLMï¼Œçº¯é—®é¢˜ç´¯ç§¯ (Q1, (Q2, Q3...))
        2. LLMæ•´åˆå‹ï¼šLLMå‚ä¸çš„è‡ªç„¶è¯­è¨€æ•´åˆï¼Œä¿æŒé—®é¢˜é¡ºåº
        """
        try:
            logger.info(f"ç”ŸæˆåŒæ ¼å¼æœ€ç»ˆç»¼åˆé—®é¢˜å’Œç­”æ¡ˆ: Tree {tree.tree_id}")
            
            # æ”¶é›†æ‰€æœ‰å±‚çº§çš„é—®é¢˜å’Œç­”æ¡ˆ
            queries_by_layer = {}
            answers_by_layer = {}
            
            for node in tree.all_nodes.values():
                layer = node.layer
                if layer not in queries_by_layer:
                    queries_by_layer[layer] = []
                    answers_by_layer[layer] = []
                queries_by_layer[layer].append(node.query.query_text)
                answers_by_layer[layer].append(node.query.answer)
            
            root_answer = tree.root_node.query.answer
            
            # ç”Ÿæˆä¸¤ç§æ ¼å¼çš„ç»¼åˆé—®é¢˜å’Œç­”æ¡ˆ
            # é—®é¢˜ç”Ÿæˆï¼šå®Œå…¨åŸºäºé—®é¢˜ï¼Œä¸ä¼ é€’ç­”æ¡ˆä¿¡æ¯
            nested_cumulative_question = self._generate_nested_cumulative_query(queries_by_layer, "")
            llm_integrated_question = self._generate_llm_integrated_query(queries_by_layer, "")
            
            # ç­”æ¡ˆç”Ÿæˆï¼šå¯ä»¥ä½¿ç”¨root_answer
            nested_cumulative_answer = self._generate_nested_cumulative_answer(answers_by_layer, root_answer)
            llm_integrated_answer = self._generate_llm_integrated_answer(answers_by_layer, root_answer)
            
            composite_queries = {
                'nested_cumulative': nested_cumulative_question,
                'nested_cumulative_answer': nested_cumulative_answer,
                'llm_integrated': llm_integrated_question,
                'llm_integrated_answer': llm_integrated_answer
            }
            
            logger.info(f"âœ… åŒæ ¼å¼ç»¼åˆé—®é¢˜å’Œç­”æ¡ˆç”ŸæˆæˆåŠŸ")
            self.stats['final_composite_queries'] += 1
            
            # è®°å½•è½¨è¿¹
            self._record_trajectory({
                'step': 'step6_composite_query',
                'tree_id': tree.tree_id,
                'original_root_answer': root_answer,
                'total_layers': max(queries_by_layer.keys()) + 1,
                'queries_per_layer': {str(k): len(v) for k, v in queries_by_layer.items()},
                'nested_cumulative_length': len(nested_cumulative_question),
                'llm_integrated_length': len(llm_integrated_question),
                'complexity_score': self._calculate_complexity_score(llm_integrated_question),
                'composite_queries': composite_queries
            })
            
            # è®°å½•å®Œæ•´æ¨ç†æ ‘ç»“æ„
            self._record_complete_tree_trajectory(tree)
            
            return composite_queries
            
        except Exception as e:
            logger.error(f"Step 6æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'nested_cumulative': f"Multi-step reasoning required for: {tree.root_node.query.answer}",
                'nested_cumulative_answer': tree.root_node.query.answer,
                'llm_integrated': f"Complex reasoning chain needed to determine: {tree.root_node.query.answer}",
                'llm_integrated_answer': tree.root_node.query.answer
            }
    
    def _generate_nested_cumulative_query(self, queries_by_layer: Dict[int, List[str]], root_answer: str = "") -> str:
        """ç”ŸæˆåµŒå¥—ç´¯ç§¯å‹é—®é¢˜ - ç®€å•æ‹¬å·æ‹¼è£…ç»“æ„"""
        try:
            # ä»æœ€æ·±å±‚å‘rooté¡¶å±‚ç´¯ç§¯é—®é¢˜
            layers = sorted(queries_by_layer.keys(), reverse=True)  # ä»æ·±å±‚åˆ°æµ…å±‚
            if not layers:
                return "(Multi-step reasoning required)"
            
            # æ„å»ºåµŒå¥—ç»“æ„ï¼š(Q_deepest, (Q_middle, Q_root))
            nested_query = ""
            for i, layer in enumerate(layers):
                layer_queries = queries_by_layer[layer]
                if not layer_queries:
                    continue
                    
                # æ¯å±‚å–ç¬¬ä¸€ä¸ªé—®é¢˜ä½œä¸ºä»£è¡¨
                current_query = layer_queries[0]
                
                if i == 0:
                    # æœ€æ·±å±‚
                    nested_query = f"({current_query})"
                else:
                    # åµŒå¥—åŒ…è£…
                    nested_query = f"({current_query}, {nested_query})"
            
            logger.info(f"âœ… åµŒå¥—ç´¯ç§¯å‹é—®é¢˜ç”Ÿæˆ: {len(nested_query)} å­—ç¬¦")
            return nested_query
            
        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå¥—ç´¯ç§¯å‹é—®é¢˜å¤±è´¥: {e}")
            return "(Multi-step reasoning required)"
    

    
    def _generate_nested_cumulative_answer(self, answers_by_layer: Dict[int, List[str]], root_answer: str) -> str:
        """ç”ŸæˆåµŒå¥—ç´¯ç§¯å‹ç­”æ¡ˆ - æ— LLMï¼Œçº¯ç­”æ¡ˆç´¯ç§¯"""
        try:
            # ä»æœ€æ·±å±‚å‘rooté¡¶å±‚ç´¯ç§¯ç­”æ¡ˆ
            layers = sorted(answers_by_layer.keys(), reverse=True)  # ä»æ·±å±‚åˆ°æµ…å±‚
            if not layers:
                return f"({root_answer})"
            
            # æ„å»ºåµŒå¥—ç»“æ„ï¼š(A_deepest, (A_middle, A_root))
            nested_answer = ""
            for i, layer in enumerate(layers):
                layer_answers = answers_by_layer[layer]
                if not layer_answers:
                    continue
                    
                # æ¯å±‚å–ç¬¬ä¸€ä¸ªç­”æ¡ˆä½œä¸ºä»£è¡¨
                current_answer = layer_answers[0]
                
                if i == 0:
                    # æœ€æ·±å±‚
                    nested_answer = f"({current_answer})"
                else:
                    # åµŒå¥—åŒ…è£…
                    nested_answer = f"({current_answer}, {nested_answer})"
            
            logger.info(f"âœ… åµŒå¥—ç´¯ç§¯å‹ç­”æ¡ˆç”Ÿæˆ: {len(nested_answer)} å­—ç¬¦")
            return nested_answer
            
        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå¥—ç´¯ç§¯å‹ç­”æ¡ˆå¤±è´¥: {e}")
            return f"(Multi-step reasoning answer for {root_answer})"
    
    def _generate_llm_integrated_query(self, queries_by_layer: Dict[int, List[str]], root_answer: str = "") -> str:
        """ç”ŸæˆLLMæ•´åˆå‹é—®é¢˜ - å®Œå…¨åŸºäºé—®é¢˜è¿›è¡Œè‡ªç„¶è¯­è¨€ç»„ç»‡ï¼Œä¸æ¶‰åŠä»»ä½•ç­”æ¡ˆ"""
        if not self.api_client or not queries_by_layer:
            return self._generate_fallback_integrated_query(queries_by_layer, "")
        
        try:
            # æ”¶é›†æ‰€æœ‰é—®é¢˜ï¼ŒæŒ‰å±‚çº§ä»æ·±åˆ°æµ…æ’åº
            all_queries_ordered = []
            for layer in sorted(queries_by_layer.keys(), reverse=True):
                all_queries_ordered.extend(queries_by_layer[layer])
            
            if not all_queries_ordered:
                return "What is the final answer that requires multi-step reasoning to determine?"
            
            integration_prompt = f"""**TASK: Create a REASONING CHAIN question that requires step-by-step logic, NOT parallel verification.**

**SUB-QUESTIONS (ordered from deepest to shallowest):**
{chr(10).join([f"{i+1}. {q}" for i, q in enumerate(all_queries_ordered)])}

**CRITICAL REQUIREMENTS:**
1. Create a **SEQUENTIAL REASONING CHAIN** where each step depends on the previous answer
2. **NOT PARALLEL CONDITIONS**: Don't ask "What satisfies A, B, and C?"
3. **STEP-BY-STEP DEPENDENCY**: Answer1 â†’ Input for Step2 â†’ Answer2 â†’ Input for Step3
4. **NO ANSWER EXPOSURE**: The question must NEVER contain or hint at any answers
5. **NATURAL FLOW**: Like a detective following clues, each answer leads to the next question
6. **LOGICAL DEPENDENCY**: Each step must genuinely need the previous step's result

**FORBIDDEN PATTERNS (PARALLEL - DON'T USE):**
âŒ "What entity satisfies conditions A, B, and C?"
âŒ "Which company meets criteria X, Y, and Z?"
âŒ "What organization has features A, B, and C?"

**REQUIRED PATTERNS (SEQUENTIAL - USE THESE):**
âœ… "Starting with [CONDITION], what [INFO] can be identified? Using that [INFO], what [NEXT_INFO] follows? Finally, with [NEXT_INFO], what is the answer?"
âœ… "Given [INITIAL_CLUE], determine [STEP1]. From [STEP1], identify [STEP2]. Using [STEP2], what is the final answer?"
âœ… "Begin by finding [A]. Once [A] is known, use it to discover [B]. With [B] established, what [C] emerges?"

**EXAMPLES OF CORRECT REASONING CHAINS:**
- "Starting with the founding year, identify the company. Using that company's name, determine its headquarters location. From the headquarters, what is the current CEO's name?"
- "First establish the technology invented. Once the technology is known, find which company pioneered it. With the company identified, what is its current market value?"
- "Begin with the location described. From that location, identify the institution. Using the institution's identity, what field does it specialize in?"

**GOAL: Create ONE question that forces Agent to solve sequentially: Answer1 â†’ Question2 â†’ Answer2 â†’ Question3 â†’ Final Answer**

**OUTPUT:** One reasoning chain question that requires genuine step-by-step logic, NOT parallel verification."""

            response = self.api_client.generate_response(
                prompt=integration_prompt,
                temperature=0.6,
                max_tokens=400
            )
            
            if response and len(response.strip()) > 50:
                integrated_query = response.strip()
                # ç®€å•æ¸…ç†
                integrated_query = integrated_query.replace('"', '').replace('*', '').strip()
                
                logger.info(f"âœ… LLMæ•´åˆå‹é—®é¢˜ç”Ÿæˆ: {len(integrated_query)} å­—ç¬¦")
                return integrated_query
            
            return self._generate_fallback_integrated_query(queries_by_layer, "")
            
        except Exception as e:
            logger.error(f"ç”ŸæˆLLMæ•´åˆå‹é—®é¢˜å¤±è´¥: {e}")
            return self._generate_fallback_integrated_query(queries_by_layer, "")
    
    def _generate_llm_integrated_answer(self, answers_by_layer: Dict[int, List[str]], root_answer: str) -> str:
        """ç”ŸæˆLLMæ•´åˆå‹ç­”æ¡ˆ - çº¯å®¢è§‚äº‹å®ç­”æ¡ˆï¼Œæ— æ€è€ƒè¿‡ç¨‹æè¿°"""
        if not self.api_client or not answers_by_layer:
            return self._generate_fallback_integrated_answer(answers_by_layer, root_answer)
        
        try:
            # æ”¶é›†æ‰€æœ‰ç­”æ¡ˆï¼ŒæŒ‰å±‚çº§ä»æ·±åˆ°æµ…æ’åº
            all_answers_ordered = []
            for layer in sorted(answers_by_layer.keys(), reverse=True):
                all_answers_ordered.extend(answers_by_layer[layer])
            
            if not all_answers_ordered:
                return root_answer
            
            integration_prompt = f"""**TASK: Create a pure, objective factual answer - NO reasoning process, NO thinking descriptions.**

**SUPPORTING FACTS:**
{chr(10).join([f"- {a}" for a in all_answers_ordered])}

**ABSOLUTE REQUIREMENTS:**
1. **ONLY factual statement** - like an encyclopedia entry
2. **ZERO reasoning words**: No "because", "since", "therefore", "thus", "through", "based on", "by", "analysis"
3. **ZERO process words**: No "determine", "examine", "investigate", "consider", "conclude", "derive"
4. **ZERO connecting phrases**: No "leads to", "indicates", "shows that", "reveals", "demonstrates"
5. **DIRECT FACT ONLY**: State the answer as pure fact without explanation

**ABSOLUTELY FORBIDDEN:**
âŒ ANY reasoning explanation
âŒ ANY thinking process description  
âŒ ANY analysis words
âŒ ANY step-by-step description
âŒ ANY cause-and-effect language

**REQUIRED FORMAT:**
âœ… Simple factual statement like: "The answer is [FACT]."
âœ… Or: "[ENTITY] is the correct answer."
âœ… Or: "[FACT] matches the specified criteria."

**GOAL: Pure factual answer exactly like a reference book - NO thinking process whatsoever.**"""

            # æ„å»ºå®Œæ•´çš„promptï¼ˆåŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            full_prompt = "You are an expert at creating clear, logical answer explanations.\n\n" + integration_prompt
            
            response = self.api_client.generate_response(
                prompt=full_prompt,
                temperature=0.4,
                max_tokens=300
            )
            
            if response and len(response.strip()) > 10:
                integrated_answer = response.strip()
                logger.info(f"âœ… LLMæ•´åˆå‹ç­”æ¡ˆç”Ÿæˆ: {len(integrated_answer)} å­—ç¬¦")
                return integrated_answer
            
            return self._generate_fallback_integrated_answer(answers_by_layer, root_answer)
            
        except Exception as e:
            logger.error(f"ç”ŸæˆLLMæ•´åˆå‹ç­”æ¡ˆå¤±è´¥: {e}")
            return self._generate_fallback_integrated_answer(answers_by_layer, root_answer)
    
    def _generate_fallback_integrated_query(self, queries_by_layer: Dict[int, List[str]], root_answer: str) -> str:
        """ç”Ÿæˆåå¤‡æ•´åˆå‹é—®é¢˜ - è‡ªç„¶æ¨ç†é“¾æè¿°"""
        all_queries = []
        for layer in sorted(queries_by_layer.keys(), reverse=True):
            all_queries.extend(queries_by_layer[layer])
        
        if not all_queries:
            return "What is the final answer that requires multi-step reasoning to determine?"
        
        if len(all_queries) == 1:
            return f"What is the answer to: {all_queries[0]}"
        elif len(all_queries) == 2:
            return f"Starting with '{all_queries[0]}', use that result to determine the answer to '{all_queries[1]}'. What is the final outcome?"
        else:
            # è‡ªç„¶çš„æ¨ç†é“¾æè¿°
            return f"Begin by solving '{all_queries[0]}'. Use that answer to address '{all_queries[1]}'. Finally, apply the result to solve '{all_queries[2]}'. What is the ultimate conclusion?"
    
    def _generate_fallback_integrated_answer(self, answers_by_layer: Dict[int, List[str]], root_answer: str) -> str:
        """ç”Ÿæˆåå¤‡æ•´åˆå‹ç­”æ¡ˆ - çº¯å®¢è§‚è¡¨è¿°"""
        all_answers = []
        for layer in sorted(answers_by_layer.keys(), reverse=True):
            all_answers.extend(answers_by_layer[layer])
        
        if not all_answers:
            return root_answer
        
        if len(all_answers) == 1:
            return f"The answer is {root_answer}."
        elif len(all_answers) == 2:
            return f"{root_answer} corresponds to the specified criteria."
        else:
            return f"{root_answer} satisfies all the required conditions."
    
    def _record_trajectory(self, record: Dict[str, Any]):
        """è®°å½•è½¨è¿¹æ•°æ®"""
        record['timestamp'] = time.time()
        record['step_id'] = len(self.trajectory_records) + 1
        self.trajectory_records.append(record)
    
    def _record_complete_tree_trajectory(self, tree: AgentReasoningTree):
        """è®°å½•å®Œæ•´çš„æ¨ç†æ ‘è½¨è¿¹ç»“æ„"""
        try:
            # è®°å½•æ¨ç†æ ‘çš„æ‰€æœ‰èŠ‚ç‚¹
            for node_id, node in tree.all_nodes.items():
                if hasattr(node, 'precise_query') and node.precise_query:
                    self._record_trajectory({
                        'step': f'tree_node_layer_{node.layer}',
                        'tree_id': tree.tree_id,
                        'node_id': node_id,
                        'layer': node.layer,
                        'query_text': node.precise_query.query_text,
                        'answer': node.precise_query.answer,
                        'minimal_keywords': [kw.keyword for kw in node.precise_query.minimal_keywords] if hasattr(node.precise_query, 'minimal_keywords') else [],
                        'keyword_count': len(node.precise_query.minimal_keywords) if hasattr(node.precise_query, 'minimal_keywords') else 0,
                        'parent_id': node.parent_id,
                        'children_ids': node.children_ids,
                        'branch_type': node.branch_type if hasattr(node, 'branch_type') else 'unknown',
                        'validation_passed': node.precise_query.validation_passed if hasattr(node.precise_query, 'validation_passed') else True
                    })
            
            # è®°å½•æœ€ç»ˆç»¼åˆé—®é¢˜
            final_composite = tree.get('final_composite_query') if isinstance(tree, dict) else tree.final_composite_query
            if final_composite:
                self._record_trajectory({
                    'step': 'final_composite_query_complete',
                    'tree_id': tree.get('tree_id') if isinstance(tree, dict) else tree.tree_id,
                    'composite_query': final_composite,
                    'root_answer': tree.get('root_node', {}).get('query', {}).get('answer') if isinstance(tree, dict) else tree.root_node.query.answer,
                    'total_nodes': len(tree.get('all_nodes', {})) if isinstance(tree, dict) else len(tree.all_nodes),
                    'max_depth': tree.get('max_layers', 3) if isinstance(tree, dict) else (tree.max_depth if hasattr(tree, 'max_depth') else 3),
                    'complexity_score': tree.get('complexity_score', 0.0) if isinstance(tree, dict) else (tree.complexity_score if hasattr(tree, 'complexity_score') else 0.0)
                })
            
            total_nodes = len(tree.get('all_nodes', {})) if isinstance(tree, dict) else len(tree.all_nodes)
            logger.info(f"âœ… å®Œæ•´æ¨ç†æ ‘è½¨è¿¹è®°å½•å®Œæˆ: {total_nodes} ä¸ªèŠ‚ç‚¹")
            
        except Exception as e:
            logger.error(f"è®°å½•å®Œæ•´æ¨ç†æ ‘è½¨è¿¹å¤±è´¥: {e}")
    
    def _create_success_result(
        self, document_id: str, reasoning_trees: List[AgentReasoningTree], 
        processing_time: float
    ) -> Dict[str, Any]:
        """åˆ›å»ºæˆåŠŸç»“æœ"""
        return {
            'success': True,
            'document_id': document_id,
            'reasoning_trees': reasoning_trees,
            'processing_time': processing_time,
            'trajectory_records': self.trajectory_records.copy(),
            'statistics': self.stats.copy(),
            'framework_type': 'agent_depth_reasoning',
            'total_trees': len(reasoning_trees),
            'total_composite_queries': sum(1 for tree in reasoning_trees if (tree.get('final_composite_query') if isinstance(tree, dict) else tree.final_composite_query))
        }
    
    def _create_error_result(self, document_id: str, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'success': False,
            'document_id': document_id,
            'error': error_message,
            'trajectory_records': self.trajectory_records.copy(),
            'statistics': self.stats.copy(),
            'framework_type': 'agent_depth_reasoning'
        }
    
    # ============ è¾…åŠ©æ–¹æ³•å ä½ç¬¦ ============
    # è¿™äº›æ–¹æ³•éœ€è¦å…·ä½“å®ç°
    
    def _extract_unique_short_answers(self, document_content: str) -> List[ShortAnswer]:
        """æå–å”¯ä¸€çŸ­ç­”æ¡ˆ"""
        if not self.api_client:
            return []
        
        try:
            # ä½¿ç”¨æ”¹è¿›çš„æå–æç¤ºè¯ï¼Œå‚è€ƒWorkFlowè®¾è®¡
            prompt = f"""**TASK: Extract precise, objective facts from this document that can serve as Short Answers for Agent reasoning tests.**

**DOCUMENT TO ANALYZE:**
{document_content[:2000]}

**EXTRACTION REQUIREMENTS (Following WorkFlow):**
1. Extract **3 clear and unique Short Answers** (proper nouns or numbers)
2. Each answer must be **highly specific** (proper nouns, numbers, technical terms)
3. All answers should be **distinctive** and not too broad or ambiguous
4. All answers must be **unique** and not repeated
5. Answers must be **verifiable** through web search (not common sense)

**TARGET ANSWER TYPES (Priority Order):**
1. **PROPER NOUNS**: Companies, organizations, technologies, products, people
2. **NUMBERS**: Specific quantities, measurements, percentages, years, prices
3. **TECHNICAL TERMS**: Scientific terminology, algorithms, model numbers
4. **DATES**: Specific time references, launch dates, establishment dates
5. **LOCATIONS**: Cities, countries, specific places

**QUALITY CRITERIA:**
- Must be **objective facts** (not subjective opinions)
- Must have **single correct answers** (no ambiguity)
- Must be **document-based** (require web search, not common sense)
- Must be **short phrases** (1-8 words maximum)

**Output Format (JSON):**
{{
    "short_answers": [
        {{
            "answer_text": "exact answer text",
            "answer_type": "noun/number/name/date/location",
            "confidence": 0.0-1.0,
            "extraction_source": "surrounding context sentence",
            "document_position": approximate_position,
            "reasoning": "why this makes a good objective answer"
        }}
    ]
}}

**TARGET: Extract exactly 3 high-quality Short Answers that will challenge Agent reasoning.**"""

            response = self.api_client.generate_response(
                prompt=prompt,
                temperature=0.3,
                max_tokens=800
            )
            
            # è§£æå“åº”
            parsed_data = self._parse_json_response(response)
            if not parsed_data or 'short_answers' not in parsed_data:
                logger.warning("æ— æ³•è§£æShort Answerå“åº”ï¼Œå°è¯•ä»åŸå§‹å“åº”ä¸­æå–")
                # å°è¯•ä»å“åº”ä¸­ç›´æ¥æå–ä¿¡æ¯
                return self._extract_short_answers_from_text(response, document_content)
            
            short_answers = []
            for i, sa_data in enumerate(parsed_data['short_answers'][:3]):
                # å®‰å…¨åœ°è§£ædocument_position
                try:
                    doc_position = int(sa_data.get('document_position', i * 100))
                except (ValueError, TypeError):
                    doc_position = i * 100  # ä½¿ç”¨é»˜è®¤å€¼
                
                # å®‰å…¨åœ°è§£æconfidence
                try:
                    confidence = float(sa_data.get('confidence', 0.5))
                except (ValueError, TypeError):
                    confidence = 0.5  # ä½¿ç”¨é»˜è®¤å€¼
                
                short_answer = ShortAnswer(
                    answer_text=sa_data.get('answer_text', ''),
                    answer_type=sa_data.get('answer_type', 'unknown'),
                    confidence=confidence,
                    extraction_source=sa_data.get('extraction_source', '')[:200],
                    document_position=doc_position
                )
                if short_answer.answer_text.strip():
                    short_answers.append(short_answer)
            
            self.stats['short_answers_extracted'] += len(short_answers)
            logger.info(f"æå–åˆ° {len(short_answers)} ä¸ªShort Answer")
            
            return short_answers
            
        except Exception as e:
            logger.error(f"æå–Short Answerå¤±è´¥: {e}")
            return []
    
    def _extract_short_answers_from_text(self, response: str, document_content: str) -> List[ShortAnswer]:
        """ä»å“åº”æ–‡æœ¬ä¸­ç›´æ¥æå–Short Answerï¼ˆåå¤‡æ–¹æ³•ï¼‰"""
        try:
            import re
            
            short_answers = []
            
            # å°è¯•æå–ç±»ä¼¼ "Answer 1:", "Question 1:" çš„æ¨¡å¼
            answer_patterns = [
                r'(?:Answer|answer)\s*\d*:\s*([^\n\r]+)',
                r'answer_text["\']?\s*:\s*["\']([^"\']+)["\']',
                r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',  # ä¸“æœ‰åè¯æ¨¡å¼
                r'(\d+(?:\.\d+)?\s*(?:seconds|years|GWh|mph|%|dollars?))',  # æ•°å­—+å•ä½æ¨¡å¼
            ]
            
            for i, pattern in enumerate(answer_patterns):
                matches = re.findall(pattern, response, re.IGNORECASE)
                for j, match in enumerate(matches[:3]):  # æœ€å¤š3ä¸ª
                    if len(match.strip()) > 2 and len(match.strip()) < 50:
                        short_answer = ShortAnswer(
                            answer_text=match.strip(),
                            answer_type=self._guess_answer_type(match.strip()),
                            confidence=0.6,  # è¾ƒä½çš„ç½®ä¿¡åº¦
                            extraction_source=f"Pattern extraction from response",
                            document_position=j * 100
                        )
                        short_answers.append(short_answer)
                
                if short_answers:
                    break  # æ‰¾åˆ°ç­”æ¡ˆå°±åœæ­¢
            
            logger.info(f"ä»æ–‡æœ¬ä¸­æå–åˆ° {len(short_answers)} ä¸ªShort Answer")
            return short_answers[:3]  # æœ€å¤šè¿”å›3ä¸ª
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æå–Short Answerå¤±è´¥: {e}")
            return []
    
    def _guess_answer_type(self, text: str) -> str:
        """çŒœæµ‹ç­”æ¡ˆç±»å‹"""
        text_lower = text.lower()
        
        # æ•°å­—æ£€æµ‹
        if any(char.isdigit() for char in text):
            return "number"
        
        # æ—¶é—´/æ—¥æœŸæ£€æµ‹
        time_keywords = ['year', 'date', '20', '19', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']
        if any(keyword in text_lower for keyword in time_keywords):
            return "date"
        
        # åœ°ç‚¹æ£€æµ‹
        location_keywords = ['city', 'country', 'university', 'center', 'institute', 'lab', 'factory']
        if any(keyword in text_lower for keyword in location_keywords):
            return "location"
        
        # ä¸“æœ‰åè¯æ£€æµ‹ï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
        if text and text[0].isupper():
            return "name"
        
        return "noun"
    
    def _build_minimal_precise_query(
        self, short_answer: ShortAnswer, document_content: str, query_id: str
    ) -> Optional[PreciseQuery]:
        """æ„å»ºæœ€å°ç²¾ç¡®é—®é¢˜"""
        if not self.api_client:
            return None
        
        try:
            logger.info(f"ä¸ºShort Answer '{short_answer.answer_text}' æ„å»ºæœ€å°ç²¾ç¡®é—®é¢˜")
            
            # Step 1: ä½¿ç”¨Webæœç´¢å¢å¼ºé—®é¢˜ç”Ÿæˆ
            search_context = ""
            if self.search_client:
                # ç›´æ¥è°ƒç”¨search_clientï¼Œwrapperä¼šå¤„ç†API key
                search_results = self.search_client(f"{short_answer.answer_text} definition characteristics")
                
                # åªæœ‰æˆåŠŸè·å–åˆ°çœŸå®æœç´¢ç»“æœæ‰ä½¿ç”¨ï¼Œå¦åˆ™ç•™ç©º
                if (search_results and 
                    search_results.get('status') == 'success' and 
                    search_results.get('results')):
                    search_context = " ".join([
                        result.get('content', '')[:200] 
                        for result in search_results['results'][:2]
                    ])
            
            # Step 2: ç”Ÿæˆé—®é¢˜ï¼ˆåŒ…å«è‡³å°‘ä¸¤ä¸ªå…³é”®è¯ï¼‰
            generation_prompt = f"""**TASK: Generate a minimal, precise question for Agent reasoning testing.**

**SHORT ANSWER TO TARGET:** {short_answer.answer_text}
**ANSWER TYPE:** {short_answer.answer_type}
**DOCUMENT CONTEXT:** {short_answer.extraction_source}
**WEB SEARCH CONTEXT:** {search_context[:300] if search_context else "Not available"}

**REQUIREMENTS (Following WorkFlow):**
1. Question must have **exactly ONE correct answer**: "{short_answer.answer_text}"
2. Question must contain **at least TWO explicit keywords** to ensure precision
3. Question must be **based on document/web knowledge** (not common sense)
4. Question must be **solvable** and **unambiguous**
5. Question should **require web search** to verify the answer

**DESIGN PRINCIPLES:**
- Make the question **specific enough** that only "{short_answer.answer_text}" is correct
- Include **contextual constraints** that eliminate other possible answers
- Use **precise language** that requires factual knowledge
- Avoid questions that are too broad or too narrow

**Output Format (JSON):**
{{
    "question_text": "Generated question with at least 2 explicit keywords",
    "minimal_keywords": ["keyword1", "keyword2"],
    "generation_method": "web_search/llm_analysis",
    "confidence": 0.0-1.0,
    "reasoning": "Why this question uniquely identifies the answer"
}}

**TARGET: Generate ONE precise question that challenges Agent reasoning.**"""

            response = self.api_client.generate_response(
                prompt=generation_prompt,
                temperature=0.4,
                max_tokens=600
            )
            
            parsed_data = self._parse_json_response(response)
            if not parsed_data or 'question_text' not in parsed_data:
                logger.warning("æ— æ³•è§£æé—®é¢˜ç”Ÿæˆå“åº”")
                return None
            
            # Step 3: ç§»é™¤éå¿…è¦å…³é”®è¯ - ä½¿ç”¨æ–°çš„ç²¾ç¡®ç®—æ³•
            question_text = parsed_data['question_text']
            initial_keywords = parsed_data.get('minimal_keywords', [])
            
            # ä¼˜å…ˆä½¿ç”¨æ–°çš„ç²¾ç¡®å…³é”®è¯æœ€å°åŒ–ç®—æ³•
            try:
                optimized_keywords = self._optimize_minimal_keywords_precisely(
                    question_text, short_answer.answer_text, initial_keywords
                )
                logger.info(f"âœ… ä½¿ç”¨ç²¾ç¡®ç®—æ³•ä¼˜åŒ–å…³é”®è¯: {len(optimized_keywords)} ä¸ªå¿…è¦å…³é”®è¯")
            except Exception as e:
                logger.warning(f"ç²¾ç¡®ç®—æ³•å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰æ–¹æ³•: {e}")
                # å›é€€åˆ°åŸæœ‰çš„å…³é”®è¯ä¼˜åŒ–æ–¹æ³•
                optimized_keywords = self._optimize_minimal_keywords(
                    question_text, short_answer.answer_text, initial_keywords
                )
            
            # Step 4: åˆ›å»ºMinimalKeywordå¯¹è±¡
            minimal_keywords = []
            for i, kw in enumerate(optimized_keywords):
                minimal_keyword = MinimalKeyword(
                    keyword=kw,
                    keyword_type=self._classify_keyword_type(kw),
                    uniqueness_score=0.8,
                    necessity_score=0.9,
                    extraction_context=question_text,
                    position_in_query=question_text.find(kw)
                )
                minimal_keywords.append(minimal_keyword)
            
            # Step 5: æœ€ç»ˆéªŒè¯ - é›†æˆæœ€å°ç²¾ç¡®é—®é¢˜éªŒè¯
            validation_passed = self._validate_root_query(
                question_text, short_answer.answer_text, minimal_keywords
            )
            
            # é¢å¤–çš„æœ€å°ç²¾ç¡®é—®é¢˜éªŒè¯
            try:
                precise_validation = self._validate_minimal_precise_question(
                    question_text, short_answer.answer_text, optimized_keywords
                )
                
                if precise_validation.get('is_minimal', False) and precise_validation.get('is_precise', False):
                    logger.info(f"âœ… æœ€å°ç²¾ç¡®é—®é¢˜éªŒè¯é€šè¿‡: è´¨é‡è¯„çº§ {precise_validation.get('overall_quality', 'fair')}")
                    validation_passed = validation_passed and True
                else:
                    logger.warning(f"âš ï¸ æœ€å°ç²¾ç¡®é—®é¢˜éªŒè¯æœªå®Œå…¨é€šè¿‡: {precise_validation.get('reasoning', 'No details')}")
                    # ä¸å¼ºåˆ¶å¤±è´¥ï¼Œä½†è®°å½•è­¦å‘Š
                    
            except Exception as e:
                logger.warning(f"æœ€å°ç²¾ç¡®é—®é¢˜éªŒè¯å¤±è´¥: {e}")
                # ä¸å½±å“ä¸»æµç¨‹
            
            precise_query = PreciseQuery(
                query_id=query_id,
                query_text=question_text,
                answer=short_answer.answer_text,
                minimal_keywords=minimal_keywords,
                generation_method=parsed_data.get('generation_method', 'llm_analysis'),
                validation_passed=validation_passed,
                layer_level=0,
                extension_type="root"
            )
            
            logger.info(f"âœ… Root Queryæ„å»ºå®Œæˆ: {question_text}")
            return precise_query
            
        except Exception as e:
            logger.error(f"æ„å»ºæœ€å°ç²¾ç¡®é—®é¢˜å¤±è´¥: {e}")
            return None
    
    def _extract_candidate_keywords(self, query_text: str, answer: str) -> List[MinimalKeyword]:
        """æå–å€™é€‰å…³é”®è¯"""
        if not self.api_client:
            return []
        
        try:
            # å‚è€ƒWorkFlowè®¾è®¡ï¼šExtract n sub-keywords where n is minimal
            prompt = f"""**TASK: Extract minimal sub-keywords from query for Agent reasoning testing.**

**QUERY:** {query_text}
**ANSWER:** {answer}

**EXTRACTION REQUIREMENTS (Following WorkFlow):**
1. Extract **n sub-keywords** where n is the **MINIMAL number** sufficient to identify answer
2. Keywords must be **HIGHLY SPECIFIC** (proper nouns, numbers, technical terms)
3. Keywords must be **DISTINCTIVE** and not too broad or ambiguous
4. Keywords must be **UNIQUE** and not repeated
5. Each keyword should be able to serve as a child question answer

**KEYWORD PRIORITIES:**
1. **PROPER NOUNS**: Names, companies, organizations, technologies
2. **NUMBERS**: Specific quantities, measurements, years, prices
3. **TECHNICAL TERMS**: Scientific/technical terminology, algorithms
4. **DATES**: Specific time references, periods
5. **LOCATIONS**: Cities, countries, institutions

**EXTRACTION RULES:**
- Focus on **CONTENT WORDS** (nouns, proper nouns, numbers, dates)
- Avoid **QUESTION WORDS**: "what", "which", "who", "when", "where", "how", "why"
- Avoid **COMMON WORDS**: "the", "a", "an", "in", "on", "at", "for", "with"
- Prefer **single-word or short-phrase** keywords (1-3 words)
- Each keyword must be **independently meaningful**

**Output Format (JSON):**
{{
    "candidate_keywords": [
        {{
            "keyword": "exact keyword text",
            "keyword_type": "proper_noun/number/technical_term/date/location",
            "extraction_context": "surrounding phrase or sentence",
            "specificity_score": 0.0-1.0,
            "necessity_reasoning": "why this keyword is necessary"
        }}
    ],
    "minimal_count": "minimum number needed to identify answer"
}}

**TARGET: Extract 2-5 highly specific keywords that together uniquely identify the answer.**"""

            response = self.api_client.generate_response(
                prompt=prompt,
                temperature=0.3,
                max_tokens=500
            )
            
            parsed_data = self._parse_json_response(response)
            if not parsed_data or 'candidate_keywords' not in parsed_data:
                return []
            
            candidate_keywords = []
            for kw_data in parsed_data['candidate_keywords']:
                # å®‰å…¨åœ°è§£æåˆ†æ•°
                try:
                    uniqueness_score = float(kw_data.get('specificity_score', 0.5))
                except (ValueError, TypeError):
                    uniqueness_score = 0.5
                
                keyword_text = kw_data.get('keyword', '')
                keyword = MinimalKeyword(
                    keyword=keyword_text,
                    keyword_type=kw_data.get('keyword_type', 'unknown'),
                    uniqueness_score=uniqueness_score,
                    necessity_score=0.8,  # å°†åœ¨åç»­éªŒè¯ä¸­è®¡ç®—
                    extraction_context=kw_data.get('extraction_context', ''),
                    position_in_query=query_text.find(keyword_text) if keyword_text else -1
                )
                if keyword.keyword.strip():
                    candidate_keywords.append(keyword)
            
            logger.info(f"æå–åˆ° {len(candidate_keywords)} ä¸ªå€™é€‰å…³é”®è¯")
            return candidate_keywords
            
        except Exception as e:
            logger.error(f"æå–å€™é€‰å…³é”®è¯å¤±è´¥: {e}")
            return []
    
    def _validate_keyword_necessity(
        self, query_text: str, answer: str, keywords: List[MinimalKeyword]
    ) -> List[MinimalKeyword]:
        """éªŒè¯å…³é”®è¯å¿…è¦æ€§"""
        if not self.api_client or not keywords:
            return keywords
        
        try:
            # å‚è€ƒWorkFlowï¼šPerform Minimum Keyword Check - masking test
            logger.info(f"éªŒè¯ {len(keywords)} ä¸ªå…³é”®è¯çš„å¿…è¦æ€§")
            
            # ä½¿ç”¨å¹¶è¡ŒéªŒè¯å™¨
            if self.parallel_validator:
                necessary_keywords = self.parallel_validator.validate_keywords_parallel(
                    keywords, query_text, answer
                )
            else:
                # å›é€€åˆ°ä¸²è¡ŒéªŒè¯
                necessary_keywords = []
                for keyword in keywords:
                    # ä¸ºæ¯ä¸ªå…³é”®è¯æ‰§è¡Œmaskingæµ‹è¯•
                    masking_prompt = f"""**TASK: Perform Minimum Keyword Check for Agent reasoning testing.**

**ORIGINAL QUERY:** {query_text}
**TARGET ANSWER:** {answer}
**KEYWORD TO TEST:** {keyword.keyword}

**MASKING TEST (Following WorkFlow):**
Mask the keyword "{keyword.keyword}" from the query and check if the remaining keywords and descriptions can still uniquely identify the answer "{answer}".

**MODIFIED QUERY (with keyword masked):** {query_text.replace(keyword.keyword, '[MASKED]')}

**EVALUATION CRITERIA:**
1. Can the modified query still **uniquely identify** the answer "{answer}"?
2. Are the **remaining keywords sufficient** to eliminate other possible answers?
3. Does masking this keyword create **ambiguity** or **multiple possible answers**?
4. Is this keyword **essential** for answer precision?

**NECESSITY LEVELS:**
- **ESSENTIAL** (1.0): Masking creates ambiguity, multiple answers possible
- **IMPORTANT** (0.8): Masking reduces precision significantly  
- **HELPFUL** (0.6): Masking has minor impact on precision
- **REDUNDANT** (0.3): Masking has no impact, other keywords sufficient

**Output Format (JSON):**
{{
    "is_necessary": true/false,
    "necessity_score": 0.0-1.0,
    "masking_impact": "essential/important/helpful/redundant",
    "reasoning": "detailed explanation of why this keyword is/isn't necessary",
    "alternative_answers_without_keyword": ["list", "of", "possible", "answers"]
}}

**TARGET: Determine if this keyword is essential for unique answer identification.**"""

                    response = self.api_client.generate_response(
                        prompt=masking_prompt,
                        temperature=0.2,
                        max_tokens=400
                    )
                    
                    parsed_data = self._parse_json_response(response)
                    if parsed_data:
                        is_necessary = parsed_data.get('is_necessary', True)
                        
                        # å®‰å…¨åœ°è§£ænecessity_score
                        try:
                            necessity_score = float(parsed_data.get('necessity_score', 0.8))
                        except (ValueError, TypeError):
                            necessity_score = 0.8
                        
                        # æ›´æ–°å…³é”®è¯çš„å¿…è¦æ€§åˆ†æ•°
                        keyword.necessity_score = necessity_score
                        
                        # åªä¿ç•™å¿…è¦çš„å…³é”®è¯ï¼ˆåˆ†æ•° > 0.5ï¼‰
                        if is_necessary and necessity_score > 0.5:
                            necessary_keywords.append(keyword)
                            logger.info(f"âœ… å…³é”®è¯ '{keyword.keyword}' æ˜¯å¿…è¦çš„ (åˆ†æ•°: {necessity_score:.2f})")
                        else:
                            logger.info(f"âŒ å…³é”®è¯ '{keyword.keyword}' ä¸æ˜¯å¿…è¦çš„ (åˆ†æ•°: {necessity_score:.2f})")
                    else:
                        # è§£æå¤±è´¥ï¼Œä¿å®ˆåœ°ä¿ç•™å…³é”®è¯
                        necessary_keywords.append(keyword)
            
            logger.info(f"éªŒè¯å®Œæˆ: {len(necessary_keywords)}/{len(keywords)} ä¸ªå…³é”®è¯æ˜¯å¿…è¦çš„")
            return necessary_keywords
            
        except Exception as e:
            logger.error(f"éªŒè¯å…³é”®è¯å¿…è¦æ€§å¤±è´¥: {e}")
            return keywords
    
    def _calculate_keyword_uniqueness(self, keyword: str, answer: str) -> float:
        """è®¡ç®—å…³é”®è¯å”¯ä¸€æ€§åˆ†æ•°"""
        try:
            # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—å”¯ä¸€æ€§åˆ†æ•°
            keyword_lower = keyword.lower()
            answer_lower = answer.lower()
            
            # 1. é•¿åº¦å› å­ - æ›´é•¿çš„å…³é”®è¯é€šå¸¸æ›´å”¯ä¸€
            length_factor = min(len(keyword) / 10.0, 1.0)
            
            # 2. ç‰¹å¼‚æ€§å› å­ - æ•°å­—ã€ä¸“æœ‰åè¯æ›´å”¯ä¸€
            specificity_factor = 0.5
            if keyword.isdigit():
                specificity_factor = 0.9  # æ•°å­—å¾ˆç‰¹å¼‚
            elif keyword[0].isupper():
                specificity_factor = 0.8  # ä¸“æœ‰åè¯è¾ƒç‰¹å¼‚
            elif any(char.isdigit() for char in keyword):
                specificity_factor = 0.7  # åŒ…å«æ•°å­—çš„è¯è¾ƒç‰¹å¼‚
            
            # 3. ä¸ç­”æ¡ˆçš„å…³è”åº¦ - å…³é”®è¯åº”è¯¥å¼ºå…³è”ç­”æ¡ˆ
            if keyword_lower in answer_lower or answer_lower in keyword_lower:
                association_factor = 0.9
            else:
                association_factor = 0.6
            
            # 4. é€šç”¨è¯æƒ©ç½š
            common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
            if keyword_lower in common_words:
                common_penalty = 0.3
            else:
                common_penalty = 1.0
            
            # ç»¼åˆè®¡ç®—å”¯ä¸€æ€§åˆ†æ•°
            uniqueness = (length_factor * 0.2 + 
                         specificity_factor * 0.4 + 
                         association_factor * 0.3 + 
                         common_penalty * 0.1)
            
            return min(max(uniqueness, 0.1), 1.0)  # é™åˆ¶åœ¨0.1-1.0èŒƒå›´å†…
            
        except Exception as e:
            logger.error(f"è®¡ç®—å…³é”®è¯å”¯ä¸€æ€§å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰å”¯ä¸€æ€§
    
    def _web_search_for_keyword(self, keyword: str) -> str:
        """ä¸ºå…³é”®è¯è¿›è¡ŒWebæœç´¢"""
        if not self.search_client:
            return f"Search context for {keyword}"
        
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = f"{keyword} definition characteristics properties"
            
            # æ‰§è¡Œæœç´¢
            # ç›´æ¥è°ƒç”¨search_clientï¼Œwrapperä¼šå¤„ç†API key
            search_results = self.search_client(search_query, max_results=3)
            
            # åªæœ‰æˆåŠŸè·å–åˆ°çœŸå®æœç´¢ç»“æœæ‰ä½¿ç”¨
            if (search_results and 
                search_results.get('status') == 'success' and 
                search_results.get('results')):
                # æå–æœç´¢å†…å®¹
                search_content = []
                for result in search_results['results'][:3]:
                    content = result.get('content', '')
                    snippet = result.get('snippet', '')
                    combined = f"{snippet} {content}"[:300]
                    if combined.strip():
                        search_content.append(combined)
                
                search_context = " ".join(search_content)
                logger.info(f"ğŸŒ Webæœç´¢ '{keyword}': è·å– {len(search_content)} ä¸ªç»“æœ")
                return search_context
            else:
                logger.warning(f"Webæœç´¢ '{keyword}' æ— ç»“æœ")
                return f"Search context for {keyword}"
                
        except Exception as e:
            logger.error(f"Webæœç´¢ '{keyword}' å¤±è´¥: {e}")
            return f"Search context for {keyword}"
    
    def _smart_web_search_for_keyword(self, keyword: str, parent_question: str, parent_answer: str) -> str:
        """æ™ºèƒ½Webæœç´¢ - é›†æˆå¾ªç¯é—®é¢˜å¤„ç†å™¨"""
        
        # å¦‚æœæ²¡æœ‰æœç´¢å®¢æˆ·ç«¯ï¼Œä½¿ç”¨ç®€å•ä¸Šä¸‹æ–‡
        if not self.search_client:
            return f"Search context for {keyword}"
        
        try:
            # 1. ä½¿ç”¨å¾ªç¯é—®é¢˜å¤„ç†å™¨è¯„ä¼°å’Œå¤„ç†å¾ªç¯é£é™©
            search_results = self.circular_handler.handle_circular_risk(
                keyword, parent_question, parent_answer, self
            )
            
            # 2. å¦‚æœå¾ªç¯å¤„ç†å™¨è¿”å›Noneï¼Œè¡¨ç¤ºè·³è¿‡è¯¥å…³é”®è¯
            if search_results is None:
                logger.warning(f"ğŸš« å¾ªç¯é£é™©è¿‡é«˜ï¼Œè·³è¿‡å…³é”®è¯: {keyword}")
                self.stats['circular_reasoning_prevented'] = self.stats.get('circular_reasoning_prevented', 0) + 1
                return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè·³è¿‡
            
            # 3. å¤„ç†æœç´¢ç»“æœ
            if search_results:
                search_content = []
                for result in search_results[:3]:  # æœ€å¤šä½¿ç”¨3ä¸ªç»“æœ
                    content = result.get('content', '')
                    snippet = result.get('snippet', '')
                    combined = f"{snippet} {content}"[:300]
                    if combined.strip():
                        search_content.append(combined)
                
                if search_content:
                    search_context = " ".join(search_content)
                    logger.info(f"ğŸŒ æ™ºèƒ½æœç´¢ '{keyword}': è·å– {len(search_content)} ä¸ªç»“æœ (å¾ªç¯å·²æ£€æµ‹)")
                    return search_context
            
            # 4. å¦‚æœæ²¡æœ‰æœ‰æ•ˆç»“æœï¼Œå›é€€åˆ°åŸå§‹æœç´¢
            logger.info(f"ğŸ”„ å¾ªç¯å¤„ç†æ— ç»“æœï¼Œå›é€€åˆ°æ ‡å‡†æœç´¢: {keyword}")
            return self._web_search_for_keyword(keyword)
                
        except Exception as e:
            logger.error(f"æ™ºèƒ½æœç´¢ '{keyword}' å¤±è´¥: {e}")
            # å‡ºé”™æ—¶å›é€€åˆ°æ ‡å‡†æœç´¢
            return self._web_search_for_keyword(keyword)
    
    def search(self, query: str, max_results: int = 3):
        """æä¾›ç»™å¾ªç¯å¤„ç†å™¨ä½¿ç”¨çš„æœç´¢æ¥å£"""
        if not self.search_client:
            return None
        
        try:
            # ç›´æ¥è°ƒç”¨search_clientï¼Œwrapperä¼šå¤„ç†API key
            results = self.search_client(query, max_results=max_results)
            
            # åªæœ‰æˆåŠŸè·å–åˆ°çœŸå®æœç´¢ç»“æœæ‰è¿”å›
            if (results and 
                results.get('status') == 'success' and 
                results.get('results')):
                return results['results']
            return None
        except Exception as e:
            logger.error(f"æœç´¢æ¥å£è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _generate_unrelated_query(
        self, keyword: MinimalKeyword, search_context: str, 
        layer: int, query_id: str
    ) -> Optional[PreciseQuery]:
        """ç”Ÿæˆæ— å…³è”é—®é¢˜"""
        if not self.api_client:
            return None
        
        try:
            logger.info(f"ä¸ºå…³é”®è¯ '{keyword.keyword}' ç”Ÿæˆæ— å…³è”é—®é¢˜ (Layer {layer})")
            
            # å‚è€ƒWorkFlowè®¾è®¡ï¼šè®¾è®¡å¯¹åº”é—®é¢˜ï¼Œç¡®ä¿æ— å…³è”æ€§å’Œæ— å¾ªç¯æ¨ç†
            generation_prompt = f"""**TASK: Generate an unrelated question for Agent reasoning testing with NO CIRCULAR REASONING.**

**TARGET KEYWORD (must be the answer):** {keyword.keyword}
**KEYWORD TYPE:** {keyword.keyword_type}
**WEB SEARCH CONTEXT:** {search_context[:500]}
**LAYER LEVEL:** {layer}

**CRITICAL REQUIREMENTS (Following WorkFlow):**
1. The question must have **"{keyword.keyword}"** as the ONLY correct answer
2. The question must be **completely unrelated** to any parent questions
3. The question must be based on **web search context** (not original document)
4. The question must be **solvable** and **unambiguous**
5. The question must require **web search verification** to answer

**CIRCULAR REASONING PREVENTION (CRITICAL):**
- **AVOID entity-attribute swaps**: If parent asks "What company was founded in 2003?" â†’ DON'T ask "When was Tesla founded?"
- **AVOID reverse relationships**: If parent asks "What acceleration does Model S have?" â†’ DON'T ask "Which car has 2.1 second acceleration?"
- **AVOID temporal inversions**: If parent mentions year/date â†’ DON'T ask about events in that year
- **AVOID attribute reversals**: If parent asks about property X of entity Y â†’ DON'T ask about entity with property X

**NO CORRELATION RULES:**
- **NO shared topic** with parent questions
- **NO shared context** with parent questions  
- **NO similar wording** or phrasing patterns
- **NO conceptual overlap** with parent domain
- **USE different knowledge domain** entirely
- **NO logical dependency** between questions

**GENERATION STRATEGY:**
- Base the question on **WEB SEARCH CONTEXT** only
- Focus on **completely different aspects** of the keyword
- Use **different knowledge domains** (if keyword is from tech domain, ask about geography/history/etc.)
- Ensure **no logical path** connects the questions
- Make answer verification require **external search**

**SAFE DESIGN PATTERNS:**
- Domain Switch: Tech keyword â†’ Ask about geography/history/biology
- Context Switch: Business keyword â†’ Ask about science/arts/sports  
- Time Switch: Modern keyword â†’ Ask about historical/ancient context
- Scale Switch: Macro keyword â†’ Ask about micro/personal level

**DANGEROUS PATTERNS TO AVOID:**
- Attribute reversal: "What X does Y have?" â†” "What Y has X?"
- Temporal reversal: "What happened in year Y?" â†” "When did X happen?"
- Entity-property swap: "Which entity has property P?" â†” "What property does entity E have?"

**Output Format (JSON):**
{{
    "question_text": "Generated unrelated question",
    "answer": "{keyword.keyword}",
    "minimal_keywords": ["extracted", "keywords"],
    "generation_method": "web_search",
    "layer_level": {layer},
    "no_correlation_verified": true/false,
    "circular_reasoning_check": "explanation of how circular reasoning was avoided",
    "domain_switch": "explanation of knowledge domain switch",
    "reasoning": "Why this question is completely unrelated and non-circular"
}}

**TARGET: Generate ONE completely unrelated question with NO circular reasoning that has "{keyword.keyword}" as the definitive answer.**"""

            response = self.api_client.generate_response(
                prompt=generation_prompt,
                temperature=0.5,  # æ›´é«˜åˆ›é€ æ€§ï¼Œç¡®ä¿å¤šæ ·æ€§
                max_tokens=600
            )
            
            parsed_data = self._parse_json_response(response)
            if not parsed_data or 'question_text' not in parsed_data:
                logger.warning("æ— æ³•è§£ææ— å…³è”é—®é¢˜ç”Ÿæˆå“åº”")
                return None
            
            # åˆ›å»ºMinimalKeywordå¯¹è±¡
            minimal_keywords = []
            for kw in parsed_data.get('minimal_keywords', []):
                minimal_keyword = MinimalKeyword(
                    keyword=kw,
                    keyword_type=self._classify_keyword_type(kw),
                    uniqueness_score=0.7,
                    necessity_score=0.8,
                    extraction_context=parsed_data['question_text'],
                    position_in_query=parsed_data['question_text'].find(kw)
                )
                minimal_keywords.append(minimal_keyword)
            
            # éªŒè¯é—®é¢˜è´¨é‡
            validation_passed = self._validate_unrelated_query(
                parsed_data['question_text'], 
                keyword.keyword,
                minimal_keywords
            )
            
            precise_query = PreciseQuery(
                query_id=query_id,
                query_text=parsed_data['question_text'],
                answer=keyword.keyword,
                minimal_keywords=minimal_keywords,
                generation_method="web_search",
                validation_passed=validation_passed,
                layer_level=layer,
                extension_type="series" if "series" in query_id else "parallel"
            )
            
            logger.info(f"âœ… æ— å…³è”é—®é¢˜ç”Ÿæˆ: {parsed_data['question_text']}")
            return precise_query
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ— å…³è”é—®é¢˜å¤±è´¥: {e}")
            return None
    
    def _validate_no_correlation(self, query1: str, query2: str) -> bool:
        """éªŒè¯ä¸¤ä¸ªé—®é¢˜æ— å…³è”"""
        if not self.api_client:
            return True  # æ— æ³•éªŒè¯æ—¶ä¿å®ˆè¿”å›True
        
        try:
            # é¦–å…ˆè¿›è¡Œå¾ªç¯æ£€æµ‹
            has_circular_reasoning = self._detect_circular_reasoning(query1, query2)
            if has_circular_reasoning:
                logger.warning(f"âŒ æ£€æµ‹åˆ°å¾ªç¯æ¨ç†é£é™©")
                return False
            
            # ç„¶åè¿›è¡Œå…³è”æ€§æ£€æµ‹
            correlation_prompt = f"""**TASK: Verify NO CORRELATION and NO CIRCULAR REASONING between two questions for Agent reasoning testing.**

**PARENT QUESTION:** {query1}
**CHILD QUESTION:** {query2}

**CRITICAL CHECKS:**
1. **CIRCULAR REASONING CHECK**: Do these questions form a circular reasoning loop?
   - Example BAD: Q1="Which company was founded in 2003?" A1="Tesla" â†’ Q2="What year was Tesla founded?" A2="2003"
   - Example BAD: Q1="What acceleration can Model S achieve?" A1="2.1 seconds" â†’ Q2="Which Tesla model has 2.1 second acceleration?" A2="Model S"

2. **CORRELATION CHECK**: Are the questions related in topic or context?
   - Questions must address **completely different domains**
   - No shared entities, attributes, or logical connections

**EVALUATION CRITERIA:**
1. **Circular Reasoning**: Would answering both questions create a logical loop?
2. **Topic Independence**: Do questions address completely different topics?
3. **Entity Overlap**: Do questions share the same entities or attributes?
4. **Logical Dependency**: Does one question's answer help answer the other?
5. **Domain Separation**: Are questions in completely different knowledge domains?

**FAILURE CONDITIONS (return false):**
- Any circular reasoning detected
- Shared entities with swapped attributes
- Questions that mutually inform each other
- Same knowledge domain with different angles

**Output Format (JSON):**
{{
    "has_circular_reasoning": true/false,
    "has_correlation": true/false,
    "correlation_score": 0.0-1.0,
    "circular_reasoning_explanation": "explanation if circular reasoning detected",
    "shared_entities": ["list", "of", "shared", "entities"],
    "validation_passed": true/false,
    "reasoning": "detailed explanation"
}}

**TARGET: Ensure questions are completely independent with NO circular reasoning.**"""

            response = self.api_client.generate_response(
                prompt=correlation_prompt,
                temperature=0.2,
                max_tokens=500
            )
            
            parsed_data = self._parse_json_response(response)
            if parsed_data:
                has_circular_reasoning = parsed_data.get('has_circular_reasoning', False)
                has_correlation = parsed_data.get('has_correlation', False)
                
                # å®‰å…¨åœ°è§£æcorrelation_score
                try:
                    correlation_score = float(parsed_data.get('correlation_score', 0.0))
                except (ValueError, TypeError):
                    correlation_score = 0.0
                
                validation_passed = parsed_data.get('validation_passed', True)
                
                # éªŒè¯é€šè¿‡æ¡ä»¶ï¼šæ— å¾ªç¯æ¨ç† ä¸” æ— å…³è” ä¸” ä½ç›¸å…³æ€§åˆ†æ•°
                no_issues = (not has_circular_reasoning and 
                           not has_correlation and 
                           correlation_score < 0.3 and 
                           validation_passed)
                
                if no_issues:
                    logger.info(f"âœ… æ— å…³è”å’Œå¾ªç¯éªŒè¯é€šè¿‡ (ç›¸å…³æ€§åˆ†æ•°: {correlation_score:.2f})")
                else:
                    if has_circular_reasoning:
                        logger.warning(f"âŒ æ£€æµ‹åˆ°å¾ªç¯æ¨ç†: {parsed_data.get('circular_reasoning_explanation', 'Unknown')}")
                    else:
                        logger.warning(f"âŒ æ£€æµ‹åˆ°å…³è”æ€§ (ç›¸å…³æ€§åˆ†æ•°: {correlation_score:.2f})")
                
                return no_issues
            else:
                # è§£æå¤±è´¥ï¼Œä¿å®ˆè¿”å›False
                logger.warning("æ— æ³•è§£æå…³è”æ€§éªŒè¯å“åº”")
                return False
                
        except Exception as e:
            logger.error(f"éªŒè¯å…³è”æ€§å¤±è´¥: {e}")
            return True  # éªŒè¯å¤±è´¥æ—¶ä¿å®ˆè¿”å›True
    
    def _detect_circular_reasoning(self, query1: str, query2: str) -> bool:
        """æ£€æµ‹å¾ªç¯æ¨ç†é£é™©"""
        try:
            # é¢„å¤„ç†
            q1_lower = query1.lower()
            q2_lower = query2.lower()
            
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶åˆ†è¯
            import re
            q1_words = set(re.findall(r'\b\w+\b', q1_lower))
            q2_words = set(re.findall(r'\b\w+\b', q2_lower))
            
            # æ‰©å±•çš„åœç”¨è¯è¡¨
            stop_words = {
                'what', 'which', 'who', 'when', 'where', 'how', 'why', 'the', 'a', 'an', 'and', 'or', 'but', 
                'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'was', 'are', 'were', 'can', 'could',
                'will', 'would', 'should', 'may', 'might', 'has', 'have', 'had', 'do', 'does', 'did', 'be',
                'been', 'being', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
                'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'
            }
            
            # æå–æœ‰æ„ä¹‰çš„è¯æ±‡
            q1_meaningful = q1_words - stop_words
            q2_meaningful = q2_words - stop_words
            
            # ç§»é™¤å•å­—æ¯è¯æ±‡
            q1_meaningful = {word for word in q1_meaningful if len(word) > 1}
            q2_meaningful = {word for word in q2_meaningful if len(word) > 1}
            
            # æ£€æŸ¥é‡å è¯æ±‡
            overlap = q1_meaningful & q2_meaningful
            
            # å¾ªç¯æ¨ç†æ¨¡å¼æ£€æµ‹
            circular_patterns = [
                # æ¨¡å¼1ï¼šå®ä½“-å±æ€§äº¤æ¢
                self._detect_entity_attribute_swap(q1_lower, q2_lower, overlap),
                
                # æ¨¡å¼2ï¼šæ—¶é—´-äº‹ä»¶äº¤æ¢  
                self._detect_temporal_event_swap(q1_lower, q2_lower, overlap),
                
                # æ¨¡å¼3ï¼šå› æœå…³ç³»é€†è½¬
                self._detect_cause_effect_reversal(q1_lower, q2_lower, overlap),
                
                # æ¨¡å¼4ï¼šé«˜é‡å ç‡ï¼ˆä¿ç•™åŸæœ‰æ£€æµ‹ï¼‰
                self._detect_high_overlap(q1_meaningful, q2_meaningful, overlap)
            ]
            
            # å¦‚æœä»»ä½•æ¨¡å¼æ£€æµ‹ä¸ºTrueï¼Œåˆ™è®¤ä¸ºå­˜åœ¨å¾ªç¯é£é™©
            has_circular = any(circular_patterns)
            
            if has_circular:
                logger.info(f"å¾ªç¯æ¨ç†æ£€æµ‹ï¼šé‡å è¯æ±‡ {overlap}, æ¨¡å¼æ£€æµ‹ç»“æœ {circular_patterns}")
            
            return has_circular
            
        except Exception as e:
            logger.error(f"å¾ªç¯æ¨ç†æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def _detect_entity_attribute_swap(self, q1: str, q2: str, overlap: set) -> bool:
        """æ£€æµ‹å®ä½“-å±æ€§äº¤æ¢æ¨¡å¼"""
        # å®ä½“è¯æ±‡ï¼ˆé€šå¸¸æ˜¯ä¸“æœ‰åè¯ï¼‰
        entity_indicators = ['tesla', 'spacex', 'apple', 'google', 'microsoft', 'amazon', 'meta', 'nvidia', 
                           'model', 'iphone', 'galaxy', 'company', 'corporation', 'manufacturer']
        
        # å±æ€§è¯æ±‡
        attribute_indicators = ['founded', 'established', 'created', 'built', 'year', 'when', 'where', 'located', 
                              'headquartered', 'based', 'price', 'cost', 'acceleration', 'speed', 'capacity',
                              'produces', 'makes', 'manufactures', 'electric', 'vehicle', 'product', 'type']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®ä½“å’Œå±æ€§çš„äº¤æ¢
        q1_has_entity = any(entity in q1 for entity in entity_indicators)
        q2_has_entity = any(entity in q2 for entity in entity_indicators)
        q1_has_attribute = any(attr in q1 for attr in attribute_indicators)
        q2_has_attribute = any(attr in q2 for attr in attribute_indicators)
        
        # ç‰¹æ®Šæ£€æµ‹ï¼šäº§å“-åˆ¶é€ å•†å¾ªç¯
        if self._detect_product_manufacturer_cycle(q1, q2, overlap):
            return True
        
        # ç‰¹æ®Šæ£€æµ‹ï¼šåœ°ç‚¹-å®ä½“å¾ªç¯
        if self._detect_location_entity_cycle(q1, q2, overlap):
            return True
        
        # å¦‚æœä¸¤ä¸ªé—®é¢˜éƒ½æ¶‰åŠå®ä½“å’Œå±æ€§ï¼Œä¸”æœ‰é‡å ï¼Œå¯èƒ½æ˜¯äº¤æ¢æ¨¡å¼
        if (q1_has_entity and q2_has_entity and q1_has_attribute and q2_has_attribute and 
            len(overlap) >= 1):  # é™ä½é˜ˆå€¼ä»2åˆ°1
            return True
        
        return False
    
    def _detect_temporal_event_swap(self, q1: str, q2: str, overlap: set) -> bool:
        """æ£€æµ‹æ—¶é—´-äº‹ä»¶äº¤æ¢æ¨¡å¼"""
        time_indicators = ['year', 'when', 'date', '19', '20', 'founded', 'established', 'created']
        event_indicators = ['company', 'organization', 'founded', 'established', 'event', 'happened']
        
        # æ£€æŸ¥æ—¶é—´ç›¸å…³çš„é—®é¢˜æ¨¡å¼
        q1_has_time = any(time_word in q1 for time_word in time_indicators)
        q2_has_time = any(time_word in q2 for time_word in time_indicators)
        q1_has_event = any(event_word in q1 for event_word in event_indicators)
        q2_has_event = any(event_word in q2 for event_word in event_indicators)
        
        # ç»å…¸æ—¶é—´-äº‹ä»¶å¾ªç¯æ¨¡å¼
        if ((q1_has_time and q2_has_time) or (q1_has_event and q2_has_event)) and len(overlap) >= 1:
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æ˜¯"ä»€ä¹ˆæ—¶å€™å‘ç”Ÿ"å’Œ"ä»€ä¹ˆåœ¨æŸæ—¶é—´å‘ç”Ÿ"çš„æ¨¡å¼
            if (('when' in q1 or 'year' in q1) and ('founded' in q2 or 'established' in q2)) or \
               (('when' in q2 or 'year' in q2) and ('founded' in q1 or 'established' in q1)):
                return True
        
        return False
    
    def _detect_cause_effect_reversal(self, q1: str, q2: str, overlap: set) -> bool:
        """æ£€æµ‹å› æœå…³ç³»é€†è½¬æ¨¡å¼"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä¸»è¦æ£€æŸ¥æ˜æ˜¾çš„é€†å‘å…³ç³»
        reversal_patterns = [
            ('produces', 'made by'),
            ('makes', 'manufactured by'),  
            ('owns', 'owned by'),
            ('creates', 'created by'),
            ('develops', 'developed by')
        ]
        
        for forward, reverse in reversal_patterns:
            if ((forward in q1 and reverse in q2) or (reverse in q1 and forward in q2)) and len(overlap) >= 1:
                return True
        
        return False
    
    def _detect_high_overlap(self, q1_meaningful: set, q2_meaningful: set, overlap: set) -> bool:
        """æ£€æµ‹é«˜é‡å ç‡"""
        if len(q1_meaningful) == 0 or len(q2_meaningful) == 0:
            return False
        
        overlap_ratio = len(overlap) / max(len(q1_meaningful), len(q2_meaningful))
        
        # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿåœ°æ£€æµ‹é‡å 
        if overlap_ratio > 0.4:  # ä»0.6é™ä½åˆ°0.4
            return True
        
        # ç‰¹åˆ«æ£€æŸ¥ï¼šå¦‚æœæœ‰3ä¸ªæˆ–æ›´å¤šé‡å çš„å…³é”®è¯ï¼Œä¹Ÿè®¤ä¸ºæ˜¯é«˜é£é™©
        if len(overlap) >= 3:
            return True
        
        return False
    
    def _detect_product_manufacturer_cycle(self, q1: str, q2: str, overlap: set) -> bool:
        """æ£€æµ‹äº§å“-åˆ¶é€ å•†å¾ªç¯"""
        # äº§å“ç›¸å…³è¯æ±‡
        product_words = ['model', 'product', 'vehicle', 'car', 'phone', 'device', 'electric']
        
        # åˆ¶é€ å•†ç›¸å…³è¯æ±‡
        manufacturer_words = ['company', 'manufacturer', 'makes', 'produces', 'tesla', 'apple', 'google']
        
        # æ£€æµ‹æ¨¡å¼ï¼šé—®Aå…¬å¸ç”Ÿäº§ä»€ä¹ˆ vs é—®ä»€ä¹ˆäº§å“ç”±Aå…¬å¸ç”Ÿäº§
        q1_has_product = any(word in q1 for word in product_words)
        q2_has_product = any(word in q2 for word in product_words)
        q1_has_manufacturer = any(word in q1 for word in manufacturer_words)
        q2_has_manufacturer = any(word in q2 for word in manufacturer_words)
        
        # å¦‚æœä¸¤ä¸ªé—®é¢˜éƒ½æ¶‰åŠäº§å“å’Œåˆ¶é€ å•†å…³ç³»ï¼Œä¸”æœ‰é‡å å®ä½“
        if ((q1_has_product and q2_has_manufacturer) or (q1_has_manufacturer and q2_has_product)) and len(overlap) >= 1:
            return True
        
        return False
    
    def _detect_location_entity_cycle(self, q1: str, q2: str, overlap: set) -> bool:
        """æ£€æµ‹åœ°ç‚¹-å®ä½“å¾ªç¯"""
        # åœ°ç‚¹ç›¸å…³è¯æ±‡
        location_words = ['where', 'located', 'headquartered', 'based', 'headquarters', 'austin', 'california', 'texas']
        
        # å®ä½“ç›¸å…³è¯æ±‡
        entity_words = ['company', 'organization', 'tesla', 'spacex', 'google', 'apple']
        
        # æ£€æµ‹æ¨¡å¼ï¼šé—®å®ä½“åœ¨å“ªé‡Œ vs é—®ä»€ä¹ˆå®ä½“åœ¨æŸåœ°
        q1_has_location = any(word in q1 for word in location_words)
        q2_has_location = any(word in q2 for word in location_words)
        q1_has_entity = any(word in q1 for word in entity_words)
        q2_has_entity = any(word in q2 for word in entity_words)
        
        # å¦‚æœä¸¤ä¸ªé—®é¢˜éƒ½æ¶‰åŠåœ°ç‚¹å’Œå®ä½“å…³ç³»ï¼Œä¸”æœ‰é‡å 
        if ((q1_has_location and q2_has_entity) or (q1_has_entity and q2_has_location)) and len(overlap) >= 1:
            return True
        
        return False
    
    def _build_second_layer_extensions(self, tree: AgentReasoningTree, parent_node: QuestionTreeNode):
        """æ„å»ºç¬¬äºŒå±‚æ‰©å±•"""
        try:
            logger.info(f"å¼€å§‹æ„å»ºç¬¬äºŒå±‚æ‰©å±•ï¼Œçˆ¶èŠ‚ç‚¹: {parent_node.question[:50]}...")
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¾¾åˆ°æœ€å¤§æ·±åº¦
            if parent_node.layer >= 2:  # é™åˆ¶æœ€å¤š3å±‚ï¼ˆ0, 1, 2ï¼‰
                logger.info("å·²è¾¾åˆ°æœ€å¤§æ‰©å±•æ·±åº¦ï¼Œè·³è¿‡ç¬¬äºŒå±‚æ‰©å±•")
                return
            
            # ä¸ºçˆ¶èŠ‚ç‚¹çš„æ¯ä¸ªå…³é”®è¯å°è¯•åˆ›å»ºSeriesæ‰©å±•
            for keyword in parent_node.keywords:
                try:
                    series_query = self._step3_create_series_extension(
                        keyword=keyword,
                        parent_query=parent_node,
                        layer=parent_node.layer + 1,
                        tree_id=tree.tree_id
                    )
                    
                    if series_query:
                        # åˆ›å»ºå­èŠ‚ç‚¹
                        child_node = QuestionTreeNode(
                            question=series_query.query_text,
                            answer=series_query.answer,
                            keywords=series_query.minimal_keywords,
                            layer=parent_node.layer + 1,
                            parent=parent_node,
                            generation_method=series_query.generation_method
                        )
                        
                        # æ·»åŠ åˆ°çˆ¶èŠ‚ç‚¹çš„å­èŠ‚ç‚¹åˆ—è¡¨
                        parent_node.children.append(child_node)
                        
                        logger.info(f"âœ… æˆåŠŸåˆ›å»ºç¬¬äºŒå±‚Seriesæ‰©å±•: {series_query.query_text[:50]}...")
                        
                        # é™åˆ¶æ¯ä¸ªçˆ¶èŠ‚ç‚¹æœ€å¤š2ä¸ªå­æ‰©å±•ï¼Œé¿å…è¿‡åº¦å¤æ‚
                        if len(parent_node.children) >= 2:
                            break
                            
                except Exception as e:
                    logger.error(f"åˆ›å»ºç¬¬äºŒå±‚Seriesæ‰©å±•å¤±è´¥ (å…³é”®è¯: {keyword.keyword}): {e}")
                    continue
            
            logger.info(f"ç¬¬äºŒå±‚æ‰©å±•å®Œæˆï¼Œç”Ÿæˆäº† {len(parent_node.children)} ä¸ªå­èŠ‚ç‚¹")
            
        except Exception as e:
            logger.error(f"æ„å»ºç¬¬äºŒå±‚æ‰©å±•å¤±è´¥: {e}")
    

    
    def _generate_simple_nested_query(self, queries: List[str], root_answer: str) -> str:
        """ç”Ÿæˆç®€å•çš„åµŒå¥—é—®é¢˜ä½œä¸ºåå¤‡ - ç»ä¸åŒ…å«ç­”æ¡ˆä¿¡æ¯"""
        if not queries:
            return "What is the final answer that requires multi-step reasoning to determine?"
        
        # é€‰æ‹©å‰3ä¸ªé—®é¢˜æ„å»ºç®€å•åµŒå¥—ï¼Œç¡®ä¿ä¸æ³„éœ²ç­”æ¡ˆ
        selected_queries = queries[:3]
        if len(selected_queries) == 1:
            return f"To determine the final answer, consider: {selected_queries[0]}"
        elif len(selected_queries) == 2:
            return f"Given that {selected_queries[0]}, and considering {selected_queries[1]}, what is the final answer?"
        else:
            return f"To identify the target answer, first determine {selected_queries[0]}, then analyze {selected_queries[1]}, and finally evaluate {selected_queries[2]}. What is the final answer?"
    
    def _calculate_complexity_score(self, composite_query: str) -> float:
        """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
        try:
            # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            length_score = min(len(composite_query) / 500, 1.0)  # é•¿åº¦å› å­
            
            # åµŒå¥—å±‚æ¬¡åˆ†æ
            nesting_indicators = ['given that', 'considering', 'first determine', 'then analyze', 'finally']
            nesting_score = sum(1 for indicator in nesting_indicators if indicator in composite_query.lower()) / len(nesting_indicators)
            
            # æ¨ç†å…³é”®è¯åˆ†æ
            reasoning_keywords = ['requires', 'analyze', 'determine', 'evaluate', 'identify', 'consider']
            reasoning_score = sum(1 for keyword in reasoning_keywords if keyword in composite_query.lower()) / len(reasoning_keywords)
            
            # ç»¼åˆè®¡ç®—
            complexity = (length_score * 0.3 + nesting_score * 0.4 + reasoning_score * 0.3)
            
            return min(max(complexity, 0.1), 1.0)  # é™åˆ¶åœ¨0.1-1.0èŒƒå›´å†…
            
        except Exception as e:
            logger.error(f"è®¡ç®—å¤æ‚åº¦åˆ†æ•°å¤±è´¥: {e}")
            return 0.8
    
    def _parse_json_response(self, response: str) -> Optional[Dict]:
        """è§£æJSONå“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except json.JSONDecodeError:
            try:
                # å°è¯•æå–JSONéƒ¨åˆ†
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    return json.loads(json_str)
            except:
                pass
            
            try:
                # å°è¯•æŸ¥æ‰¾å¹¶ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                import re
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                cleaned = re.sub(r'```json\s*|\s*```', '', response)
                cleaned = cleaned.strip()
                
                # æ‰¾åˆ°æœ€å¤–å±‚çš„å¤§æ‹¬å·
                json_start = cleaned.find('{')
                json_end = cleaned.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = cleaned[json_start:json_end]
                    return json.loads(json_str)
            except:
                pass
            
            logger.warning(f"æ— æ³•è§£æJSONå“åº”: {response[:200]}...")
            return None
    
    def _classify_keyword_type(self, keyword: str) -> str:
        """åˆ†ç±»å…³é”®è¯ç±»å‹"""
        keyword_lower = keyword.lower()
        
        # æ•°å­—æ£€æµ‹
        if any(char.isdigit() for char in keyword):
            return "number"
        
        # æŠ€æœ¯æœ¯è¯­æ£€æµ‹ï¼ˆåŒ…å«ç‰¹å®šåç¼€ï¼‰
        tech_suffixes = ['algorithm', 'protocol', 'system', 'technology', 'method']
        if any(suffix in keyword_lower for suffix in tech_suffixes):
            return "technical_term"
        
        # æ—¥æœŸæ£€æµ‹
        date_indicators = ['year', 'date', '20', '19']
        if any(indicator in keyword_lower for indicator in date_indicators):
            return "date"
        
        # åœ°ç‚¹æ£€æµ‹
        location_indicators = ['city', 'country', 'location', 'university', 'center']
        if any(indicator in keyword_lower for indicator in location_indicators):
            return "location"
        
        # ä¸“æœ‰åè¯æ£€æµ‹ï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
        if keyword and keyword[0].isupper():
            return "proper_noun"
        
        return "noun"
    
    def _optimize_minimal_keywords(self, question_text: str, answer: str, initial_keywords: List[str]) -> List[str]:
        """ä¼˜åŒ–æœ€å°å…³é”®è¯åˆ—è¡¨"""
        if not initial_keywords:
            return []
        
        # ç®€å•çš„ä¼˜åŒ–ï¼šç§»é™¤è¿‡äºé€šç”¨çš„è¯
        generic_words = {'the', 'a', 'an', 'is', 'was', 'are', 'were', 'what', 'which', 'how', 'system', 'method'}
        
        optimized = []
        for keyword in initial_keywords:
            if keyword.lower() not in generic_words and len(keyword) > 2:
                optimized.append(keyword)
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå…³é”®è¯
        if not optimized and initial_keywords:
            optimized = [initial_keywords[0]]
        
        return optimized[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
    
    def _validate_root_query(self, question_text: str, answer: str, keywords: List[MinimalKeyword]) -> bool:
        """éªŒè¯Root Queryçš„è´¨é‡"""
        try:
            # åŸºæœ¬éªŒè¯
            if not question_text or not answer or not keywords:
                return False
            
            # é•¿åº¦æ£€æŸ¥
            if len(question_text) < 10 or len(answer) < 2:
                return False
            
            # å…³é”®è¯æ•°é‡æ£€æŸ¥
            if len(keywords) < 1:
                return False
            
            # ç­”æ¡ˆåŒ…å«æ£€æŸ¥ï¼ˆç­”æ¡ˆåº”è¯¥ä¸åœ¨é—®é¢˜ä¸­ç›´æ¥å‡ºç°ï¼‰
            if answer.lower() in question_text.lower():
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯Root Queryå¤±è´¥: {e}")
            return False
    
    def _validate_unrelated_query(self, question_text: str, answer: str, keywords: List[MinimalKeyword]) -> bool:
        """éªŒè¯æ— å…³è”é—®é¢˜çš„è´¨é‡"""
        try:
            # å¤ç”¨Root QueryéªŒè¯é€»è¾‘
            basic_validation = self._validate_root_query(question_text, answer, keywords)
            
            if not basic_validation:
                return False
            
            # é¢å¤–çš„æ— å…³è”æ€§æ£€æŸ¥
            # ç¡®ä¿é—®é¢˜ä¸­åŒ…å«ç­”æ¡ˆç›¸å…³çš„å…³é”®è¯
            answer_in_question = any(answer.lower() in kw.keyword.lower() or kw.keyword.lower() in answer.lower() 
                                   for kw in keywords)
            
            return not answer_in_question  # ç­”æ¡ˆä¸åº”è¯¥ç›´æ¥å‡ºç°åœ¨å…³é”®è¯ä¸­
            
        except Exception as e:
            logger.error(f"éªŒè¯æ— å…³è”é—®é¢˜å¤±è´¥: {e}")
            return False
    
    def _build_second_layer_extensions(self, tree: AgentReasoningTree, parent_node: QuestionTreeNode):
        """æ„å»ºç¬¬äºŒå±‚æ‰©å±•"""
        try:
            logger.info(f"ä¸ºèŠ‚ç‚¹ {parent_node.node_id} æ„å»ºç¬¬äºŒå±‚æ‰©å±•")
            
            # æå–çˆ¶èŠ‚ç‚¹çš„å…³é”®è¯
            parent_keywords = self._step2_extract_minimal_keywords(parent_node.query)
            
            if not parent_keywords:
                logger.warning(f"æ— æ³•ä¸ºèŠ‚ç‚¹ {parent_node.node_id} æå–å…³é”®è¯")
                return
            
            # ä¸ºç¬¬ä¸€ä¸ªå…³é”®è¯åˆ›å»ºSeriesæ‰©å±•
            if parent_keywords:
                series_query = self._step3_create_series_extension(
                    parent_node.query, parent_keywords[0], layer=2, 
                    tree_id=f"{tree.tree_id}_series2"
                )
                
                if series_query:
                    series_node = QuestionTreeNode(
                        node_id=f"{parent_node.node_id}_series2",
                        query=series_query,
                        parent_id=parent_node.node_id,
                        layer=2,
                        branch_type="series"
                    )
                    tree.all_nodes[series_node.node_id] = series_node
                    parent_node.children_ids.append(series_node.node_id)
                    self.stats['series_extensions_created'] += 1
            
            logger.info(f"âœ… ç¬¬äºŒå±‚æ‰©å±•æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ„å»ºç¬¬äºŒå±‚æ‰©å±•å¤±è´¥: {e}") 

    # ============ æ–°å¢ä¼˜åŒ–æ–¹æ³• (åŸºäºç”¨æˆ·æ–°è®¾è®¡è¦æ±‚) ============

    def _optimize_minimal_keywords_precisely(self, question_text: str, answer: str, initial_keywords: List[str]) -> List[str]:
        """
        ç²¾ç¡®çš„å…³é”®è¯æœ€å°åŒ–ç®—æ³• - åŸºäºç”¨æˆ·æ–°è®¾è®¡è¦æ±‚
        
        è¦æ±‚ï¼šå¦‚æœç§»é™¤æŸä¸ªkeywordä¹Ÿèƒ½å¾—åˆ°answerï¼Œå°±ç›´æ¥ç§»é™¤è¿™ä¸ªkeywordï¼Œ
        ä¿ç•™å¦ä¸€ä¸ªkeywordï¼Œæœ€åçš„ä¸€ä¸ªkeywordçš„é—®é¢˜æ‰æ˜¯æˆ‘ä»¬è¦çš„æœ€å°ã€æœ€ç²¾ç¡®é—®é¢˜ã€‚
        """
        if not self.api_client or not initial_keywords:
            return self._optimize_minimal_keywords(question_text, answer, initial_keywords)
        
        try:
            logger.info(f"å¼€å§‹ç²¾ç¡®å…³é”®è¯æœ€å°åŒ–æµ‹è¯•: {len(initial_keywords)} ä¸ªå€™é€‰å…³é”®è¯")
            
            essential_keywords = []
            
            for keyword in initial_keywords:
                # æ„é€ ç§»é™¤è¯¥å…³é”®è¯çš„é—®é¢˜
                modified_question = question_text.replace(keyword, "[MASKED]")
                
                # æµ‹è¯•å‰©ä½™å…³é”®è¯æ˜¯å¦ä»èƒ½å”¯ä¸€ç¡®å®šç­”æ¡ˆ
                can_still_determine = self._test_answer_determination_without_keyword(
                    modified_question, answer, keyword, [k for k in initial_keywords if k != keyword]
                )
                
                if not can_still_determine:
                    # ç§»é™¤è¯¥å…³é”®è¯ä¼šå½±å“ç­”æ¡ˆçš„å”¯ä¸€æ€§ï¼Œå› æ­¤æ˜¯å¿…è¦çš„
                    essential_keywords.append(keyword)
                    logger.info(f"âœ… å…³é”®è¯ '{keyword}' æ˜¯å¿…è¦çš„ - ç§»é™¤åæ— æ³•å”¯ä¸€ç¡®å®šç­”æ¡ˆ")
                else:
                    logger.info(f"âŒ å…³é”®è¯ '{keyword}' éå¿…è¦ - ç§»é™¤åä»èƒ½ç¡®å®šç­”æ¡ˆ")
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå…³é”®è¯
            if not essential_keywords and initial_keywords:
                essential_keywords = [initial_keywords[0]]
                logger.info(f"âš ï¸ ä¿ç•™ç¬¬ä¸€ä¸ªå…³é”®è¯ä½œä¸ºæœ€å°å¿…è¦å…³é”®è¯: {initial_keywords[0]}")
            
            logger.info(f"âœ… ç²¾ç¡®æœ€å°åŒ–å®Œæˆ: {len(essential_keywords)}/{len(initial_keywords)} ä¸ªå…³é”®è¯æ˜¯å¿…è¦çš„")
            return essential_keywords
            
        except Exception as e:
            logger.error(f"ç²¾ç¡®å…³é”®è¯æœ€å°åŒ–å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰æ–¹æ³•
            return self._optimize_minimal_keywords(question_text, answer, initial_keywords)
    
    def _test_answer_determination_without_keyword(
        self, modified_question: str, target_answer: str, removed_keyword: str, remaining_keywords: List[str]
    ) -> bool:
        """
        æµ‹è¯•ç§»é™¤æŸä¸ªå…³é”®è¯åï¼Œå‰©ä½™å…³é”®è¯æ˜¯å¦ä»èƒ½å”¯ä¸€ç¡®å®šç­”æ¡ˆ
        
        Args:
            modified_question: ç§»é™¤å…³é”®è¯åçš„é—®é¢˜
            target_answer: ç›®æ ‡ç­”æ¡ˆ
            removed_keyword: è¢«ç§»é™¤çš„å…³é”®è¯
            remaining_keywords: å‰©ä½™çš„å…³é”®è¯
            
        Returns:
            True: ä»èƒ½å”¯ä¸€ç¡®å®šç­”æ¡ˆï¼ˆè¯´æ˜è¢«ç§»é™¤çš„å…³é”®è¯éå¿…è¦ï¼‰
            False: æ— æ³•å”¯ä¸€ç¡®å®šç­”æ¡ˆï¼ˆè¯´æ˜è¢«ç§»é™¤çš„å…³é”®è¯æ˜¯å¿…è¦çš„ï¼‰
        """
        if not self.api_client:
            return False
        
        try:
            test_prompt = f"""**TASK: Test if remaining keywords can uniquely determine the answer after keyword removal.**

**ORIGINAL QUESTION:** {modified_question.replace('[MASKED]', removed_keyword)}
**MODIFIED QUESTION (keyword masked):** {modified_question}
**REMOVED KEYWORD:** {removed_keyword}
**REMAINING KEYWORDS:** {', '.join(remaining_keywords)}
**TARGET ANSWER:** {target_answer}

**EVALUATION CRITERIA:**
1. Can the MODIFIED QUESTION with remaining keywords still **uniquely identify** the answer "{target_answer}"?
2. Are there **multiple possible answers** when "{removed_keyword}" is removed?
3. Does the removal create **ambiguity** or **uncertainty**?

**TEST SCENARIOS:**
- If removing "{removed_keyword}" allows other valid answers besides "{target_answer}" â†’ Answer: "multiple_answers"
- If the modified question becomes unclear or ambiguous â†’ Answer: "ambiguous"  
- If the remaining keywords still uniquely point to "{target_answer}" â†’ Answer: "still_unique"

**Output Format (JSON):**
{{
    "can_still_determine": true/false,
    "determination_level": "still_unique/ambiguous/multiple_answers",
    "alternative_answers": ["list", "of", "other", "possible", "answers"],
    "reasoning": "detailed explanation of the determination test",
    "necessity_of_removed_keyword": "essential/helpful/redundant"
}}

**TARGET: Determine if "{removed_keyword}" is essential for unique answer identification.**"""

            response = self.api_client.generate_response(
                prompt=test_prompt,
                temperature=0.2,  # ä½æ¸©åº¦ç¡®ä¿å®¢è§‚åˆ¤æ–­
                max_tokens=400
            )
            
            parsed_data = self._parse_json_response(response)
            if parsed_data:
                can_still_determine = parsed_data.get('can_still_determine', False)
                determination_level = parsed_data.get('determination_level', 'ambiguous')
                
                # åªæœ‰åœ¨"still_unique"çš„æƒ…å†µä¸‹æ‰è®¤ä¸ºå¯ä»¥ç§»é™¤è¯¥å…³é”®è¯
                return can_still_determine and determination_level == 'still_unique'
            else:
                # è§£æå¤±è´¥æ—¶ä¿å®ˆå¤„ç†ï¼Œè®¤ä¸ºå…³é”®è¯æ˜¯å¿…è¦çš„
                return False
                
        except Exception as e:
            logger.error(f"æµ‹è¯•å…³é”®è¯å¿…è¦æ€§å¤±è´¥: {e}")
            return False
    
    def _record_detailed_trajectory_enhanced(self, step: str, **kwargs):
        """
        å¢å¼ºçš„è½¨è¿¹è®°å½• - åŸºäºç”¨æˆ·æ–°è®¾è®¡è¦æ±‚
        
        è¯¦ç»†è®°å½•æ¯ä¸ªé˜¶æ®µçš„å±‚çº§ã€å…³é”®è¯ã€ä¸Šä¸€å±‚é—®é¢˜ã€ç­”æ¡ˆã€keywordsç­‰ä¿¡æ¯
        """
        try:
            trajectory_entry = {
                'timestamp': time.time(),
                'step': step,
                'layer_level': kwargs.get('layer_level', 0),
                'current_keywords': kwargs.get('current_keywords', kwargs.get('keywords', [])),
                'keyword_count': len(kwargs.get('current_keywords', kwargs.get('keywords', []))),
                'parent_question': kwargs.get('parent_question', ''),
                'parent_answer': kwargs.get('parent_answer', ''),
                'parent_keywords': kwargs.get('parent_keywords', []),
                'current_question': kwargs.get('current_question', ''),
                'current_answer': kwargs.get('current_answer', ''),
                'generation_method': kwargs.get('generation_method', ''),
                'validation_results': kwargs.get('validation_results', {}),
                'keyword_necessity_scores': kwargs.get('keyword_necessity_scores', {}),
                'circular_check_result': kwargs.get('circular_check', 'passed'),
                'no_correlation_verified': kwargs.get('no_correlation_verified', False),
                'processing_time_ms': kwargs.get('processing_time', 0) * 1000,
                'tree_id': kwargs.get('tree_id', ''),
                'query_id': kwargs.get('query_id', ''),
                'extension_type': kwargs.get('extension_type', ''),  # root, series, parallel
                'api_calls_count': kwargs.get('api_calls_count', kwargs.get('api_calls', 1)),  # é»˜è®¤1æ¬¡APIè°ƒç”¨
                'search_queries_used': kwargs.get('search_queries', []),
                'quality_metrics': {
                    'question_length': len(kwargs.get('current_question', '')),
                    'answer_length': len(kwargs.get('current_answer', '')),
                    'complexity_score': kwargs.get('complexity_score', 0),
                    'uniqueness_verified': kwargs.get('uniqueness_verified', False)
                }
            }
            
            # æ·»åŠ é˜¶æ®µç‰¹å®šçš„ä¿¡æ¯
            if step == 'step1_root_query_generation':
                trajectory_entry['short_answer_info'] = kwargs.get('short_answer_info', {})
                trajectory_entry['web_search_context'] = kwargs.get('web_search_context', '')
                
            elif step == 'step2_keyword_extraction':
                trajectory_entry['candidate_keywords'] = kwargs.get('candidate_keywords', [])
                trajectory_entry['masking_test_results'] = kwargs.get('masking_test_results', {})
                trajectory_entry['minimal_keywords_found'] = kwargs.get('minimal_keywords_found', [])
                
            elif step in ['step3_series_extension', 'step4_parallel_extension']:
                trajectory_entry['target_keyword'] = kwargs.get('target_keyword', '')
                trajectory_entry['search_context'] = kwargs.get('search_context', '')
                trajectory_entry['correlation_check'] = kwargs.get('correlation_check', {})
                
            elif step == 'step6_composite_query':
                trajectory_entry['queries_by_layer'] = kwargs.get('queries_by_layer', {})
                trajectory_entry['composite_formats'] = kwargs.get('composite_formats', {})
                trajectory_entry['reasoning_chain_length'] = kwargs.get('reasoning_chain_length', 0)
            
            self.trajectory_records.append(trajectory_entry)
            
            # åŒæ—¶è®°å½•åˆ°åŸæœ‰ç³»ç»Ÿä¸­ä¿æŒå…¼å®¹æ€§
            self._record_trajectory(trajectory_entry)
            
        except Exception as e:
            logger.error(f"å¢å¼ºè½¨è¿¹è®°å½•å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰è®°å½•æ–¹æ³•
            self._record_trajectory({
                'step': step,
                'error': str(e),
                'kwargs': str(kwargs)
            })
    
    def _validate_strict_no_correlation(self, parent_questions: List[str], new_question: str, target_layer: int) -> bool:
        """
        ä¸¥æ ¼çš„æ— å…³è”æ€§éªŒè¯ - åŸºäºç”¨æˆ·æ–°è®¾è®¡è¦æ±‚
        
        ç¡®ä¿ä»»æ„å±‚çš„queryé—®é¢˜ï¼Œäº’ç›¸å±‚éƒ½ä¸èƒ½æœ‰ä»»ä½•å…³è”ï¼Œåªæœ‰parallelçš„æœ‰å…³è”
        """
        if not parent_questions or not new_question:
            return True
        
        try:
            for parent_q in parent_questions:
                # 1. å…³é”®è¯é‡å æ£€æµ‹
                if self._detect_keyword_overlap(parent_q, new_question):
                    logger.warning(f"æ£€æµ‹åˆ°å…³é”®è¯é‡å : '{parent_q}' vs '{new_question}'")
                    return False
                
                # 2. ä¸»é¢˜åŸŸæ£€æµ‹
                if self._detect_same_knowledge_domain(parent_q, new_question):
                    logger.warning(f"æ£€æµ‹åˆ°ç›¸åŒçŸ¥è¯†åŸŸ: '{parent_q}' vs '{new_question}'")
                    return False
                
                # 3. è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹
                similarity_score = self._calculate_semantic_similarity(parent_q, new_question)
                if similarity_score > 0.3:  # é˜ˆå€¼å¯è°ƒ
                    logger.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡é«˜ ({similarity_score:.2f}): '{parent_q}' vs '{new_question}'")
                    return False
                
                # 4. é€»è¾‘ä¾èµ–æ£€æµ‹
                if self._detect_logical_dependency(parent_q, new_question):
                    logger.warning(f"æ£€æµ‹åˆ°é€»è¾‘ä¾èµ–: '{parent_q}' vs '{new_question}'")
                    return False
            
            logger.info(f"âœ… æ— å…³è”æ€§éªŒè¯é€šè¿‡: Layer {target_layer}")
            return True
            
        except Exception as e:
            logger.error(f"æ— å…³è”æ€§éªŒè¯å¤±è´¥: {e}")
            return False  # ä¿å®ˆå¤„ç†
    
    def _detect_keyword_overlap(self, question1: str, question2: str) -> bool:
        """æ£€æµ‹ä¸¤ä¸ªé—®é¢˜ä¹‹é—´çš„å…³é”®è¯é‡å """
        try:
            keywords1 = self._extract_keywords_simple(question1)
            keywords2 = self._extract_keywords_simple(question2)
            
            # è®¡ç®—é‡å ç‡
            overlap = set(keywords1) & set(keywords2)
            total_unique = set(keywords1) | set(keywords2)
            
            if len(total_unique) == 0:
                return False
            
            overlap_ratio = len(overlap) / len(total_unique)
            return overlap_ratio > 0.2  # 20%ä»¥ä¸Šé‡å è®¤ä¸ºæœ‰å…³è”
            
        except Exception as e:
            logger.error(f"å…³é”®è¯é‡å æ£€æµ‹å¤±è´¥: {e}")
            return True  # ä¿å®ˆå¤„ç†
    
    def _extract_keywords_simple(self, text: str) -> List[str]:
        """ç®€å•çš„å…³é”®è¯æå–"""
        import re
        
        # ç§»é™¤å¸¸è§è¯æ±‡
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'was', 'are', 'were', 'what', 'when', 'where', 'who', 'which', 'how', 'why',
            'this', 'that', 'these', 'those', 'can', 'could', 'should', 'would', 'will', 'shall'
        }
        
        # æå–æœ‰æ„ä¹‰çš„è¯æ±‡ï¼ˆè‡³å°‘3ä¸ªå­—ç¬¦ï¼Œä¸åœ¨åœç”¨è¯ä¸­ï¼‰
        words = re.findall(r'\b\w{3,}\b', text.lower())
        keywords = [word for word in words if word not in stop_words]
        
        return keywords
    
    def _detect_same_knowledge_domain(self, question1: str, question2: str) -> bool:
        """æ£€æµ‹æ˜¯å¦å±äºç›¸åŒçŸ¥è¯†åŸŸ"""
        try:
            # å®šä¹‰çŸ¥è¯†åŸŸå…³é”®è¯
            domains = {
                'technology': ['software', 'computer', 'algorithm', 'programming', 'system', 'data', 'digital'],
                'business': ['company', 'corporation', 'market', 'sales', 'revenue', 'profit', 'industry'],
                'science': ['research', 'study', 'experiment', 'analysis', 'theory', 'hypothesis', 'method'],
                'geography': ['country', 'city', 'location', 'region', 'area', 'place', 'territory'],
                'history': ['year', 'century', 'period', 'era', 'ancient', 'historical', 'past'],
                'medicine': ['health', 'medical', 'disease', 'treatment', 'hospital', 'doctor', 'patient']
            }
            
            q1_lower = question1.lower()
            q2_lower = question2.lower()
            
            for domain, keywords in domains.items():
                q1_matches = sum(1 for kw in keywords if kw in q1_lower)
                q2_matches = sum(1 for kw in keywords if kw in q2_lower)
                
                # å¦‚æœä¸¤ä¸ªé—®é¢˜éƒ½æœ‰è¯¥åŸŸçš„å¤šä¸ªå…³é”®è¯ï¼Œè®¤ä¸ºå±äºåŒä¸€åŸŸ
                if q1_matches >= 2 and q2_matches >= 2:
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†åŸŸæ£€æµ‹å¤±è´¥: {e}")
            return True  # ä¿å®ˆå¤„ç†
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆTF-IDFï¼‰"""
        try:
            from collections import Counter
            import math
            
            # ç®€å•çš„è¯é¢‘ç»Ÿè®¡
            words1 = self._extract_keywords_simple(text1)
            words2 = self._extract_keywords_simple(text2)
            
            if not words1 or not words2:
                return 0.0
            
            # è®¡ç®—è¯é¢‘
            freq1 = Counter(words1)
            freq2 = Counter(words2)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            all_words = set(words1) | set(words2)
            
            vec1 = [freq1.get(word, 0) for word in all_words]
            vec2 = [freq2.get(word, 0) for word in all_words]
            
            # ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = math.sqrt(sum(a * a for a in vec1))
            magnitude2 = math.sqrt(sum(a * a for a in vec2))
            
            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0
            
            similarity = dot_product / (magnitude1 * magnitude2)
            return similarity
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 1.0  # ä¿å®ˆå¤„ç†ï¼Œè®¤ä¸ºç›¸ä¼¼
    
    def _detect_logical_dependency(self, question1: str, question2: str) -> bool:
        """æ£€æµ‹é€»è¾‘ä¾èµ–å…³ç³»"""
        try:
            q1_lower = question1.lower()
            q2_lower = question2.lower()
            
            # æ£€æµ‹æ—¶é—´ä¾èµ–
            time_patterns = [
                r'\b(19|20)\d{2}\b',  # å¹´ä»½
                r'\b(january|february|march|april|may|june|july|august|september|october|november|december)\b',
                r'\b(before|after|during|since|until)\b'
            ]
            
            for pattern in time_patterns:
                if re.search(pattern, q1_lower) and re.search(pattern, q2_lower):
                    return True
            
            # æ£€æµ‹å› æœå…³ç³»
            causal_patterns = [
                r'\b(because|since|therefore|thus|consequently|as a result)\b',
                r'\b(cause|effect|reason|due to|leads to)\b'
            ]
            
            for pattern in causal_patterns:
                if re.search(pattern, q1_lower) or re.search(pattern, q2_lower):
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"é€»è¾‘ä¾èµ–æ£€æµ‹å¤±è´¥: {e}")
            return True  # ä¿å®ˆå¤„ç†
    
    def _validate_minimal_precise_question(self, question_text: str, answer: str, keywords: List[str]) -> Dict[str, Any]:
        """
        éªŒè¯æ˜¯å¦ä¸ºæœ€å°ç²¾ç¡®é—®é¢˜ - åŸºäºç”¨æˆ·æ–°è®¾è®¡è¦æ±‚
        
        éªŒè¯é—®é¢˜æ˜¯å¦åŒ…å«æœ€å°‘çš„å¿…è¦å…³é”®è¯ï¼Œä¸”èƒ½å”¯ä¸€ç¡®å®šç­”æ¡ˆ
        """
        if not self.api_client:
            return {'is_minimal': True, 'is_precise': True, 'reasoning': 'No API client available'}
        
        try:
            validation_prompt = f"""**TASK: Validate if this is a minimal and precise question for Agent reasoning testing.**

**QUESTION:** {question_text}
**ANSWER:** {answer}
**KEYWORDS:** {', '.join(keywords)}

**VALIDATION CRITERIA:**

1. **MINIMALITY CHECK:**
   - Does the question contain the MINIMUM number of keywords necessary?
   - Can any keyword be removed without affecting answer uniqueness?
   - Are all keywords essential for identifying "{answer}"?

2. **PRECISION CHECK:**
   - Does the question have EXACTLY ONE correct answer: "{answer}"?
   - Are there NO other valid answers possible?
   - Is the question specific enough to eliminate ambiguity?

3. **KEYWORD NECESSITY:**
   - Test: If we remove each keyword individually, can we still uniquely identify "{answer}"?
   - Only keep keywords that are ESSENTIAL for answer determination

**EVALUATION TESTS:**
For each keyword in [{', '.join(keywords)}]:
- Remove the keyword and check if "{answer}" is still the ONLY possible answer
- If YES â†’ keyword is NOT necessary
- If NO â†’ keyword IS necessary

**Output Format (JSON):**
{{
    "is_minimal": true/false,
    "is_precise": true/false,
    "essential_keywords": ["list", "of", "truly", "necessary", "keywords"],
    "redundant_keywords": ["list", "of", "removable", "keywords"],
    "precision_score": 0.0-1.0,
    "minimality_score": 0.0-1.0,
    "overall_quality": "excellent/good/fair/poor",
    "improvement_suggestions": "specific suggestions if needed",
    "alternative_answers": ["other", "possible", "answers", "if", "any"],
    "reasoning": "detailed explanation of the validation"
}}

**TARGET: Determine if this question meets the standard of minimal and precise for Agent testing.**"""

            response = self.api_client.generate_response(
                prompt=validation_prompt,
                temperature=0.1,  # éå¸¸ä½çš„æ¸©åº¦ç¡®ä¿å®¢è§‚è¯„ä¼°
                max_tokens=600
            )
            
            parsed_data = self._parse_json_response(response)
            if parsed_data:
                return {
                    'is_minimal': parsed_data.get('is_minimal', False),
                    'is_precise': parsed_data.get('is_precise', False),
                    'essential_keywords': parsed_data.get('essential_keywords', keywords),
                    'redundant_keywords': parsed_data.get('redundant_keywords', []),
                    'precision_score': parsed_data.get('precision_score', 0.5),
                    'minimality_score': parsed_data.get('minimality_score', 0.5),
                    'overall_quality': parsed_data.get('overall_quality', 'fair'),
                    'reasoning': parsed_data.get('reasoning', ''),
                    'alternative_answers': parsed_data.get('alternative_answers', [])
                }
            else:
                return {'is_minimal': False, 'is_precise': False, 'reasoning': 'Failed to parse validation response'}
                
        except Exception as e:
            logger.error(f"æœ€å°ç²¾ç¡®é—®é¢˜éªŒè¯å¤±è´¥: {e}")
            return {'is_minimal': False, 'is_precise': False, 'reasoning': f'Validation error: {e}'}
    
    def _validate_no_root_answer_exposure(self, question_text: str, root_answer: str, current_layer: int) -> bool:
        """
        éªŒè¯é—®é¢˜æ˜¯å¦ä¼šç›´æ¥æš´éœ²æ ¹ç­”æ¡ˆ - é˜²æ­¢Agentæ¨ç†è¿‡ç¨‹ä¸­ç­”æ¡ˆæ³„éœ²
        
        Args:
            question_text: å¾…éªŒè¯çš„é—®é¢˜
            root_answer: æ ¹ç­”æ¡ˆï¼ˆæœ€ç»ˆç›®æ ‡ç­”æ¡ˆï¼‰
            current_layer: å½“å‰é—®é¢˜æ‰€åœ¨å±‚çº§
            
        Returns:
            True: é—®é¢˜å®‰å…¨ï¼Œä¸ä¼šæš´éœ²æ ¹ç­”æ¡ˆ
            False: é—®é¢˜æœ‰é£é™©ï¼Œå¯èƒ½æš´éœ²æ ¹ç­”æ¡ˆ
        """
        if not self.api_client:
            return True  # æ— æ³•éªŒè¯æ—¶ä¿å®ˆè¿”å›True
        
        try:
            exposure_test_prompt = f"""**TASK: Test if this question could directly expose the target answer during Agent reasoning.**

**QUESTION TO TEST:** {question_text}
**TARGET ANSWER (MUST NOT BE EXPOSED):** {root_answer}
**QUESTION LAYER:** {current_layer}

**CRITICAL TEST:**
Imagine an Agent analyzing this question step by step. Would the Agent be able to directly identify or deduce "{root_answer}" as a likely answer during the reasoning process?

**EXPOSURE RISK SCENARIOS:**
1. **Direct Mention**: Does the question directly contain "{root_answer}"?
2. **Obvious Implication**: Would a reasoning Agent immediately think of "{root_answer}" when seeing this question?
3. **Contextual Clues**: Do the keywords/context strongly suggest "{root_answer}"?
4. **Domain Overlap**: Is the question in the same knowledge domain as "{root_answer}"?
5. **Logical Path**: Is there a short logical path from question to "{root_answer}"?

**RISK ASSESSMENT:**
- **HIGH RISK**: Agent would likely think of "{root_answer}" within 1-2 reasoning steps
- **MEDIUM RISK**: Agent might consider "{root_answer}" among several possibilities
- **LOW RISK**: Agent would need many steps and different information to reach "{root_answer}"
- **SAFE**: No reasonable path from question to "{root_answer}"

**EXAMPLES OF HIGH RISK:**
- If root_answer="Tesla" and question asks "Which company makes electric vehicles?"
- If root_answer="20" and question asks "How many years has X been in business?"
- If root_answer="Apple" and question asks "Which tech company was founded by Steve Jobs?"

**Output Format (JSON):**
{{
    "exposure_risk": "high/medium/low/safe",
    "will_expose_answer": true/false,
    "risk_factors": ["list", "of", "specific", "risk", "factors"],
    "reasoning_path_to_answer": "explanation of how Agent might reach the target answer",
    "safety_score": 0.0-1.0,
    "recommendations": "suggestions to reduce exposure risk if needed"
}}

**TARGET: Determine if this question safely avoids exposing "{root_answer}" during Agent reasoning.**"""

            response = self.api_client.generate_response(
                prompt=exposure_test_prompt,
                temperature=0.1,  # å¾ˆä½æ¸©åº¦ç¡®ä¿å®¢è§‚åˆ†æ
                max_tokens=500
            )
            
            parsed_data = self._parse_json_response(response)
            if parsed_data:
                will_expose = parsed_data.get('will_expose_answer', False)
                exposure_risk = parsed_data.get('exposure_risk', 'high')
                safety_score = parsed_data.get('safety_score', 0.0)
                
                # éªŒè¯é€šè¿‡æ¡ä»¶ï¼šä¸ä¼šæš´éœ²ç­”æ¡ˆ ä¸” é£é™©ç­‰çº§ä¸ºsafeæˆ–low ä¸” å®‰å…¨åˆ†æ•°é«˜
                is_safe = (not will_expose and 
                          exposure_risk in ['safe', 'low'] and 
                          safety_score >= 0.7)
                
                if is_safe:
                    logger.info(f"âœ… æ ¹ç­”æ¡ˆæš´éœ²éªŒè¯é€šè¿‡ (å®‰å…¨åˆ†æ•°: {safety_score:.2f}, é£é™©: {exposure_risk})")
                else:
                    logger.warning(f"âŒ æ£€æµ‹åˆ°æ ¹ç­”æ¡ˆæš´éœ²é£é™© (å®‰å…¨åˆ†æ•°: {safety_score:.2f}, é£é™©: {exposure_risk})")
                    if parsed_data.get('reasoning_path_to_answer'):
                        logger.warning(f"   æš´éœ²è·¯å¾„: {parsed_data['reasoning_path_to_answer']}")
                
                return is_safe
            else:
                logger.warning("æ— æ³•è§£ææ ¹ç­”æ¡ˆæš´éœ²éªŒè¯å“åº”")
                return False  # ä¿å®ˆå¤„ç†
                
        except Exception as e:
            logger.error(f"æ ¹ç­”æ¡ˆæš´éœ²éªŒè¯å¤±è´¥: {e}")
            return True  # éªŒè¯å¤±è´¥æ—¶ä¿å®ˆè¿”å›True
    
    def _extract_root_answer_from_tree_id(self, tree_id: str) -> Optional[str]:
        """
        ä»tree_idä¸­æå–æ ¹ç­”æ¡ˆ - ç”¨äºæ ¹ç­”æ¡ˆæš´éœ²éªŒè¯
        
        Args:
            tree_id: æ¨ç†æ ‘ID
            
        Returns:
            æ ¹ç­”æ¡ˆå­—ç¬¦ä¸²ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        try:
            # å°è¯•ä»å½“å‰è½¨è¿¹è®°å½•ä¸­æ‰¾åˆ°å¯¹åº”çš„æ ¹ç­”æ¡ˆ
            for record in reversed(self.trajectory_records):
                if (record.get('tree_id') == tree_id and 
                    record.get('extension_type') == 'root' and
                    record.get('current_answer')):
                    return record['current_answer']
            
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›None
            return None
            
        except Exception as e:
            logger.error(f"ä»tree_idæå–æ ¹ç­”æ¡ˆå¤±è´¥: {e}")
            return None