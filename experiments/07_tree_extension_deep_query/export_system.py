"""
Export System for Tree Extension Deep Query Results
Generates JSON and Excel formatted results for comprehensive analysis
"""

import logging
import json
import pandas as pd
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils.dataframe import dataframe_to_rows
import re # Added for _extract_field

# Imports for type hints - loaded dynamically when needed

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class ExportSystem:
    """Export system for comprehensive results output"""
    
    def __init__(self):
        # Get configuration
        from config import get_config
        self.config = get_config()
        
        self.results_dir = Path(self.config.output_dir)
        self.export_formats = self.config.export_formats
        self.include_trajectories = self.config.record_full_trajectory
        
        # Ensure results directory exists
        self.results_dir.mkdir(exist_ok=True)
        
        # Statistics
        self.stats = {
            'total_exports': 0,
            'json_exports': 0,
            'excel_exports': 0,
            'failed_exports': 0
        }
    
    def export_experiment_results(self, question_trees: List[Any], 
                                trajectories: List[Any],
                                experiment_metadata: Dict[str, Any]) -> Dict[str, str]:
        """å¯¼å‡ºå®Œæ•´çš„å®éªŒç»“æœ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_results = {}
        
        logger.info(f"å¼€å§‹å¯¼å‡ºå®éªŒç»“æœ - æ—¶é—´æˆ³: {timestamp}")
        
        try:
            # å‡†å¤‡å¯¼å‡ºæ•°æ®
            export_data = self._prepare_export_data(question_trees, trajectories, experiment_metadata)
            
            # å¯¼å‡ºJSONæ ¼å¼
            if 'json' in self.export_formats:
                json_file = self._export_json(export_data, timestamp)
                export_results['json'] = str(json_file)
                self.stats['json_exports'] += 1
            
            # å¯¼å‡ºExcelæ ¼å¼
            if 'excel' in self.export_formats:
                excel_file = self._export_excel(export_data, timestamp)
                if excel_file: # Only add to results if export was successful
                    export_results['excel'] = str(excel_file)
                    self.stats['excel_exports'] += 1
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if self.config.include_analysis_report:
                analysis_file = self._generate_analysis_report(export_data, timestamp)
                export_results['analysis'] = str(analysis_file)
            
            self.stats['total_exports'] += 1
            logger.info(f"å®éªŒç»“æœå¯¼å‡ºå®Œæˆ: {len(export_results)} ä¸ªæ–‡ä»¶")
            
            return export_results
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºå®éªŒç»“æœå¤±è´¥: {e}")
            self.stats['failed_exports'] += 1
            return {}
    
    def _prepare_export_data(self, question_trees: List[Any],
                           trajectories: List[Any],
                           experiment_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡å¯¼å‡ºæ•°æ®"""
        
        # å‡†å¤‡é—®é¢˜æ ‘æ•°æ®
        trees_data = []
        for tree in question_trees:
            tree_data = {
                'tree_id': f"tree_{len(trees_data) + 1}",
                'root_question': getattr(tree, 'root_question', {}),
                'nodes': getattr(tree, 'nodes', {}),
                'total_nodes': getattr(tree, 'total_nodes', 0),
                'max_depth': getattr(tree, 'current_depth', 0),
                'extensions_count': len(getattr(tree, 'nodes', {}))
            }
            trees_data.append(tree_data)
        
        # å‡†å¤‡è½¨è¿¹æ•°æ®
        trajectories_data = []
        for trajectory in trajectories:
            traj_data = {
                'trajectory_id': getattr(trajectory, 'trajectory_id', ''),
                'document_id': getattr(trajectory, 'document_id', ''),
                'total_hops': getattr(trajectory, 'total_hops', 0),
                'success_rate': getattr(trajectory, 'success_rate', 0.0),
                'steps': getattr(trajectory, 'extension_steps', [])
            }
            trajectories_data.append(traj_data)
        
        return {
            'experiment_metadata': experiment_metadata,
            'question_trees': trees_data,
            'trajectories': trajectories_data,
            'export_timestamp': datetime.now().isoformat(),
            'statistics': {
                'total_trees': len(question_trees),
                'total_trajectories': len(trajectories),
                'total_extensions': sum(tree_data['extensions_count'] for tree_data in trees_data)
            }
        }
    
    def _extract_tree_structure(self, tree: Any) -> Dict[str, Any]:
        """æå–æ ‘ç»“æ„"""
        structure = {
            'root': {
                'question': tree.root_question.text,
                'answer': tree.root_question.short_answer,
                'type': 'root'
            },
            'nodes': {}
        }
        
        for node_id, node in tree.nodes.items():
            structure['nodes'][node_id] = {
                'question': node.question,
                'answer': node.answer,
                'short_answer': getattr(node, 'short_answer', None),
                'type': node.extension_type,
                'depth': node.depth_level,
                'parent': node.parent_node_id,
                'confidence': node.confidence,
                'verified': node.search_verified
            }
        
        return structure
    
    def _export_json(self, export_data: Dict[str, Any], timestamp: str) -> Path:
        """å¯¼å‡ºJSONæ ¼å¼"""
        json_file = self.results_dir / f"tree_extension_experiment_{timestamp}.json"
        
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"JSONç»“æœå·²å¯¼å‡º: {json_file}")
            return json_file
            
        except Exception as e:
            logger.error(f"JSONå¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def _export_excel(self, export_data: Dict[str, Any], timestamp: str) -> Path:
        """å¯¼å‡ºExcelæ ¼å¼ - é‡å†™ç‰ˆæœ¬"""
        excel_file = self.results_dir / f"tree_extension_experiment_{timestamp}.xlsx"
        
        try:
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # 1. ä¸»è¦é—®ç­”è¡¨
                qa_pairs = []
                for tree_data in export_data.get('question_trees', []):
                    tree_id = tree_data.get('tree_id', 'unknown')
                    
                    # ç›´æ¥ä»å¯¹è±¡ç»“æ„æå–æ ¹é—®é¢˜ä¿¡æ¯
                    root_question = tree_data.get('root_question', {})
                    
                    # å¤„ç†æ ¹é—®é¢˜ - å¯èƒ½æ˜¯å¯¹è±¡æˆ–å­—ç¬¦ä¸²
                    if hasattr(root_question, 'question_text'):
                        # BaseQuestionå¯¹è±¡ç»“æ„
                        question_text = root_question.question_text
                        document_id = root_question.document_id
                        if hasattr(root_question, 'short_answer') and hasattr(root_question.short_answer, 'answer_text'):
                            short_answer_text = root_question.short_answer.answer_text
                        else:
                            short_answer_text = ''
                    elif isinstance(root_question, dict):
                        # å­—å…¸ç»“æ„
                        question_text = root_question.get('question_text', '')
                        document_id = root_question.get('document_id', '')
                        short_answer = root_question.get('short_answer', {})
                        if isinstance(short_answer, dict):
                            short_answer_text = short_answer.get('answer_text', '')
                        else:
                            short_answer_text = str(short_answer)
                    else:
                        # å­—ç¬¦ä¸²ç»“æ„ - ä½¿ç”¨åŸè§£ææ–¹æ³•
                        root_info = self._parse_question_string(str(root_question))
                        question_text = root_info.get('question', '')
                        document_id = root_info.get('document_id', '')
                        short_answer_text = root_info.get('answer', '')  # å¯¹äºæ ¹é—®é¢˜ï¼Œè¿™å°±æ˜¯çŸ­ç­”æ¡ˆ
                    
                    # æ·»åŠ æ ¹é—®é¢˜
                    qa_pairs.append({
                        'Tree_ID': tree_id,
                        'Node_Type': 'Root',
                        'Node_ID': 'root',
                        'Question': question_text,
                        'Answer': short_answer_text,  # æ ¹é—®é¢˜åªæœ‰çŸ­ç­”æ¡ˆï¼Œæ²¡æœ‰é•¿ç­”æ¡ˆ
                        'Short_Answer': short_answer_text,
                        'Document_ID': document_id,
                        'Topic': self._extract_topic_from_doc_id(document_id),
                        'TXT_File': f"{document_id}.txt" if document_id else '',
                        'Question_Type': 'factual',
                        'Difficulty': '',
                        'Extension_Type': '',
                        'Depth_Level': 0,
                        'Keywords': '',
                        'Confidence': '',
                        'Verification_Score': ''
                    })
                    
                    # æ·»åŠ æ‰©å±•èŠ‚ç‚¹
                    nodes = tree_data.get('nodes', {})
                    for node_id, node_data in nodes.items():
                        # å¤„ç†ExtensionNodeå¯¹è±¡
                        ext_info = None  # åˆå§‹åŒ–ext_infoå˜é‡
                        if hasattr(node_data, 'question'):
                            # ExtensionNodeå¯¹è±¡ç»“æ„
                            ext_question = node_data.question
                            ext_answer = node_data.answer if hasattr(node_data, 'answer') else ''
                            ext_short_answer = getattr(node_data, 'short_answer', ext_answer)
                            ext_type = node_data.extension_type if hasattr(node_data, 'extension_type') else 'unknown'
                            depth_level = node_data.depth_level if hasattr(node_data, 'depth_level') else 0
                            confidence = node_data.confidence if hasattr(node_data, 'confidence') else 0.0
                            verification_score = node_data.verification_score if hasattr(node_data, 'verification_score') else 0.0
                            keywords = node_data.keywords if hasattr(node_data, 'keywords') else []
                            if isinstance(keywords, list):
                                keywords_str = ', '.join(map(str, keywords))
                            else:
                                keywords_str = str(keywords)
                        elif isinstance(node_data, dict):
                            # å­—å…¸ç»“æ„
                            ext_question = node_data.get('question', '')
                            ext_answer = node_data.get('answer', '')
                            
                            # å¤„ç†short_answerå­—å…¸æ ¼å¼
                            short_answer_data = node_data.get('short_answer', ext_answer)
                            if isinstance(short_answer_data, dict) and 'answer_text' in short_answer_data:
                                ext_short_answer = short_answer_data['answer_text']
                            else:
                                ext_short_answer = str(short_answer_data) if short_answer_data else ext_answer
                            
                            ext_type = node_data.get('extension_type', 'unknown')
                            depth_level = node_data.get('depth_level', 0)
                            confidence = node_data.get('confidence', 0.0)
                            verification_score = node_data.get('verification_score', 0.0)
                            keywords = node_data.get('keywords', [])
                            if isinstance(keywords, list):
                                keywords_str = ', '.join(map(str, keywords))
                            else:
                                keywords_str = str(keywords)
                        else:
                            # å­—ç¬¦ä¸²ç»“æ„ - ä½¿ç”¨è§£ææ–¹æ³•
                            ext_info = self._parse_extension_string(str(node_data))
                            ext_question = ext_info.get('question', '')
                            ext_answer = ext_info.get('answer', '')
                            ext_short_answer = ext_info.get('short_answer', ext_answer)
                            ext_type = ext_info.get('extension_type', 'unknown')
                            depth_level = ext_info.get('depth_level', 0)
                            confidence = ext_info.get('confidence', 0.0)
                            verification_score = ext_info.get('verification_score', 0.0)
                            keywords_str = ext_info.get('keywords', '')
                        
                        qa_pairs.append({
                            'Tree_ID': tree_id,
                            'Node_Type': 'Extension',
                            'Node_ID': node_id,
                            'Question': ext_question,
                            'Answer': ext_answer,
                            'Short_Answer': ext_short_answer,
                            'Document_ID': document_id,
                            'Topic': self._extract_topic_from_doc_id(document_id),
                            'TXT_File': f"{document_id}.txt" if document_id else '',
                            'Question_Type': f"{ext_type}_extension",
                            'Difficulty': '',
                            'Extension_Type': ext_type,
                            'Depth_Level': depth_level,
                            'Keywords': keywords_str,
                            'Confidence': confidence,
                            'Verification_Score': verification_score
                        })
                
                if qa_pairs:
                    qa_df = pd.DataFrame(qa_pairs)
                    qa_df.to_excel(writer, sheet_name='é—®ç­”æ•°æ®', index=False)
                
                # 2. å®éªŒç»Ÿè®¡æ‘˜è¦
                stats = export_data.get('experiment_metadata', {}).get('statistics', {})
                summary_data = {
                    'ç»Ÿè®¡é¡¹ç›®': [
                        'æ–‡æ¡£æ€»æ•°', 'ç­›é€‰é€šè¿‡æ–‡æ¡£', 'ç”Ÿæˆé—®é¢˜æ ‘æ•°', 
                        'æ€»æ‰©å±•æ•°', 'å¤„ç†é”™è¯¯æ•°', 'å®Œæˆç‡(%)'
                    ],
                    'æ•°å€¼': [
                        stats.get('documents_loaded', 0),
                        stats.get('suitable_documents', 0),
                        stats.get('trees_completed', 0),
                        stats.get('extensions_created', 0),
                        stats.get('processing_errors', 0),
                        round(stats.get('trees_completed', 0) / max(stats.get('suitable_documents', 1), 1) * 100, 1)
                    ]
                }
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='å®éªŒç»Ÿè®¡', index=False)
                
                # 3. é…ç½®ä¿¡æ¯
                config = export_data.get('experiment_metadata', {}).get('configuration', {})
                config_data = {
                    'é…ç½®é¡¹': list(config.keys()),
                    'è®¾ç½®å€¼': [str(v) for v in config.values()]
                }
                if config_data['é…ç½®é¡¹']:
                    config_df = pd.DataFrame(config_data)
                    config_df.to_excel(writer, sheet_name='å®éªŒé…ç½®', index=False)
                
                # 4. æ ‘ç»“æ„æ€»è§ˆ
                tree_summary = []
                for tree_data in export_data.get('question_trees', []):
                    # å®‰å…¨è·å–æ–‡æ¡£ID
                    root_question = tree_data.get('root_question', {})
                    if isinstance(root_question, dict):
                        doc_id = root_question.get('document_id', '')
                    elif hasattr(root_question, 'document_id'):
                        doc_id = root_question.document_id
                    else:
                        doc_id = self._extract_field(str(root_question), 'document_id') if root_question else ''
                    
                    tree_summary.append({
                        'æ ‘ID': tree_data['tree_id'],
                        'æ€»èŠ‚ç‚¹æ•°': tree_data.get('total_nodes', 0),
                        'æœ€å¤§æ·±åº¦': tree_data.get('max_depth', 0),
                        'æ‰©å±•æ•°é‡': tree_data.get('extensions_count', 0),
                        'æ–‡æ¡£ID': doc_id
                    })
                
                if tree_summary:
                    tree_df = pd.DataFrame(tree_summary)
                    tree_df.to_excel(writer, sheet_name='æ ‘ç»“æ„æ€»è§ˆ', index=False)
                
                # 5. è½¨è¿¹æ•°æ® - å¢å¼ºç‰ˆ
                trajectories_data = export_data.get('trajectories', [])
                if trajectories_data:
                    trajectory_records = []
                    for traj in trajectories_data:
                        if isinstance(traj, dict):
                            # å¤„ç†æ–°çš„è½¨è¿¹è®°å½•æ ¼å¼
                            root_validation = traj.get('root_validation', {})
                            keyword_extraction = traj.get('keyword_extraction', {})
                            extension_steps = traj.get('extension_steps', [])
                            
                            trajectory_records.append({
                                'è½¨è¿¹ID': traj.get('trajectory_id', ''),
                                'æ–‡æ¡£ID': traj.get('document_id', ''),
                                'æ€»å¤„ç†æ—¶é—´': round(traj.get('total_duration', 0.0), 2),
                                'æˆåŠŸç‡': f"{traj.get('success_rate', 0.0):.1%}",
                                'éªŒè¯æ€»æ•°': traj.get('total_validations', 0),
                                'æˆåŠŸéªŒè¯æ•°': traj.get('successful_validations', 0),
                                'Webæœç´¢æ¬¡æ•°': traj.get('total_web_searches', 0),
                                'æœ€ç»ˆæ ‘æ·±åº¦': traj.get('final_tree_depth', 0),
                                'æœ€ç»ˆæ ‘å¤§å°': traj.get('final_tree_size', 0),
                                'å¹³å‡éªŒè¯åˆ†æ•°': f"{traj.get('average_validation_score', 0.0):.3f}",
                                'å…³é”®è¯å±‚æ¬¡åˆè§„ç‡': f"{traj.get('keyword_hierarchy_compliance', 0.0):.1%}",
                                'å¿«æ·è·¯å¾„é˜²æ­¢ç‡': f"{traj.get('shortcut_prevention_rate', 0.0):.1%}",
                                'æ ¹é—®é¢˜éªŒè¯åˆ†æ•°': f"{root_validation.get('validation_result', {}).get('overall_score', 0.0):.3f}" if root_validation else '',
                                'å…³é”®è¯æå–è´¨é‡': f"{keyword_extraction.get('validation_result', {}).get('keywords_quality', 0.0):.3f}" if keyword_extraction else '',
                                'æ‰©å±•æ­¥éª¤æ•°': len(extension_steps)
                            })
                        else:
                            # å…¼å®¹æ—§æ ¼å¼
                            trajectory_records.append({
                                'è½¨è¿¹ID': getattr(traj, 'trajectory_id', ''),
                                'æ–‡æ¡£ID': getattr(traj, 'document_id', ''),
                                'æ€»å¤„ç†æ—¶é—´': getattr(traj, 'total_duration', 0),
                                'æˆåŠŸç‡': f"{getattr(traj, 'success_rate', 0.0):.1%}",
                                'éªŒè¯æ€»æ•°': getattr(traj, 'total_validations', 0),
                                'æˆåŠŸéªŒè¯æ•°': getattr(traj, 'successful_validations', 0),
                                'Webæœç´¢æ¬¡æ•°': 0,
                                'æœ€ç»ˆæ ‘æ·±åº¦': 0,
                                'æœ€ç»ˆæ ‘å¤§å°': 0,
                                'å¹³å‡éªŒè¯åˆ†æ•°': '0.000',
                                'å…³é”®è¯å±‚æ¬¡åˆè§„ç‡': '0.0%',
                                'å¿«æ·è·¯å¾„é˜²æ­¢ç‡': '0.0%',
                                'æ ¹é—®é¢˜éªŒè¯åˆ†æ•°': '',
                                'å…³é”®è¯æå–è´¨é‡': '',
                                'æ‰©å±•æ­¥éª¤æ•°': 0
                            })
                    
                    if trajectory_records:
                        traj_df = pd.DataFrame(trajectory_records)
                        traj_df.to_excel(writer, sheet_name='è½¨è¿¹æ•°æ®', index=False)
            
            logger.info(f"Excelç»“æœå·²å¯¼å‡º: {excel_file}")
            return excel_file
            
        except Exception as e:
            logger.error(f"Excelå¯¼å‡ºå¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯ç»§ç»­
            return None
    
    def _format_excel_file(self, excel_file: Path):
        """æ ¼å¼åŒ–Excelæ–‡ä»¶"""
        try:
            wb = openpyxl.load_workbook(excel_file)
            
            # æ ‡é¢˜æ ·å¼
            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            header_alignment = Alignment(horizontal="center", vertical="center")
            
            # è¾¹æ¡†
            thin_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                
                # æ ¼å¼åŒ–æ ‡é¢˜è¡Œ
                if ws.max_row > 0:
                    for cell in ws[1]:
                        cell.font = header_font
                        cell.fill = header_fill
                        cell.alignment = header_alignment
                        cell.border = thin_border
                
                # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
                for column in ws.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        if cell.value:
                            cell_length = len(str(cell.value))
                            if cell_length > max_length:
                                max_length = cell_length
                    
                    adjusted_width = min(max_length + 2, 50)
                    ws.column_dimensions[column_letter].width = adjusted_width
                
                # å†»ç»“é¦–è¡Œ
                ws.freeze_panes = 'A2'
            
            wb.save(excel_file)
            logger.debug(f"Excelæ ¼å¼åŒ–å®Œæˆ: {excel_file}")
            
        except Exception as e:
            logger.warning(f"Excelæ ¼å¼åŒ–å¤±è´¥: {e}")
    
    def _export_analysis_report(self, export_data: Dict[str, Any], timestamp: str) -> Path:
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        report_file = self.results_dir / f"analysis_report_{timestamp}.md"
        
        try:
            report_content = self._generate_analysis_report(export_data)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"åˆ†ææŠ¥å‘Šå·²å¯¼å‡º: {report_file}")
            return report_file
            
        except Exception as e:
            logger.error(f"åˆ†ææŠ¥å‘Šå¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def _generate_analysis_report(self, export_data: Dict[str, Any], timestamp: str) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Šå†…å®¹"""
        
        stats = export_data['summary_statistics']
        ext_stats = stats['extension_statistics']
        
        # è®¡ç®—é¢å¤–æŒ‡æ ‡
        avg_quality = sum(ext_stats['quality_scores']) / len(ext_stats['quality_scores']) if ext_stats['quality_scores'] else 0
        parallel_ratio = ext_stats['parallel_count'] / max(stats['total_extensions'], 1)
        series_ratio = ext_stats['series_count'] / max(stats['total_extensions'], 1)
        
        report = f"""# Tree Extension Deep Query å®éªŒåˆ†ææŠ¥å‘Š

## ğŸ“Š å®éªŒæ¦‚è§ˆ

**ç”Ÿæˆæ—¶é—´**: {export_data['export_timestamp']}

**æ ¸å¿ƒæŒ‡æ ‡**:
- ğŸ“ å¤„ç†æ–‡æ¡£æ•°: {stats['total_documents']}
- ğŸŒ³ ç”Ÿæˆé—®é¢˜æ ‘: {stats['total_question_trees']}
- â“ æ€»é—®ç­”å¯¹æ•°: {stats['total_qa_pairs']}
- ğŸ”„ æ€»æ‰©å±•æ•°: {stats['total_extensions']}

## ğŸ” æ‰©å±•åˆ†æ

### æ‰©å±•ç±»å‹åˆ†å¸ƒ
- **Parallelæ‰©å±•**: {ext_stats['parallel_count']} ({parallel_ratio:.1%})
- **Seriesæ‰©å±•**: {ext_stats['series_count']} ({series_ratio:.1%})

### æ·±åº¦ä¸å¤æ‚åº¦
- **æœ€å¤§æ‰©å±•æ·±åº¦**: {ext_stats['max_depth']}
- **å¹³å‡åˆ†æ”¯æ•°**: {ext_stats['avg_branches']:.2f}
- **å¹³å‡è´¨é‡åˆ†æ•°**: {avg_quality:.3f}

## ğŸ“ˆ è´¨é‡è¯„ä¼°

### éªŒè¯é€šè¿‡ç‡
"""
        
        if export_data['trajectories']:
            # è½¨è¿¹åˆ†æ
            trajectories = export_data['trajectories']
            avg_success_rate = sum(t['success_rate'] for t in trajectories) / len(trajectories)
            avg_duration = sum(t['duration'] for t in trajectories) / len(trajectories)
            avg_verification = sum(t['verification_score'] for t in trajectories) / len(trajectories)
            
            report += f"""
- **å¹³å‡æˆåŠŸç‡**: {avg_success_rate:.1%}
- **å¹³å‡éªŒè¯åˆ†æ•°**: {avg_verification:.3f}
- **å¹³å‡å¤„ç†æ—¶é—´**: {avg_duration:.2f}ç§’

### è½¨è¿¹ç»Ÿè®¡
- **æ€»è½¨è¿¹æ•°**: {len(trajectories)}
- **å¹³å‡æ­¥éª¤æ•°**: {sum(t['total_steps'] for t in trajectories) / len(trajectories):.1f}
- **å¹³å‡å†³ç­–ç‚¹**: {sum(t['decision_points_count'] for t in trajectories) / len(trajectories):.1f}
"""
        
        # è´¨é‡åˆ†å¸ƒ
        if ext_stats['quality_scores']:
            high_quality = sum(1 for score in ext_stats['quality_scores'] if score >= 0.8)
            medium_quality = sum(1 for score in ext_stats['quality_scores'] if 0.6 <= score < 0.8)
            low_quality = sum(1 for score in ext_stats['quality_scores'] if score < 0.6)
            
            report += f"""
### è´¨é‡åˆ†æ•°åˆ†å¸ƒ
- **é«˜è´¨é‡** (â‰¥0.8): {high_quality} ({high_quality/len(ext_stats['quality_scores']):.1%})
- **ä¸­ç­‰è´¨é‡** (0.6-0.8): {medium_quality} ({medium_quality/len(ext_stats['quality_scores']):.1%})
- **ä½è´¨é‡** (<0.6): {low_quality} ({low_quality/len(ext_stats['quality_scores']):.1%})
"""
        
        report += f"""
## ğŸ“‹ å®éªŒé…ç½®

| é…ç½®é¡¹ | å€¼ |
|--------|-----|
"""
        
        for key, value in export_data['experiment_metadata'].items():
            report += f"| {key} | {value} |\n"
        
        report += f"""
## ğŸ“Š æ•°æ®ç»Ÿè®¡

### é—®ç­”å¯¹ç±»å‹åˆ†å¸ƒ
- **æ ¹é—®é¢˜**: {stats['total_question_trees']}
- **æ‰©å±•é—®é¢˜**: {stats['total_extensions']}

### æ‰©å±•æ•ˆç‡
- **æ¯æ ‘å¹³å‡æ‰©å±•æ•°**: {stats['total_extensions'] / max(stats['total_question_trees'], 1):.2f}
- **æ‰©å±•æˆåŠŸç‡**: {avg_quality:.1%}

## ğŸ’¡ å…³é”®å‘ç°

1. **æ‰©å±•ç­–ç•¥**: {"Parallelæ‰©å±•ä¸ºä¸»" if parallel_ratio > series_ratio else "Seriesæ‰©å±•ä¸ºä¸»"} ({max(parallel_ratio, series_ratio):.1%})
2. **è´¨é‡ä¿è¯**: å¹³å‡éªŒè¯åˆ†æ•° {avg_quality:.3f}
3. **å¤æ‚åº¦æ§åˆ¶**: æœ€å¤§æ·±åº¦ {ext_stats['max_depth']} å±‚ï¼Œç¬¦åˆè®¾è®¡è¦æ±‚

## ğŸ“ æ”¹è¿›å»ºè®®

1. æ ¹æ®è´¨é‡åˆ†æ•°åˆ†å¸ƒè°ƒæ•´éªŒè¯é˜ˆå€¼
2. ä¼˜åŒ–{"parallel" if parallel_ratio < 0.3 else "series"}æ‰©å±•ç­–ç•¥çš„æ¯”ä¾‹
3. è€ƒè™‘å¢åŠ {"æ·±åº¦" if ext_stats['max_depth'] < 3 else "å¹¿åº¦"}æ‰©å±•æœºåˆ¶

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
        
        return report
    
    def export_individual_tree(self, question_tree: Any, output_file: Path):
        """å¯¼å‡ºå•ä¸ªé—®é¢˜æ ‘"""
        try:
            tree_data = {
                'tree_id': question_tree.root_question.document_id,
                'root_question': {
                    'text': question_tree.root_question.text,
                    'answer': question_tree.root_question.short_answer,
                    'type': question_tree.root_question.question_type,
                    'difficulty': question_tree.root_question.difficulty_level,
                    'confidence': question_tree.root_question.confidence
                },
                'extensions': {},
                'metadata': {
                    'total_nodes': question_tree.total_nodes,
                    'max_depth': question_tree.current_depth,
                    'export_timestamp': datetime.now().isoformat()
                }
            }
            
            for node_id, node in question_tree.nodes.items():
                tree_data['extensions'][node_id] = {
                    'question': node.question,
                    'answer': node.answer,
                    'short_answer': getattr(node, 'short_answer', None),
                    'extension_type': node.extension_type,
                    'depth_level': node.depth_level,
                    'parent_node_id': node.parent_node_id,
                    'confidence': node.confidence,
                    'verification_score': node.verification_score,
                    'search_verified': node.search_verified,
                    'keywords': node.keywords,
                    'reasoning': node.reasoning
                }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(tree_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"å•ä¸ªé—®é¢˜æ ‘å·²å¯¼å‡º: {output_file}")
            
        except Exception as e:
            logger.error(f"å•ä¸ªé—®é¢˜æ ‘å¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def log_export_statistics(self):
        """è®°å½•å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=== å¯¼å‡ºç³»ç»Ÿç»Ÿè®¡ ===")
        logger.info(f"æ€»å¯¼å‡ºæ¬¡æ•°: {self.stats['total_exports']}")
        logger.info(f"JSONå¯¼å‡º: {self.stats['json_exports']}")
        logger.info(f"Excelå¯¼å‡º: {self.stats['excel_exports']}")
        logger.info(f"å¤±è´¥å¯¼å‡º: {self.stats['failed_exports']}")

    def _extract_field(self, text: str, field_name: str) -> Any:
        """ä»å­—ç¬¦ä¸²ä¸­æå–ç‰¹å®šå­—æ®µå€¼"""
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å­—æ®µå
            match = re.search(f"{field_name}=([^,}}]*)", text)
            if match:
                # å°è¯•è§£æä¸ºæ•°å­—æˆ–å¸ƒå°”å€¼
                try:
                    return float(match.group(1))
                except ValueError:
                    return match.group(1)
            return None
        except Exception as e:
            logger.warning(f"Failed to extract field '{field_name}' from text: {e}")
            return None
    
    def _parse_question_string(self, question_str: str) -> Dict[str, str]:
        """è§£æé—®é¢˜å­—ç¬¦ä¸²ï¼Œæå–å…³é”®ä¿¡æ¯"""
        result = {
            'question': '',
            'answer': '',
            'short_answer': '',
            'document_id': '',
            'question_type': '',
            'difficulty': ''
        }
        
        if not question_str:
            return result
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¿¡æ¯
            import re
            
            # æå– question_text (å¤„ç†å•å¼•å·å†…å®¹)
            question_match = re.search(r"question_text='([^']*(?:''[^']*)*)'", question_str)
            if question_match:
                result['question'] = question_match.group(1).replace("''", "'")
            
            # æå– ShortAnswer ä¸­çš„ answer_text (å¤„ç†å•å¼•å·å†…å®¹)
            short_answer_match = re.search(r"ShortAnswer\([^)]*answer_text='([^']*(?:''[^']*)*)'", question_str)
            if short_answer_match:
                result['answer'] = short_answer_match.group(1).replace("''", "'")
            
            # ä¹Ÿå°è¯•ä» short_answer= åé¢çš„å­—å…¸æ ¼å¼æå–
            if not result['answer']:
                answer_dict_match = re.search(r"'answer_text':\s*'([^']*(?:''[^']*)*)'", question_str)
                if answer_dict_match:
                    result['answer'] = answer_dict_match.group(1).replace("''", "'")
            
            # æå– document_id
            doc_id_match = re.search(r"document_id='([^']*)'", question_str)
            if doc_id_match:
                result['document_id'] = doc_id_match.group(1)
            
            # æå– question_type
            type_match = re.search(r"question_type='([^']*)'", question_str)
            if type_match:
                result['question_type'] = type_match.group(1)
            
            # æå– difficulty
            diff_match = re.search(r"difficulty='([^']*)'", question_str)
            if diff_match:
                result['difficulty'] = diff_match.group(1)
                
        except Exception as e:
            logger.warning(f"Failed to parse question string: {e}")
        
        return result
    
    def _extract_topic_from_doc_id(self, doc_id: str) -> str:
        """ä»æ–‡æ¡£IDæå–ä¸»é¢˜"""
        if not doc_id:
            return 'unknown'
        
        # ClueWeb22æ ¼å¼ï¼šclueweb22-en0000-xx-xxxxx
        import re
        topic_match = re.search(r'clueweb22-(en\d+)', doc_id)
        if topic_match:
            return topic_match.group(1)
        return 'unknown'

    def _parse_extension_string(self, ext_str: str) -> Dict[str, str]:
        """è§£ææ‰©å±•èŠ‚ç‚¹å­—ç¬¦ä¸²ï¼Œæå–å…³é”®ä¿¡æ¯"""
        result = {
            'question': '',
            'answer': '',
            'short_answer': '',
            'extension_type': '',
            'depth_level': '',
            'keywords': '',
            'confidence': '',
            'verification_score': ''
        }
        
        if not ext_str:
            return result
        
        try:
            import re
            
            # å¤„ç†ExtensionNodeæ ¼å¼ï¼Œæå– question (å¤„ç†å¤æ‚å¼•å·)
            # å…ˆå°è¯•å•å¼•å·åŒ…å›´çš„é—®é¢˜
            question_match = re.search(r"question='([^']*(?:''[^']*)*)'", ext_str)
            if question_match:
                result['question'] = question_match.group(1).replace("''", "'")
            else:
                # å°è¯•åŒå¼•å·åŒ…å›´çš„é—®é¢˜
                question_match = re.search(r'question="([^"]*(?:""[^"]*)*)"', ext_str)
                if question_match:
                    result['question'] = question_match.group(1).replace('""', '"')
            
            # æå– answer (å¤„ç†å¤æ‚å¼•å·å’Œé•¿æ–‡æœ¬)
            # å…ˆå°è¯•å•å¼•å·åŒ…å›´çš„ç­”æ¡ˆ
            answer_match = re.search(r"answer='([^']*(?:''[^']*)*)'", ext_str)
            if answer_match:
                result['answer'] = answer_match.group(1).replace("''", "'")
            else:
                # å°è¯•åŒå¼•å·åŒ…å›´çš„ç­”æ¡ˆ
                answer_match = re.search(r'answer="([^"]*(?:""[^"]*)*)"', ext_str)
                if answer_match:
                    result['answer'] = answer_match.group(1).replace('""', '"')
            
            # æå– short_answer å­—å…¸ä¸­çš„ answer_text
            short_answer_dict_match = re.search(r"short_answer=\{[^}]*'answer_text':\s*'([^']*(?:''[^']*)*)'", ext_str)
            if short_answer_dict_match:
                result['short_answer'] = short_answer_dict_match.group(1).replace("''", "'")
            
            # æå– extension_type
            type_match = re.search(r"extension_type='([^']*)'", ext_str)
            if type_match:
                result['extension_type'] = type_match.group(1)
            
            # æå– depth_level
            depth_match = re.search(r"depth_level=(\d+)", ext_str)
            if depth_match:
                result['depth_level'] = int(depth_match.group(1))
            
            # æå– keywords (å¤„ç†åˆ—è¡¨æ ¼å¼)
            keywords_match = re.search(r"keywords=\[([^\]]*)\]", ext_str)
            if keywords_match:
                keywords_content = keywords_match.group(1)
                # æå–å•å¼•å·å†…çš„å…³é”®è¯
                keyword_list = re.findall(r"'([^']*)'", keywords_content)
                result['keywords'] = ', '.join(keyword_list)
            
            # æå– confidence
            conf_match = re.search(r"confidence=([0-9.]+)", ext_str)
            if conf_match:
                result['confidence'] = float(conf_match.group(1))
            
            # æå– verification_score
            score_match = re.search(r"verification_score=([0-9.]+)", ext_str)
            if score_match:
                result['verification_score'] = float(score_match.group(1))
                
        except Exception as e:
            logger.warning(f"Failed to parse extension string: {e}")
        
        return result


# ====== æµ‹è¯•ä»£ç  ======
def test_export_system():
    """æµ‹è¯•å¯¼å‡ºç³»ç»Ÿ"""
    print("=== æµ‹è¯•å¯¼å‡ºç³»ç»Ÿ ===")
    
    # è¿™é‡Œéœ€è¦å®é™…çš„é—®é¢˜æ ‘å’Œè½¨è¿¹æ•°æ®æ¥æµ‹è¯•
    # ç”±äºä¾èµ–å…¶ä»–ç»„ä»¶ï¼Œè¿™é‡ŒåªåšåŸºæœ¬æµ‹è¯•
    
    export_system = ExportSystem()
    
    # æ¨¡æ‹Ÿæ•°æ®
    mock_metadata = {
        'experiment_name': '07_tree_extension_deep_query',
        'version': '1.0.0',
        'max_depth': 3,
        'max_branches': 2
    }
    
    print("å¯¼å‡ºç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    print(f"ç»“æœç›®å½•: {export_system.results_dir}")
    print(f"å¯¼å‡ºæ ¼å¼: {export_system.export_formats}")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = export_system.get_statistics()
    print(f"å¯¼å‡ºç»Ÿè®¡: {stats}")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    test_export_system() 