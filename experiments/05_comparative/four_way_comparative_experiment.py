#!/usr/bin/env python3
"""
å››æ–¹å¯¹æ¯”å®éªŒè„šæœ¬
Four-Way Comparative Experiment

å¯¹æ¯”å®éªŒè®¾è®¡ï¼š
1. ClueWeb22æ•°æ®é›† + OpenAI GPT-4o (å·²æœ‰ç»“æœ)
2. ClueWeb22æ•°æ®é›† + Claude Sonnet 4 (æ–°å®éªŒ)
3. éšæœºæ–‡æ¡£100ç¯‡ + OpenAI GPT-4o (æ–°å®éªŒ)
4. éšæœºæ–‡æ¡£100ç¯‡ + Claude Sonnet 4 (æ–°å®éªŒ)

è¾“å‡ºç»“æœï¼š
- å››ä»½å®Œæ•´çš„ç»“æœæ–‡ä»¶
- ç»Ÿä¸€æ ¼å¼çš„Excelå¯¹æ¯”è¡¨
- MarkdownæŠ¥å‘Šæ–‡ä»¶
"""

import os
import json
import time
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
import random
import re

# å¯¼å…¥APIå®¢æˆ·ç«¯
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'core'))

from llm_clients.openai_api_client import OpenAIClient
from llm_clients.claude_api_client import ClaudeAPIClient
from llm_clients.llm_manager import DynamicLLMManager

class FourWayComparativeExperiment:
    """å››æ–¹å¯¹æ¯”å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, openai_api_key: str, claude_api_key: str):
        """
        åˆå§‹åŒ–å®éªŒ
        
        Args:
            openai_api_key: OpenAI APIå¯†é’¥
            claude_api_key: Claude APIå¯†é’¥
        """
        self.openai_api_key = openai_api_key
        self.claude_api_key = claude_api_key
        
        # åˆ›å»ºå…¨æ–°çš„è¾“å‡ºç›®å½• - é¿å…ä¸å†å²æ•°æ®æ··æ·†
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"FRESH_FOUR_WAY_EXPERIMENT_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['OPENAI_API_KEY'] = openai_api_key
        os.environ['ANTHROPIC_API_KEY'] = claude_api_key
        
        # åˆå§‹åŒ–LLMç®¡ç†å™¨
        self.llm_manager = DynamicLLMManager()
        
        # æ•°æ®æºé…ç½®
        self.clueweb_data_dir = Path("task_file/clueweb22_query_results")
        self.random_docs_dir = Path("task_file/random_documents") 
        
        # ğŸ”„ å®éªŒé…ç½® - å…¨éƒ¨é‡æ–°æ‰§è¡Œ
        self.experiments = {
            "clueweb_openai": {
                "name": "ClueWeb22 + OpenAI GPT-4o",
                "data_source": "clueweb22",
                "provider": "openai",
                "model": "gpt-4o",
                "status": "pending"  # ğŸ”„ æ”¹ä¸ºpendingï¼Œé‡æ–°æ‰§è¡Œ
            },
            "clueweb_claude": {
                "name": "ClueWeb22 + Claude Sonnet 4", 
                "data_source": "clueweb22",
                "provider": "claude",
                "model": "claude-sonnet-4-20250514",
                "status": "pending"
            },
            "random_openai": {
                "name": "Random Documents + OpenAI GPT-4o",
                "data_source": "random_documents", 
                "provider": "openai",
                "model": "gpt-4o",
                "status": "pending"
            },
            "random_claude": {
                "name": "Random Documents + Claude Sonnet 4",
                "data_source": "random_documents",
                "provider": "claude", 
                "model": "claude-sonnet-4-20250514",
                "status": "pending"
            }
        }
        
        print("ğŸ”§ å…¨æ–°å››æ–¹å¯¹æ¯”å®éªŒåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ†• å…¨æ–°è¾“å‡ºç›®å½•: {self.output_dir}")
        print("ğŸ”„ æ‰€æœ‰å®éªŒå°†ä»é›¶å¼€å§‹æ‰§è¡Œ:")
        for exp_id, config in self.experiments.items():
            print(f"  {exp_id}: {config['name']} - âœ‹ å¾…æ‰§è¡Œ")
    
    def prepare_clueweb_data(self) -> List[Dict[str, Any]]:
        """å‡†å¤‡ClueWeb22æ•°æ®"""
        print("ğŸ“š å‡†å¤‡ClueWeb22æ•°æ®...")
        
        # ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„ä¸»é¢˜å’Œæ–‡æ¡£
        clueweb_topics = [
            "clueweb22-ja0009-18-07874",
            "clueweb22-en0023-77-17052", 
            "clueweb22-en0044-53-10967",
            "clueweb22-en0028-68-06349",
            "clueweb22-en0000-00-00000",
            "clueweb22-en0005-84-07694",
            "clueweb22-ja0001-17-28828",
            "clueweb22-en0037-99-02648",
            "clueweb22-en0026-20-03284"
        ]
        
        topics_data = []
        
        for topic_id in clueweb_topics:
            try:
                # æŸ¥æ‰¾è¯¥ä¸»é¢˜çš„æ–‡æ¡£æ–‡ä»¶
                txt_files = list(self.clueweb_data_dir.glob(f"{topic_id}_top*.txt"))
                
                if not txt_files:
                    print(f"âš ï¸ æœªæ‰¾åˆ°ä¸»é¢˜ {topic_id} çš„æ–‡æ¡£æ–‡ä»¶")
                    continue
                
                # æŒ‰æ–‡ä»¶ç¼–å·æ’åº
                txt_files.sort(key=lambda x: int(re.search(r'_top(\d+)\.txt', x.name).group(1)))
                
                # åŠ è½½æ–‡æ¡£å†…å®¹
                documents = []
                for i, file_path in enumerate(txt_files):
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read().strip()
                        
                        if content and len(content) > 50:
                            documents.append({
                                'doc_id': f"{topic_id}_doc_{i:03d}",
                                'source': file_path.name,
                                'content': content,
                                'word_count': len(content.split()),
                                'char_count': len(content)
                            })
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
                        continue
                
                if documents:
                    topics_data.append({
                        'topic_id': topic_id,
                        'data_source': 'clueweb22',
                        'documents': documents,
                        'document_count': len(documents)
                    })
                    print(f"âœ… {topic_id}: {len(documents)} ä¸ªæ–‡æ¡£")
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†ä¸»é¢˜ {topic_id} å¤±è´¥: {e}")
                continue
        
        print(f"âœ… ClueWeb22æ•°æ®å‡†å¤‡å®Œæˆ: {len(topics_data)} ä¸ªä¸»é¢˜")
        return topics_data
    
    def prepare_random_documents_data(self) -> List[Dict[str, Any]]:
        """å‡†å¤‡éšæœºæ–‡æ¡£æ•°æ®"""
        print("ğŸ“š å‡†å¤‡éšæœºæ–‡æ¡£æ•°æ®...")
        
        # æŸ¥æ‰¾æœ€æ–°çš„éšæœºæ–‡æ¡£JSONæ–‡ä»¶
        json_files = list(self.random_docs_dir.glob("random_documents_*.json"))
        
        if not json_files:
            print("âš ï¸ æœªæ‰¾åˆ°éšæœºæ–‡æ¡£æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œrandom_documents_crawler.py")
            return []
        
        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        latest_json = max(json_files, key=lambda x: x.stat().st_mtime)
        print(f"ğŸ“ ä½¿ç”¨éšæœºæ–‡æ¡£æ–‡ä»¶: {latest_json}")
        
        try:
            with open(latest_json, 'r', encoding='utf-8') as f:
                papers = json.load(f)
            
            # æŒ‰é¢†åŸŸåˆ†ç»„
            domain_groups = {}
            for paper in papers:
                domain = paper.get('domain', 'unknown')
                if domain not in domain_groups:
                    domain_groups[domain] = []
                domain_groups[domain].append(paper)
            
            # ä¸ºæ¯ä¸ªé¢†åŸŸåˆ›å»ºä¸€ä¸ªä¸»é¢˜
            topics_data = []
            for domain, papers_list in domain_groups.items():
                if len(papers_list) >= 5:  # è‡³å°‘5ç¯‡æ–‡æ¡£æ‰åˆ›å»ºä¸»é¢˜
                    documents = []
                    for i, paper in enumerate(papers_list):
                        documents.append({
                            'doc_id': f"random_{domain}_doc_{i:03d}",
                            'source': f"{paper.get('source', 'unknown')}",
                            'content': paper.get('full_content', ''),
                            'word_count': paper.get('word_count', 0),
                            'char_count': paper.get('char_count', 0),
                            'title': paper.get('title', ''),
                            'language': paper.get('language', 'en')
                        })
                    
                    topics_data.append({
                        'topic_id': f"random_{domain}",
                        'data_source': 'random_documents',
                        'domain': domain,
                        'documents': documents,
                        'document_count': len(documents)
                    })
                    print(f"âœ… {domain}: {len(documents)} ä¸ªæ–‡æ¡£")
            
            print(f"âœ… éšæœºæ–‡æ¡£æ•°æ®å‡†å¤‡å®Œæˆ: {len(topics_data)} ä¸ªä¸»é¢˜")
            return topics_data
            
        except Exception as e:
            print(f"âŒ è¯»å–éšæœºæ–‡æ¡£æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _generate_questions_in_batches(self, report_content: str, topic_id: str, provider: str, test_mode: bool = False) -> List[Dict[str, Any]]:
        """åˆ†æ®µç”Ÿæˆé—®é¢˜"""
        if test_mode:
            print("  ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šç”Ÿæˆ3ä¸ªé—®é¢˜")
            questions_result = self.llm_manager.generate_questions(report_content, topic_id, num_questions=3, provider=provider)
            
            if questions_result.success and hasattr(questions_result, 'questions'):
                return questions_result.questions
            else:
                print("  âš ï¸ æµ‹è¯•é—®é¢˜ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜")
                return [
                    {'question': 'What are the main findings in this research?', 'difficulty': 'Easy', 'type': 'factual'},
                    {'question': 'How do these findings relate to current field developments?', 'difficulty': 'Medium', 'type': 'analytical'},
                    {'question': 'What are the implications and future directions?', 'difficulty': 'Hard', 'type': 'evaluative'}
                ]
        else:
            print("  ğŸ“ åˆ†æ®µç”Ÿæˆ50ä¸ªé—®é¢˜...")
            all_questions = []
            
            # åˆ†3æ‰¹ç”Ÿæˆï¼šEasy(15) + Medium(20) + Hard(15) = 50
            batches = [
                ("Easy", 15),
                ("Medium", 20), 
                ("Hard", 15)
            ]
            
            for difficulty, count in batches:
                print(f"    ğŸ¯ ç”Ÿæˆ {difficulty} éš¾åº¦é—®é¢˜ ({count}ä¸ª)...")
                
                batch_result = self.llm_manager.generate_questions(
                    report_content, 
                    f"{topic_id}_{difficulty.lower()}", 
                    num_questions=count, 
                    provider=provider
                )
                
                if batch_result.success and hasattr(batch_result, 'questions'):
                    batch_questions = batch_result.questions
                    # ç¡®ä¿éš¾åº¦è®¾ç½®æ­£ç¡®
                    for q in batch_questions:
                        q['difficulty'] = difficulty
                    all_questions.extend(batch_questions)
                    print(f"    âœ… {difficulty}: {len(batch_questions)} ä¸ªé—®é¢˜")
                else:
                    print(f"    âš ï¸ {difficulty} æ‰¹æ¬¡å¤±è´¥ï¼Œè·³è¿‡")
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                time.sleep(1)
            
            return all_questions
    
    def process_topic_with_llm(self, topic_data: Dict[str, Any], provider: str, model: str, test_mode: bool = False) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šLLMå¤„ç†å•ä¸ªä¸»é¢˜"""
        topic_id = topic_data['topic_id']
        documents = topic_data['documents']
        
        print(f"ğŸ” å¤„ç†ä¸»é¢˜: {topic_id} (ä½¿ç”¨ {provider})")
        
        start_time = time.time()
        
        try:
            # Step 1: ç”ŸæˆæŠ¥å‘Š
            print("  ğŸ“ ç”Ÿæˆé¢†åŸŸæŠ¥å‘Š...")
            report_result = self.llm_manager.generate_report(documents, topic_id, provider)
            
            if not report_result.success:
                print(f"  âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report_result.error}")
                return {
                    'topic': topic_id,
                    'data_source': topic_data['data_source'],
                    'provider': provider,
                    'documents_count': len(documents),
                    'success': False,
                    'error': f"Report generation failed: {report_result.error}",
                    'processing_time': time.time() - start_time
                }
            
            report_content = report_result.content
            print(f"  âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ ({len(report_content.split())} è¯)")
            
            # Step 2: ç”Ÿæˆé—®é¢˜ (åˆ†æ®µç”Ÿæˆ)
            print("  â“ ç”Ÿæˆç ”ç©¶é—®é¢˜...")
            questions_data = self._generate_questions_in_batches(report_content, topic_id, provider, test_mode)
            
            if not questions_data:
                print(f"  âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šåˆ›å»ºé»˜è®¤é—®é¢˜
                questions_data = [
                    {'question': 'What are the main findings in this research?', 'difficulty': 'Easy', 'type': 'factual'},
                    {'question': 'How do these findings relate to current field developments?', 'difficulty': 'Medium', 'type': 'analytical'},
                    {'question': 'What are the implications and future directions?', 'difficulty': 'Hard', 'type': 'evaluative'}
                ]
            
            print(f"  âœ… é—®é¢˜ç”Ÿæˆå®Œæˆ ({len(questions_data)} ä¸ªé—®é¢˜)")
            
            # Step 3: ç”Ÿæˆç­”æ¡ˆ
            print("  ğŸ’¬ ç”Ÿæˆç­”æ¡ˆ...")
            answers_result = self.llm_manager.generate_answers(questions_data, report_content, provider, max_answers=50)
            
            if not answers_result['success']:
                print(f"  âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {answers_result.get('error', 'Unknown error')}")
                answers_result = {
                    'success': True,
                    'count': 0,
                    'total_answer_length': 0
                }
            else:
                print(f"  âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ ({answers_result['count']} ä¸ªç­”æ¡ˆ)")
            
            # ç¼–è¯‘ç»“æœ
            processing_time = time.time() - start_time
            
            result = {
                'topic': topic_id,
                'data_source': topic_data['data_source'],
                'provider': provider,
                'documents_count': len(documents),
                'success': True,
                'processing_time': processing_time,
                'steps': {
                    'report': {
                        'success': True,
                        'content': report_content,
                        'model': report_result.model,
                        'usage': report_result.usage
                    },
                    'questions': {
                        'success': True,
                        'content': f"Generated {len(questions_data)} questions",
                        'questions': questions_data,
                        'count': len(questions_data),
                        'model': report_result.model,  # ä½¿ç”¨reportçš„modelä¿¡æ¯
                        'usage': {'total_tokens': 0}  # åˆ†æ®µç”Ÿæˆï¼Œæš‚æ—¶è®¾ä¸º0
                    },
                    'answers': answers_result
                }
            }
            
            # æ·»åŠ QAå¯¹
            if 'qa_pairs' in answers_result:
                result['qa_pairs'] = answers_result['qa_pairs']
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            result['statistics'] = {
                'total_qa_pairs': answers_result['count'],
                'difficulty_distribution': {},
                'type_distribution': {},
                'answer_length_stats': {},
                'answer_word_count_stats': {}
            }
            
            if 'qa_pairs' in result:
                qa_pairs = result['qa_pairs']
                if qa_pairs:
                    # è®¡ç®—éš¾åº¦åˆ†å¸ƒ
                    difficulties = [qa['difficulty'] for qa in qa_pairs]
                    for diff in ['Easy', 'Medium', 'Hard']:
                        result['statistics']['difficulty_distribution'][diff] = difficulties.count(diff)
                        result['statistics']['difficulty_percentages'] = {
                            diff: (difficulties.count(diff) / len(difficulties) * 100) if difficulties else 0
                            for diff in ['Easy', 'Medium', 'Hard']
                        }
                    
                    # è®¡ç®—ç±»å‹åˆ†å¸ƒ
                    types = [qa['type'] for qa in qa_pairs]
                    unique_types = list(set(types))
                    for qtype in unique_types:
                        result['statistics']['type_distribution'][qtype] = types.count(qtype)
                    
                    # è®¡ç®—ç­”æ¡ˆé•¿åº¦ç»Ÿè®¡
                    answer_lengths = [qa['answer_length'] for qa in qa_pairs if 'answer_length' in qa]
                    answer_word_counts = [qa['answer_word_count'] for qa in qa_pairs if 'answer_word_count' in qa]
                    
                    if answer_lengths:
                        result['statistics']['answer_length_stats'] = {
                            'min': min(answer_lengths),
                            'max': max(answer_lengths),
                            'avg': sum(answer_lengths) / len(answer_lengths)
                        }
                    
                    if answer_word_counts:
                        result['statistics']['answer_word_count_stats'] = {
                            'min': min(answer_word_counts),
                            'max': max(answer_word_counts),
                            'avg': sum(answer_word_counts) / len(answer_word_counts)
                        }
            
            print(f"  âœ… ä¸»é¢˜ {topic_id} å¤„ç†å®Œæˆ ({processing_time:.1f}ç§’)")
            return result
            
        except Exception as e:
            print(f"  âŒ ä¸»é¢˜ {topic_id} å¤„ç†å¤±è´¥: {e}")
            return {
                'topic': topic_id,
                'data_source': topic_data['data_source'],
                'provider': provider,
                'documents_count': len(documents),
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def run_experiment(self, experiment_id: str, topics_data: List[Dict[str, Any]]) -> str:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        config = self.experiments[experiment_id]
        
        print(f"\nğŸš€ å¼€å§‹å®éªŒ: {config['name']}")
        print("=" * 60)
        
        # åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exp_output_dir = self.output_dir / f"{experiment_id}_{timestamp}"
        exp_output_dir.mkdir(exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„å®éªŒç›®å½•ï¼ˆæ–­ç‚¹ç»­åšï¼‰
        existing_dirs = list(self.output_dir.glob(f"{experiment_id}_*"))
        if existing_dirs:
            latest_dir = max(existing_dirs, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ”„ å‘ç°å·²æœ‰å®éªŒç›®å½•: {latest_dir}")
            
            # æ£€æŸ¥å·²å®Œæˆçš„ä¸»é¢˜
            completed_topics = set()
            for json_file in latest_dir.glob("*.json"):
                if json_file.name != "complete_experiment_results.json":
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            result = json.load(f)
                        if result.get('success', False):
                            completed_topics.add(result.get('topic', ''))
                    except:
                        continue
            
            if completed_topics:
                print(f"âœ… å‘ç°å·²å®Œæˆä¸»é¢˜: {len(completed_topics)} ä¸ª")
                print(f"ğŸ“‚ ç»§ç»­ä½¿ç”¨ç›®å½•: {latest_dir}")
                exp_output_dir = latest_dir
                
                # è¿‡æ»¤æ‰å·²å®Œæˆçš„ä¸»é¢˜
                remaining_topics = []
                for topic_data in topics_data:
                    if topic_data['topic_id'] not in completed_topics:
                        remaining_topics.append(topic_data)
                topics_data = remaining_topics
                
                print(f"ğŸ”„ å‰©ä½™å¾…å¤„ç†ä¸»é¢˜: {len(topics_data)} ä¸ª")
                
                if not topics_data:
                    print("âœ… æ‰€æœ‰ä¸»é¢˜å·²å®Œæˆï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆç»“æœ")
                    # è¯»å–å·²æœ‰ç»“æœ
                    results = []
                    for json_file in exp_output_dir.glob("*.json"):
                        if json_file.name != "complete_experiment_results.json":
                            try:
                                with open(json_file, 'r', encoding='utf-8') as f:
                                    result = json.load(f)
                                results.append(result)
                            except:
                                continue
                    
                    # ä¿å­˜å®Œæ•´å®éªŒç»“æœ
                    self._save_complete_results(exp_output_dir, config, experiment_id, results)
                    return str(exp_output_dir)
        
        results = []
        
        try:
            for i, topic_data in enumerate(topics_data, 1):
                print(f"\nğŸ“Š è¿›åº¦: {i}/{len(topics_data)}")
                
                result = self.process_topic_with_llm(
                    topic_data, 
                    config['provider'], 
                    config['model'],
                    test_mode=False  # å®Œæ•´å®éªŒæ¨¡å¼
                )
                
                results.append(result)
                
                # ä¿å­˜å•ä¸ªä¸»é¢˜ç»“æœ
                result_file = exp_output_dir / f"{config['provider']}_{i}_{result['topic']}.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ’¾ ä¿å­˜: {result_file}")
                
                # çŸ­æš‚ä¼‘æ¯é¿å…APIé™åˆ¶
                time.sleep(1)
        
        except Exception as e:
            print(f"âŒ å®éªŒ {experiment_id} æ‰§è¡Œå¤±è´¥: {e}")
            print(f"ğŸ“Š å·²å®Œæˆ: {len(results)} ä¸ªä¸»é¢˜")
            return ""
        
        # ä¿å­˜å®Œæ•´å®éªŒç»“æœ
        self._save_complete_results(exp_output_dir, config, experiment_id, results)
        return str(exp_output_dir)
    
    def _save_complete_results(self, exp_output_dir: Path, config: Dict, experiment_id: str, results: List):
        """ä¿å­˜å®Œæ•´å®éªŒç»“æœ"""
        experiment_result = {
            'experiment_info': {
                'experiment_id': experiment_id,
                'name': config['name'],
                'data_source': config['data_source'],
                'provider': config['provider'],
                'model': config['model'],
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'total_topics': len(results),
                'successful_topics': len([r for r in results if r.get('success', False)]),
                'failed_topics': len([r for r in results if not r.get('success', False)])
            },
            'results': results
        }
        
        complete_results_file = exp_output_dir / "complete_experiment_results.json"
        with open(complete_results_file, 'w', encoding='utf-8') as f:
            json.dump(experiment_result, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… å®éªŒ {config['name']} å®Œæˆ!")
        print(f"ğŸ“ ç»“æœç›®å½•: {exp_output_dir}")
        print(f"ğŸ“Š æˆåŠŸ: {experiment_result['experiment_info']['successful_topics']}/{len(results)} ä¸ªä¸»é¢˜")
    
    def run_all_experiments(self) -> Dict[str, str]:
        """è¿è¡Œæ‰€æœ‰å®éªŒ - å…¨éƒ¨ä»é›¶å¼€å§‹"""
        print("ğŸš€ å¼€å§‹å…¨æ–°å››æ–¹å¯¹æ¯”å®éªŒ")
        print("=" * 70)
        print("ğŸ†• æœ¬æ¬¡å®éªŒå°†å½»åº•é‡æ–°æ‰§è¡Œå››ä¸ªå¯¹ç…§ç»„:")
        print("1. ClueWeb22æ•°æ®é›† + OpenAI GPT-4o (å…¨æ–°æ‰§è¡Œ)")
        print("2. ClueWeb22æ•°æ®é›† + Claude Sonnet 4 (å…¨æ–°æ‰§è¡Œ)")
        print("3. éšæœºæ–‡æ¡£100ç¯‡ + OpenAI GPT-4o (å…¨æ–°æ‰§è¡Œ)")
        print("4. éšæœºæ–‡æ¡£100ç¯‡ + Claude Sonnet 4 (å…¨æ–°æ‰§è¡Œ)")
        print("=" * 70)
        print("âš ï¸  æ³¨æ„: æœ¬æ¬¡å®éªŒä¸ä½¿ç”¨ä»»ä½•å†å²æ•°æ®ï¼Œæ‰€æœ‰ç»“æœå­˜å‚¨åœ¨æ–°ç›®å½•")
        print("=" * 70)
        
        experiment_results = {}
        
        # ğŸ“š å‡†å¤‡å®éªŒæ•°æ®
        print("\nğŸ“š å‡†å¤‡å®éªŒæ•°æ®...")
        clueweb_data = self.prepare_clueweb_data()
        random_docs_data = self.prepare_random_documents_data()
        
        if not clueweb_data:
            print("âŒ ClueWeb22æ•°æ®å‡†å¤‡å¤±è´¥")
            return experiment_results
        
        if not random_docs_data:
            print("âŒ éšæœºæ–‡æ¡£æ•°æ®å‡†å¤‡å¤±è´¥")
            return experiment_results
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   ClueWeb22: {len(clueweb_data)} ä¸ªä¸»é¢˜")
        print(f"   éšæœºæ–‡æ¡£: {len(random_docs_data)} ä¸ªä¸»é¢˜")
        
        # ğŸ”„ æ‰§è¡Œæ‰€æœ‰å››ä¸ªå®éªŒ
        experiments_to_run = [
            ('clueweb_openai', clueweb_data, 'ClueWeb22 + OpenAI GPT-4o'),
            ('clueweb_claude', clueweb_data, 'ClueWeb22 + Claude Sonnet 4'),
            ('random_openai', random_docs_data, 'éšæœºæ–‡æ¡£ + OpenAI GPT-4o'),
            ('random_claude', random_docs_data, 'éšæœºæ–‡æ¡£ + Claude Sonnet 4')
        ]
        
        for i, (exp_id, data, description) in enumerate(experiments_to_run, 1):
            print(f"\nğŸ¯ æ‰§è¡Œå®éªŒ {i}/4: {description}")
            print("=" * 60)
            
            try:
                result_dir = self.run_experiment(exp_id, data)
                if result_dir:
                    experiment_results[exp_id] = result_dir
                    self.experiments[exp_id]['status'] = 'completed'
                    print(f"âœ… å®éªŒ {exp_id} æˆåŠŸå®Œæˆ")
                else:
                    print(f"âŒ å®éªŒ {exp_id} æ‰§è¡Œå¤±è´¥")
            except Exception as e:
                print(f"âŒ å®éªŒ {exp_id} å¼‚å¸¸: {e}")
                continue
        
        print(f"\nğŸ‰ å››æ–¹å¯¹æ¯”å®éªŒå®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸå®éªŒ: {len(experiment_results)}/4")
        
        return experiment_results
    
    def generate_comparison_excel(self, experiment_results: Dict[str, str]) -> str:
        """ç”Ÿæˆå¯¹æ¯”Excelæ–‡ä»¶"""
        print("\nğŸ“Š ç”Ÿæˆå¯¹æ¯”Excelæ–‡ä»¶...")
        
        all_data = []
        
        for exp_id, result_dir in experiment_results.items():
            if not result_dir:
                continue
            
            # å¤„ç†æµ‹è¯•å®éªŒIDçš„æ˜ å°„
            if exp_id.endswith('_test'):
                base_exp_id = exp_id.replace('_test', '')
                if base_exp_id in self.experiments:
                    config = self.experiments[base_exp_id].copy()
                    config['name'] = config['name'] + ' (æµ‹è¯•)'
                else:
                    # æµ‹è¯•å®éªŒçš„é»˜è®¤é…ç½®
                    config = {
                        'name': exp_id.replace('_', ' ').title(),
                        'data_source': 'clueweb22' if 'clueweb' in exp_id else 'random_documents',
                        'provider': 'openai' if 'openai' in exp_id else 'claude',
                        'model': 'gpt-4o' if 'openai' in exp_id else 'claude-sonnet-4-20250514'
                    }
            else:
                config = self.experiments[exp_id]
            result_path = Path(result_dir)
            
            # æŸ¥æ‰¾å®Œæ•´ç»“æœæ–‡ä»¶
            complete_file = result_path / "complete_experiment_results.json"
            if not complete_file.exists():
                # å°è¯•å…¶ä»–å¯èƒ½çš„æ–‡ä»¶å
                json_files = list(result_path.glob("*results*.json"))
                if json_files:
                    complete_file = json_files[0]
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°å®éªŒç»“æœæ–‡ä»¶: {result_path}")
                    continue
            
            try:
                with open(complete_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # å¤„ç†ä¸åŒçš„æ–‡ä»¶æ ¼å¼
                if 'results' in data:
                    results_list = data['results']
                elif 'topic_results' in data:
                    results_list = list(data['topic_results'].values())
                else:
                    print(f"âš ï¸ æ— æ³•è¯†åˆ«ç»“æœæ–‡ä»¶æ ¼å¼: {complete_file}")
                    continue
                
                # æå–æ•°æ®
                for result in results_list:
                    if not result.get('success', False):
                        continue
                    
                    # ä¸»é¢˜æ‘˜è¦è¡Œ
                    topic_summary = {
                        'Experiment_ID': exp_id,
                        'Experiment_Name': config['name'],
                        'Data_Source': config['data_source'],
                        'Provider': config['provider'],
                        'Model': config['model'],
                        'Topic_ID': result.get('topic', 'Unknown'),
                        'Type': 'TOPIC_SUMMARY',
                        'Documents_Count': result.get('documents_count', 0),
                        'Processing_Time': result.get('processing_time', 0),
                        'Success': result.get('success', False),
                        'Report_Content': '',
                        'Report_Length': 0,
                        'Questions_Count': 0,
                        'QA_Pairs_Count': 0,
                        'Report_Usage_Tokens': 0,
                        'Questions_Usage_Tokens': 0,
                        'Total_Usage_Tokens': 0
                    }
                    
                    # æå–æŠ¥å‘Šå†…å®¹
                    if 'steps' in result and 'report' in result['steps']:
                        report_step = result['steps']['report']
                        if report_step.get('success') and 'content' in report_step:
                            topic_summary['Report_Content'] = report_step['content']
                            topic_summary['Report_Length'] = len(report_step['content'].split())
                            # æå–tokenä½¿ç”¨æƒ…å†µ
                            if 'usage' in report_step:
                                usage = report_step['usage']
                                topic_summary['Report_Usage_Tokens'] = usage.get('total_tokens', 0)
                    
                    # æå–é—®é¢˜æ•°é‡
                    if 'steps' in result and 'questions' in result['steps']:
                        questions_step = result['steps']['questions']
                        if questions_step.get('success'):
                            topic_summary['Questions_Count'] = questions_step.get('count', 0)
                            # æå–tokenä½¿ç”¨æƒ…å†µ
                            if 'usage' in questions_step:
                                usage = questions_step['usage']
                                topic_summary['Questions_Usage_Tokens'] = usage.get('total_tokens', 0)
                    
                    # æå–QAå¯¹æ•°é‡
                    if 'qa_pairs' in result:
                        topic_summary['QA_Pairs_Count'] = len(result['qa_pairs'])
                    elif 'statistics' in result:
                        topic_summary['QA_Pairs_Count'] = result['statistics'].get('total_qa_pairs', 0)
                    
                    # è®¡ç®—æ€»tokenä½¿ç”¨é‡
                    topic_summary['Total_Usage_Tokens'] = topic_summary['Report_Usage_Tokens'] + topic_summary['Questions_Usage_Tokens']
                    
                    all_data.append(topic_summary)
                    
                    # QAå¯¹è¯¦ç»†ä¿¡æ¯
                    if 'qa_pairs' in result:
                        for qa in result['qa_pairs']:
                            qa_row = {
                                'Experiment_ID': exp_id,
                                'Experiment_Name': config['name'],
                                'Data_Source': config['data_source'],
                                'Provider': config['provider'],
                                'Model': config['model'],
                                'Topic_ID': result.get('topic', 'Unknown'),
                                'Type': 'QA_PAIR',
                                'Question_ID': qa.get('question_id', ''),
                                'Question': qa.get('question', ''),
                                'Answer': qa.get('answer', ''),
                                'Difficulty': qa.get('difficulty', ''),
                                'Question_Type': qa.get('type', ''),
                                'Answer_Length': qa.get('answer_length', 0),
                                'Answer_Word_Count': qa.get('answer_word_count', 0),
                                'Question_Reasoning': qa.get('reasoning', ''),
                                'Success': qa.get('success', True)
                            }
                            all_data.append(qa_row)
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å®éªŒç»“æœå¤±è´¥ {exp_id}: {e}")
                continue
        
        if not all_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å®éªŒæ•°æ®")
            return ""
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(all_data)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_file = self.output_dir / f"four_way_comparative_results_{timestamp}.xlsx"
        
        try:
            df.to_excel(excel_file, index=False, engine='openpyxl')
            print(f"âœ… Excelæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {excel_file}")
            print(f"ğŸ“Š æ€»è®¡æ•°æ®è¡Œæ•°: {len(all_data)}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            topic_summaries = df[df['Type'] == 'TOPIC_SUMMARY']
            qa_pairs = df[df['Type'] == 'QA_PAIR']
            
            print(f"   ä¸»é¢˜æ‘˜è¦: {len(topic_summaries)} è¡Œ")
            print(f"   QAå¯¹: {len(qa_pairs)} è¡Œ")
            print(f"   å®éªŒåˆ†å¸ƒ:")
            for exp_id in df['Experiment_ID'].unique():
                exp_data = df[df['Experiment_ID'] == exp_id]
                exp_topics = len(exp_data[exp_data['Type'] == 'TOPIC_SUMMARY'])
                exp_qas = len(exp_data[exp_data['Type'] == 'QA_PAIR'])
                print(f"     {exp_id}: {exp_topics} ä¸»é¢˜, {exp_qas} QAå¯¹")
            
            return str(excel_file)
            
        except Exception as e:
            print(f"âŒ Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def generate_comparison_report(self, experiment_results: Dict[str, str], excel_file: str) -> str:
        """ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š"""
        print("\nğŸ“ ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.output_dir / f"FOUR_WAY_COMPARATIVE_ANALYSIS_{timestamp}.md"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# ğŸ“Š å››æ–¹å¯¹æ¯”å®éªŒåˆ†ææŠ¥å‘Š\n")
                f.write("## Four-Way Comparative Experiment Analysis Report\n\n")
                f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n")
                f.write(f"**å®éªŒç›®çš„**: å¯¹æ¯”ClueWeb22æ•°æ®é›†ä¸éšæœºæ–‡æ¡£åœ¨OpenAI GPT-4oå’ŒClaude Sonnet 4ä¸Šçš„è¡¨ç°\n\n")
                
                f.write("---\n\n")
                
                f.write("## ğŸ“‹ å®éªŒè®¾è®¡\n\n")
                f.write("### å®éªŒé…ç½®\n\n")
                f.write("| å®éªŒID | æ•°æ®æº | æ¨¡å‹ | çŠ¶æ€ |\n")
                f.write("|--------|--------|------|------|\n")
                
                # å…ˆå†™å…¥åŸºç¡€å®éªŒé…ç½®
                for exp_id, config in self.experiments.items():
                    status = "âœ… å®Œæˆ" if experiment_results.get(exp_id) else "âŒ å¤±è´¥"
                    f.write(f"| {exp_id} | {config['data_source']} | {config['model']} | {status} |\n")
                
                # ç„¶åå†™å…¥æµ‹è¯•å®éªŒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                for exp_id in experiment_results.keys():
                    if exp_id.endswith('_test') and exp_id not in [k for k in self.experiments.keys()]:
                        test_config = {
                            'name': exp_id.replace('_', ' ').title(),
                            'data_source': 'clueweb22' if 'clueweb' in exp_id else 'random_documents',
                            'provider': 'openai' if 'openai' in exp_id else 'claude',
                            'model': 'gpt-4o' if 'openai' in exp_id else 'claude-sonnet-4-20250514'
                        }
                        status = "âœ… å®Œæˆ" if experiment_results.get(exp_id) else "âŒ å¤±è´¥"
                        f.write(f"| {exp_id} | {test_config['data_source']} | {test_config['model']} | {status} |\n")
                
                f.write("\n### æ•°æ®æºè¯´æ˜\n\n")
                f.write("- **ClueWeb22**: ç½‘ç»œçˆ¬å–çš„å¤šè¯­è¨€æ–‡æ¡£é›†åˆï¼ŒåŒ…å«æ—¥æ–‡å’Œè‹±æ–‡å†…å®¹\n")
                f.write("- **Random Documents**: ä»å­¦æœ¯æ•°æ®åº“çˆ¬å–çš„100ç¯‡è‹±æ–‡+æ—¥æ–‡æ··åˆç ”ç©¶è®ºæ–‡\n\n")
                
                f.write("### æ¨¡å‹è¯´æ˜\n\n")
                f.write("- **OpenAI GPT-4o**: OpenAIæœ€æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹\n")
                f.write("- **Claude Sonnet 4**: Anthropicçš„Claude Sonnet 4æ¨¡å‹\n\n")
                
                f.write("---\n\n")
                
                f.write("## ğŸ“Š å®éªŒç»“æœç»Ÿè®¡\n\n")
                
                # ç»Ÿè®¡æ¯ä¸ªå®éªŒçš„ç»“æœ
                for exp_id, result_dir in experiment_results.items():
                    if not result_dir:
                        continue
                    
                    # å¤„ç†æµ‹è¯•å®éªŒIDçš„æ˜ å°„
                    if exp_id.endswith('_test'):
                        base_exp_id = exp_id.replace('_test', '')
                        if base_exp_id in self.experiments:
                            config = self.experiments[base_exp_id].copy()
                            config['name'] = config['name'] + ' (æµ‹è¯•)'
                        else:
                            # æµ‹è¯•å®éªŒçš„é»˜è®¤é…ç½®
                            config = {
                                'name': exp_id.replace('_', ' ').title(),
                                'data_source': 'clueweb22' if 'clueweb' in exp_id else 'random_documents',
                                'provider': 'openai' if 'openai' in exp_id else 'claude',
                                'model': 'gpt-4o' if 'openai' in exp_id else 'claude-sonnet-4-20250514'
                            }
                    else:
                        config = self.experiments[exp_id]
                    f.write(f"### {config['name']}\n\n")
                    
                    # è¯»å–å®éªŒç»“æœè¿›è¡Œç»Ÿè®¡
                    result_path = Path(result_dir)
                    complete_file = result_path / "complete_experiment_results.json"
                    
                    if complete_file.exists():
                        try:
                            with open(complete_file, 'r', encoding='utf-8') as rf:
                                data = json.load(rf)
                            
                            if 'experiment_info' in data:
                                info = data['experiment_info']
                                f.write(f"- **å¤„ç†ä¸»é¢˜æ•°**: {info.get('total_topics', 0)}\n")
                                f.write(f"- **æˆåŠŸä¸»é¢˜æ•°**: {info.get('successful_topics', 0)}\n")
                                f.write(f"- **æˆåŠŸç‡**: {info.get('successful_topics', 0) / max(info.get('total_topics', 1), 1) * 100:.1f}%\n")
                            
                            # ç»Ÿè®¡å¤„ç†æ—¶é—´
                            if 'results' in data:
                                times = [r.get('processing_time', 0) for r in data['results'] if r.get('success')]
                                if times:
                                    f.write(f"- **å¹³å‡å¤„ç†æ—¶é—´**: {sum(times) / len(times):.1f}ç§’\n")
                                    f.write(f"- **æ€»å¤„ç†æ—¶é—´**: {sum(times):.1f}ç§’\n")
                                
                                # ç»Ÿè®¡ç”Ÿæˆå†…å®¹
                                total_qa_pairs = 0
                                total_report_words = 0
                                
                                for result in data['results']:
                                    if result.get('success'):
                                        if 'qa_pairs' in result:
                                            total_qa_pairs += len(result['qa_pairs'])
                                        if 'steps' in result and 'report' in result['steps']:
                                            report_content = result['steps']['report'].get('content', '')
                                            total_report_words += len(report_content.split())
                                
                                f.write(f"- **ç”ŸæˆQAå¯¹æ•°**: {total_qa_pairs}\n")
                                f.write(f"- **æŠ¥å‘Šæ€»å­—æ•°**: {total_report_words:,}\n")
                        
                        except Exception as e:
                            f.write(f"- **çŠ¶æ€**: ç»“æœæ–‡ä»¶è¯»å–å¤±è´¥ - {e}\n")
                    else:
                        f.write("- **çŠ¶æ€**: ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°\n")
                    
                    f.write(f"- **ç»“æœç›®å½•**: `{result_dir}`\n\n")
                
                f.write("---\n\n")
                
                f.write("## ğŸ” å…³é”®å‘ç°\n\n")
                f.write("### æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n\n")
                f.write("åŸºäºå®éªŒç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦åˆ†æä¸åŒæ¨¡å‹çš„è¡¨ç°ï¼š\n\n")
                f.write("1. **å¤„ç†æ•ˆç‡**: å¤„ç†æ—¶é—´å’ŒæˆåŠŸç‡å¯¹æ¯”\n")
                f.write("2. **å†…å®¹è´¨é‡**: ç”ŸæˆæŠ¥å‘Šå’Œç­”æ¡ˆçš„è´¨é‡\n")
                f.write("3. **æ•°æ®é€‚åº”æ€§**: ä¸åŒæ•°æ®æºä¸Šçš„è¡¨ç°å·®å¼‚\n")
                f.write("4. **å¤šè¯­è¨€å¤„ç†**: å¯¹æ—¥è‹±æ··åˆå†…å®¹çš„å¤„ç†èƒ½åŠ›\n\n")
                
                f.write("### æ•°æ®æºå½±å“åˆ†æ\n\n")
                f.write("é€šè¿‡å¯¹æ¯”ç›¸åŒæ¨¡å‹åœ¨ä¸åŒæ•°æ®æºä¸Šçš„è¡¨ç°ï¼Œå¯ä»¥åˆ†æï¼š\n\n")
                f.write("- **ClueWeb22æ•°æ®é›†**: ç½‘ç»œå†…å®¹çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§\n")
                f.write("- **å­¦æœ¯è®ºæ–‡æ•°æ®**: ç»“æ„åŒ–å†…å®¹çš„å¤„ç†ä¼˜åŠ¿\n")
                f.write("- **è¯­è¨€æ··åˆå½±å“**: å¤šè¯­è¨€å†…å®¹å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“\n\n")
                
                f.write("---\n\n")
                
                f.write("## ğŸ“ˆ è¯¦ç»†åˆ†æ\n\n")
                f.write(f"å®Œæ•´çš„æ•°æ®åˆ†æè¯·å‚è€ƒExcelæ–‡ä»¶: `{Path(excel_file).name if excel_file else 'N/A'}`\n\n")
                f.write("Excelæ–‡ä»¶åŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨ï¼š\n")
                f.write("- **åŸå§‹æ•°æ®**: æ‰€æœ‰å®éªŒçš„è¯¦ç»†ç»“æœ\n")
                f.write("- **ä¸»é¢˜æ‘˜è¦**: æ¯ä¸ªä¸»é¢˜çš„å¤„ç†ç»Ÿè®¡\n")
                f.write("- **QAå¯¹è¯¦æƒ…**: æ‰€æœ‰é—®ç­”å¯¹çš„å…·ä½“å†…å®¹\n\n")
                
                f.write("---\n\n")
                
                f.write("## ğŸ¯ ç»“è®ºä¸å»ºè®®\n\n")
                f.write("### ä¸»è¦ç»“è®º\n\n")
                f.write("1. **æ¨¡å‹é€‚ç”¨æ€§**: ä¸åŒæ¨¡å‹åœ¨ä¸åŒç±»å‹å†…å®¹ä¸Šçš„ä¼˜åŠ¿\n")
                f.write("2. **æ•°æ®æºé€‰æ‹©**: æ•°æ®ç‰¹å¾å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“\n")
                f.write("3. **å¤šè¯­è¨€èƒ½åŠ›**: å„æ¨¡å‹çš„å¤šè¯­è¨€å¤„ç†è¡¨ç°\n")
                f.write("4. **å®ç”¨å»ºè®®**: åŸºäºå®éªŒç»“æœçš„åº”ç”¨å»ºè®®\n\n")
                
                f.write("### åç»­ç ”ç©¶æ–¹å‘\n\n")
                f.write("- æ‰©å¤§æ•°æ®é›†è§„æ¨¡è¿›è¡Œæ›´å¤§è§„æ¨¡å¯¹æ¯”\n")
                f.write("- å¼•å…¥äººå·¥è¯„ä¼°æé«˜è´¨é‡åˆ†æå‡†ç¡®æ€§\n")
                f.write("- é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œä¸“é—¨ä¼˜åŒ–å®éªŒ\n")
                f.write("- æ¢ç´¢æ¨¡å‹ç»„åˆä½¿ç”¨çš„å¯èƒ½æ€§\n\n")
                
                f.write("---\n\n")
                f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
                f.write(f"*å®éªŒç³»ç»Ÿç‰ˆæœ¬: Four-Way Comparative v1.0*\n")
            
            print(f"âœ… å¯¹æ¯”åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_file}")
            return str(report_file)
            
        except Exception as e:
            print(f"âŒ å¯¹æ¯”åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def run_test_experiment(self) -> Dict[str, str]:
        """è¿è¡Œæµ‹è¯•å®éªŒï¼šåªå¤„ç†ä¸€ä¸ªtopic"""
        print("ğŸ§ª å¼€å§‹æµ‹è¯•å®éªŒ")
        print("=" * 70)
        
        experiment_results = {}
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®ï¼šåªå–ä¸€ä¸ªä¸»é¢˜
        print("\nğŸ“š å‡†å¤‡æµ‹è¯•æ•°æ®...")
        clueweb_data = self.prepare_clueweb_data()
        random_docs_data = self.prepare_random_documents_data()
        
        if not clueweb_data and not random_docs_data:
            print("âŒ æµ‹è¯•æ•°æ®å‡†å¤‡å¤±è´¥")
            return experiment_results
        
        # å‡†å¤‡æµ‹è¯•ä¸»é¢˜ï¼šåŒæ—¶æµ‹è¯•ä¸¤ç§æ•°æ®æº
        test_topics = []
        
        if random_docs_data:
            random_topic = random_docs_data[0]  # å–ç¬¬ä¸€ä¸ªéšæœºæ–‡æ¡£ä¸»é¢˜
            test_topics.append({
                'topic': random_topic,
                'data_source': 'random_documents',
                'experiments': [
                    ('random_openai', 'Random Documents + OpenAI GPT-4o'),
                    ('random_claude', 'Random Documents + Claude Sonnet 4')
                ]
            })
            print(f"ğŸ“ éšæœºæ–‡æ¡£æµ‹è¯•ä¸»é¢˜: {random_topic['topic_id']} ({random_topic['document_count']} ä¸ªæ–‡æ¡£)")
        
        if clueweb_data:
            # é€‰æ‹©ClueWeb22ä¸­æ–‡æ¡£è¾ƒå°‘çš„ä¸»é¢˜
            clueweb_data.sort(key=lambda x: x['document_count'])
            clueweb_topic = clueweb_data[0]  # å–æ–‡æ¡£æœ€å°‘çš„ä¸»é¢˜
            test_topics.append({
                'topic': clueweb_topic,
                'data_source': 'clueweb22',
                'experiments': [
                    ('clueweb_openai_test', 'ClueWeb22 + OpenAI GPT-4o (æµ‹è¯•)'),
                    ('clueweb_claude_test', 'ClueWeb22 + Claude Sonnet 4 (æµ‹è¯•)')
                ]
            })
            print(f"ğŸ“ ClueWeb22æµ‹è¯•ä¸»é¢˜: {clueweb_topic['topic_id']} ({clueweb_topic['document_count']} ä¸ªæ–‡æ¡£)")
        
        if not test_topics:
            print("âŒ æœªæ‰¾åˆ°åˆé€‚çš„æµ‹è¯•ä¸»é¢˜")
            return experiment_results
        
        print(f"ğŸ¯ å°†æµ‹è¯• {len(test_topics)} ç§æ•°æ®æº")
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•å®éªŒ
        for topic_info in test_topics:
            test_topic = topic_info['topic']
            test_data_source = topic_info['data_source']
            test_experiments = topic_info['experiments']
            
            print(f"\nğŸ“Š æµ‹è¯•æ•°æ®æº: {test_data_source}")
            print(f"   ä¸»é¢˜: {test_topic['topic_id']}")
            print(f"   æ–‡æ¡£æ•°: {test_topic['document_count']}")
            
            for exp_id, exp_name in test_experiments:
                print(f"\nğŸš€ å¼€å§‹æµ‹è¯•å®éªŒ: {exp_name}")
                print("=" * 60)
                
                try:
                    # åˆ›å»ºæµ‹è¯•å®éªŒé…ç½®
                    test_config = {
                        'name': exp_name,
                        'data_source': test_data_source,
                        'provider': 'openai' if 'openai' in exp_id else 'claude',
                        'model': 'gpt-4o' if 'openai' in exp_id else 'claude-sonnet-4-20250514'
                    }
                    
                    # è¿è¡Œå•ä¸ªä¸»é¢˜æµ‹è¯•
                    result = self.process_topic_with_llm(
                        test_topic,
                        test_config['provider'],
                        test_config['model'],
                        test_mode=True  # æµ‹è¯•æ¨¡å¼ï¼šåªç”Ÿæˆ3ä¸ªé—®é¢˜
                    )
                    
                    # åˆ›å»ºæµ‹è¯•ç»“æœç›®å½•
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    test_output_dir = self.output_dir / f"test_{exp_id}_{timestamp}"
                    test_output_dir.mkdir(exist_ok=True)
                    
                    # ä¿å­˜æµ‹è¯•ç»“æœ
                    result_file = test_output_dir / f"test_{result['topic']}.json"
                    with open(result_file, 'w', encoding='utf-8') as f:
                        json.dump(result, f, indent=2, ensure_ascii=False)
                    
                    # ä¿å­˜å®Œæ•´æµ‹è¯•ç»“æœ
                    test_experiment_result = {
                        'experiment_info': {
                            'experiment_id': exp_id,
                            'name': exp_name,
                            'data_source': test_data_source,
                            'provider': test_config['provider'],
                            'model': test_config['model'],
                            'timestamp': timestamp,
                            'total_topics': 1,
                            'successful_topics': 1 if result.get('success', False) else 0,
                            'failed_topics': 0 if result.get('success', False) else 1,
                            'test_mode': True
                        },
                        'results': [result]
                    }
                    
                    complete_results_file = test_output_dir / "complete_experiment_results.json"
                    with open(complete_results_file, 'w', encoding='utf-8') as f:
                        json.dump(test_experiment_result, f, indent=2, ensure_ascii=False)
                    
                    if result.get('success', False):
                        experiment_results[exp_id] = str(test_output_dir)
                        print(f"âœ… æµ‹è¯•å®éªŒæˆåŠŸ: {exp_name}")
                        print(f"ğŸ“ ç»“æœç›®å½•: {test_output_dir}")
                        
                        # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
                        if 'qa_pairs' in result:
                            print(f"ğŸ“ ç”ŸæˆQAå¯¹: {len(result['qa_pairs'])} ä¸ª")
                        if 'steps' in result and 'report' in result['steps']:
                            report_length = len(result['steps']['report'].get('content', '').split())
                            print(f"ğŸ“„ æŠ¥å‘Šé•¿åº¦: {report_length} è¯")
                        print(f"â±ï¸ å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.1f} ç§’")
                    else:
                        print(f"âŒ æµ‹è¯•å®éªŒå¤±è´¥: {exp_name}")
                        print(f"ğŸ’¥ é”™è¯¯ä¿¡æ¯: {result.get('error', 'Unknown error')}")
                    
                except Exception as e:
                    print(f"âŒ æµ‹è¯•å®éªŒ {exp_name} æ‰§è¡Œå¼‚å¸¸: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        total_expected = sum(len(topic_info['experiments']) for topic_info in test_topics)
        print(f"\nğŸ¯ æµ‹è¯•å®Œæˆï¼æˆåŠŸ: {len(experiment_results)}/{total_expected} ä¸ªå®éªŒ")
        return experiment_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å…¨æ–°å››æ–¹å¯¹æ¯”å®éªŒç³»ç»Ÿ")
    print("=" * 70)
    print("ğŸ†• æœ¬æ¬¡å®éªŒå°†å½»åº•é‡æ–°æ‰§è¡Œå››ä¸ªå¯¹ç…§ç»„:")
    print("1. ClueWeb22æ•°æ®é›† + OpenAI GPT-4o (å…¨æ–°æ‰§è¡Œ)")
    print("2. ClueWeb22æ•°æ®é›† + Claude Sonnet 4 (å…¨æ–°æ‰§è¡Œ)")
    print("3. éšæœºæ–‡æ¡£100ç¯‡ + OpenAI GPT-4o (å…¨æ–°æ‰§è¡Œ)")
    print("4. éšæœºæ–‡æ¡£100ç¯‡ + Claude Sonnet 4 (å…¨æ–°æ‰§è¡Œ)")
    print("=" * 70)
    print("âš ï¸  æ³¨æ„: æœ¬æ¬¡å®éªŒä¸ä½¿ç”¨ä»»ä½•å†å²æ•°æ®ï¼Œæ‰€æœ‰ç»“æœå­˜å‚¨åœ¨æ–°ç›®å½•")
    print("=" * 70)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
    import sys
    test_mode = len(sys.argv) > 1 and sys.argv[1] == "test"
    
    if test_mode:
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªè¿è¡Œä¸€ä¸ªtopicè¿›è¡Œå¿«é€ŸéªŒè¯")
    else:
        # æ·»åŠ ç”¨æˆ·ç¡®è®¤
        print("ğŸ”„ å‡†å¤‡å¼€å§‹å®Œæ•´çš„å››æ–¹å¯¹æ¯”å®éªŒ...")
        print("   é¢„è®¡æ¯ä¸ªä¸»é¢˜éœ€è¦3-5åˆ†é’Ÿå¤„ç†æ—¶é—´")
        print("   å®Œæ•´å®éªŒå¯èƒ½éœ€è¦æ•°å°æ—¶å®Œæˆ")
        
        confirm = input("\næ˜¯å¦ç»§ç»­æ‰§è¡Œå®Œæ•´å®éªŒ? (y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("âŒ å®éªŒå·²å–æ¶ˆ")
            return
    
    # APIå¯†é’¥é…ç½®
    openai_api_key = "xxxx"  # è¯·æ›¿æ¢ä¸ºå®é™…å¯†é’¥
    claude_api_key = os.getenv('ANTHROPIC_API_KEY', 'xxxx')  # è¯·æ›¿æ¢ä¸ºå®é™…å¯†é’¥
    
    try:
        # åˆå§‹åŒ–å®éªŒç³»ç»Ÿ
        experiment = FourWayComparativeExperiment(openai_api_key, claude_api_key)
        
        if test_mode:
            # æµ‹è¯•æ¨¡å¼ï¼šè¿è¡Œå•ä¸ªtopicæµ‹è¯•
            experiment_results = experiment.run_test_experiment()
        else:
            # å®Œæ•´å®éªŒæ¨¡å¼
            experiment_results = experiment.run_all_experiments()
        
        # ç”Ÿæˆå¯¹æ¯”Excelæ–‡ä»¶
        excel_file = experiment.generate_comparison_excel(experiment_results)
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š
        report_file = experiment.generate_comparison_report(experiment_results, excel_file)
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print("\nğŸ‰ å…¨æ–°å››æ–¹å¯¹æ¯”å®éªŒå®Œæˆ!")
        print("=" * 70)
        print("ğŸ“Š å®éªŒç»“æœ:")
        
        for exp_id, result_dir in experiment_results.items():
            # å¤„ç†æµ‹è¯•å®éªŒIDçš„æ˜ å°„
            if exp_id.endswith('_test'):
                base_exp_id = exp_id.replace('_test', '')
                if base_exp_id in experiment.experiments:
                    config = experiment.experiments[base_exp_id].copy()
                    config['name'] = config['name'] + ' (æµ‹è¯•)'
                else:
                    # æµ‹è¯•å®éªŒçš„é»˜è®¤é…ç½®
                    config = {
                        'name': exp_id.replace('_', ' ').title(),
                        'data_source': 'clueweb22' if 'clueweb' in exp_id else 'random_documents',
                        'provider': 'openai' if 'openai' in exp_id else 'claude',
                        'model': 'gpt-4o' if 'openai' in exp_id else 'claude-sonnet-4-20250514'
                    }
            else:
                config = experiment.experiments[exp_id]
            status = "âœ… æˆåŠŸ" if result_dir else "âŒ å¤±è´¥"
            print(f"  {config['name']}: {status}")
            if result_dir:
                print(f"    ç»“æœç›®å½•: {result_dir}")
        
        print(f"\nğŸ“ˆ ç»¼åˆåˆ†æ:")
        if excel_file:
            print(f"  Excelå¯¹æ¯”æ–‡ä»¶: {excel_file}")
        if report_file:
            print(f"  åˆ†ææŠ¥å‘Š: {report_file}")
        
        completed_count = len([r for r in experiment_results.values() if r])
        total_count = 4 if not test_mode else 2  # æµ‹è¯•æ¨¡å¼åªæµ‹è¯•2ä¸ªå®éªŒ
        print(f"\nâœ… å®Œæˆå®éªŒ: {completed_count}/{total_count}")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {experiment.output_dir}")
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 