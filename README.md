# ClueWeb22 问题生成系统

一个基于大语言模型的自动化学术问题生成系统，专为ClueWeb22数据集设计。该项目通过五种不同的实验方法探索了从基础PROMPT到高级多阶段深度思考的问题生成技术演进。

## 🎯 项目概述

本系统能够：
- 自动分析学术文档集合
- 生成高质量的研究问题和详细答案
- 支持多种生成策略和评估方法
- 提供完整的四路对比实验框架

## 📊 实验方法对比

| 方法 | 技术路线 | 优势 | 局限性 | 状态 |
|------|----------|------|--------|------|
| **01-PROMPT纯提示词** | 单步骤直接生成 | 简单快速 | 深度有限 | 已完成 |
| **02-RAG检索增强** | 外部知识库+生成 | 权威性强 | 领域局限 | 已完成 |
| **03-PROMPT+RAG混合** | 智能混合策略 | 平衡效果 | 复杂度高 | 已完成 |
| **04-多阶段深度思考** | 三步骤思考流程 | 质量优秀 | 能源专精 | 已完成 |
| **05-四路对比框架** | 多步骤深度思考 | 通用高质量 | - | 已完成 |
| **06-短答案深度查询** | 短答案+压缩优化 | 精确性强 | 答案单一 | 已完成 |
| **07-Agent推理测试** | 6步树状扩展流程 | 防暴露+客观性 | 复杂度高 | **最新** 🧠 |

## 🏆 推荐方案

### 🧠 最新：07 Agent推理测试框架 (专业Agent测试)

**技术特点**：
- 🎯 **6步设计流程**：短答案提取 → 关键词优化 → Series扩展 → Parallel扩展 → 树构建 → 综合问题
- 🛡️ **防答案暴露**：智能检测并防止Agent推理过程中答案泄露
- 📝 **纯客观表述**：完全消除LLM思考过程，确保绝对客观
- 🌐 **真实Web搜索**：OpenAI官方API，无Mock数据污染
- 🏗️ **树状扩展**：最多3层问题依赖关系，测试深度推理

**使用场景**：AI Agent推理能力测试、防止直接答案获取

### 📊 经典：05四路对比框架 (通用问题生成)

**技术特点**：
- 📊 **四路对比矩阵**：ClueWeb22/随机学术文档 × OpenAI GPT-4o/Claude Sonnet-4
- 🧠 **多步骤深度思考**：Topic收集 → Report生成 → 问题生成 → 答案生成
- 📈 **100%成功率**：已验证的稳定生产级框架
- ⚡ **高效执行**：OpenAI 32.3分钟/主题，Claude 35.4分钟/主题

**使用场景**：传统问答生成、学术研究、教育评估

## 📁 项目结构

```
clueweb22_question_generate/
├── experiments/                     # 七种实验方法
│   ├── 01_prompt_only/             # 纯PROMPT方法
│   ├── 02_rag_approach/            # RAG检索增强方法  
│   ├── 03_hybrid_prompt_rag/       # PROMPT+RAG混合方法
│   ├── 04_multi_stage/             # 多阶段深度思考方法
│   ├── 05_comparative/             # 四路对比框架
│   ├── 06_short_answer_deep_query/ # 短答案深度查询方法
│   └── 07_tree_extension_deep_query/ # Agent推理测试框架(最新🧠)
│       ├── main.py                 # 主实验脚本
│       ├── core_framework.py       # 核心推理框架
│       ├── utils/                  # 工具模块
│       ├── results/                # 实验结果
│       └── *.md                    # 技术文档
├── core/                           # 核心共享组件
│   ├── llm_clients/               # LLM API客户端
│   ├── data_processing/           # 数据处理工具
│   └── evaluation/                # 评估框架
├── data/                           # 数据源
│   ├── clueweb22/                 # ClueWeb22原始数据
│   └── academic_papers/           # 随机学术文档
├── tools/                          # 工具脚本
│   └── analysis/                  # 结果分析工具
├── client_projects/                # 客户项目结果
└── archived/                       # 历史版本存档
```

## 🚀 快速开始

### 1. 环境准备
```bash
pip install -r requirements.txt
```

### 2. 配置API密钥
```bash
export OPENAI_API_KEY="your_openai_api_key"
export ANTHROPIC_API_KEY="your_claude_api_key"
```

### 3. 运行推荐方案

**最新Agent推理测试框架**：
```bash
cd experiments/07_tree_extension_deep_query
python main.py
```

**经典四路对比框架**：
```bash
cd experiments/05_comparative
python four_way_comparative_experiment.py
```

### 4. 分析结果
```bash
cd tools/analysis
python clueweb22_comparative_analysis.py
```

## 📋 实验方法详解

### 🔥 实验05：四路对比框架（推荐）
- **文档**：[技术深度解析](experiments/05_comparative/技术深度解析_05四路对比框架.md)
- **特点**：纯PROMPT多步骤深度思考，无RAG依赖
- **成果**：100%成功率，已验证的生产级方案

### 📚 实验01：纯PROMPT方法
- **文档**：[方法详解](experiments/01_prompt_only/README_PROMPT_ONLY_METHOD.md)
- **特点**：最简单的baseline方法
- **价值**：提供基础对比基准

### 🔍 实验02：RAG检索增强方法
- **文档**：[方法详解](experiments/02_rag_approach/README_RAG_METHOD.md)
- **特点**：570篇能源论文语料库增强
- **价值**：验证外部知识增强效果

### 🔀 实验03：PROMPT+RAG混合方法
- **文档**：[方法详解](experiments/03_hybrid_prompt_rag/README_HYBRID_METHOD.md)
- **特点**：智能混合策略
- **价值**：探索混合架构可行性

### 🧠 实验04：多阶段深度思考方法
- **文档**：[方法详解](experiments/04_multi_stage/README_MULTI_STAGE_METHOD.md)
- **特点**：三步骤思考流程，能源领域专精
- **价值**：验证多阶段思考有效性

### 🎯 实验06：短答案深度查询方法
- **文档**：[实验文档](experiments/06_short_answer_deep_query/)
- **特点**：短答案聚焦+答案压缩优化
- **价值**：精确性强，适合特定答案类型

### 🧠 实验07：Agent推理测试框架（最新v1.1.0）
- **文档**：[README](experiments/07_tree_extension_deep_query/README.md) | [架构文档](experiments/07_tree_extension_deep_query/CODE_ARCHITECTURE.md)
- **特点**：6步设计流程+防答案暴露+纯客观表述+三种糅合问题类型+数据纯净性保证
- **价值**：专门为AI Agent推理能力测试设计，防止直接获取答案
- **技术亮点**：根答案暴露防护、真实Web搜索、树状扩展结构、模糊化认知负担
- **最新功能**：嵌套累积型+LLM整合型+模糊化整合型、增强前缀清理、兜底标记系统
- **质量保证**：英文prompt标准化、零Mock数据、兜底安全修复、历史数据修复工具

## 📊 性能对比

| 指标 | 01-PROMPT | 02-RAG | 03-混合 | 04-多阶段 | 05-四路对比 | 06-短答案 | 07-Agent推理 |
|------|-----------|--------|---------|-----------|-------------|-----------|--------------|
| 问题深度 | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 通用性 | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 实现复杂度 | ⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 成功率 | 85% | 95% | 92% | 89% | **100%** | 88% | **100%** |
| 处理速度 | 最快 | 慢 | 中等 | 中等 | 快 | 中等 | 中等 |
| **特殊能力** | - | 权威引用 | 混合优势 | 深度思考 | 全面对比 | 精确答案 | **防答案暴露** |

## 🎯 项目成果

### 核心技术突破
1. **多步骤深度思考验证**：证明了分阶段生成的有效性
2. **通用化高质量生成**：实现了跨领域的高质量问题生成
3. **稳定的生产级框架**：100%成功率的可靠系统
4. **完整的对比评估体系**：系统性的方法比较和评估
5. **🆕 Agent推理测试创新**：首创防答案暴露的Agent测试框架
6. **🆕 纯客观问答标准**：建立了无思考过程的客观表述规范

### 生成质量标准
- **问题层次**：Easy(15) + Medium(20) + Hard(15) = 50问题/主题
- **答案深度**：Easy(400-600字) + Medium(800-1200字) + Hard(1500-2000字)
- **学术标准**：专家级研究方法论和批判性分析
- **领域覆盖**：计算机科学、生物医学、物理、材料等多学科

## 📖 使用指南

### 新用户推荐路径
1. **了解项目**：阅读本README和[工作流指南](WORKFLOW_GUIDE.md)
2. **体验推荐方案**：直接运行实验05四路对比框架
3. **深入学习**：阅读各实验方法的详细技术文档
4. **自定义实验**：基于需求修改和扩展现有方法

### 高级用户
- **方法对比研究**：使用五种方法进行横向对比实验
- **新领域扩展**：基于05框架扩展到新的学术领域
- **性能优化**：优化API调用策略和生成参数

## 🤝 贡献指南

欢迎贡献！请查看：
- [技术文档](experiments/)：了解各方法的实现细节
- [Issues](../../issues)：报告问题或提出改进建议
- [Pull Requests](../../pulls)：提交代码改进

## 📄 许可证

本项目采用MIT许可证 - 详见[LICENSE](LICENSE)文件

## 🏅 项目特色

- ✅ **完整的技术演进链**：从基础到高级的完整方法探索
- ✅ **生产级质量**：100%成功率的稳定系统  
- ✅ **详尽的技术文档**：每种方法都有完整的技术说明
- ✅ **可扩展架构**：易于扩展到新领域和新需求
- ✅ **开源友好**：完全开源，欢迎社区贡献
- 🆕 **创新Agent测试**：首个防答案暴露的Agent推理测试框架
- 🆕 **客观性标准**：建立纯客观问答表述的行业标准

---

**开始探索高质量学术问题生成的奇妙世界！🚀** 